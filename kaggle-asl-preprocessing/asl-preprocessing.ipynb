{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"def get_char_dict():\n    char_dict = {\n        \" \": 0,\n        \"!\": 1,\n        \"#\": 2,\n        \"$\": 3,\n        \"%\": 4,\n        \"&\": 5,\n        \"'\": 6,\n        \"(\": 7,\n        \")\": 8,\n        \"*\": 9,\n        \"+\": 10,\n        \",\": 11,\n        \"-\": 12,\n        \".\": 13,\n        \"/\": 14,\n        \"0\": 15,\n        \"1\": 16,\n        \"2\": 17,\n        \"3\": 18,\n        \"4\": 19,\n        \"5\": 20,\n        \"6\": 21,\n        \"7\": 22,\n        \"8\": 23,\n        \"9\": 24,\n        \":\": 25,\n        \";\": 26,\n        \"=\": 27,\n        \"?\": 28,\n        \"@\": 29,\n        \"[\": 30,\n        \"_\": 31,\n        \"a\": 32,\n        \"b\": 33,\n        \"c\": 34,\n        \"d\": 35,\n        \"e\": 36,\n        \"f\": 37,\n        \"g\": 38,\n        \"h\": 39,\n        \"i\": 40,\n        \"j\": 41,\n        \"k\": 42,\n        \"l\": 43,\n        \"m\": 44,\n        \"n\": 45,\n        \"o\": 46,\n        \"p\": 47,\n        \"q\": 48,\n        \"r\": 49,\n        \"s\": 50,\n        \"t\": 51,\n        \"u\": 52,\n        \"v\": 53,\n        \"w\": 54,\n        \"x\": 55,\n        \"y\": 56,\n        \"z\": 57,\n        \"~\": 58,\n    }\n    char_dict[\"P\"] = 59\n    char_dict[\"SOS\"] = 60\n    char_dict[\"EOS\"] = 61\n    return char_dict\n\n\nclass Constants:\n    ROWS_PER_FRAME = 543\n    MAX_STRING_LEN = 50\n    INPUT_PAD = -100.0\n    char_dict = get_char_dict()\n    LABEL_PAD = char_dict[\"P\"]\n    inv_dict = {v: k for k, v in char_dict.items()}\n    NOSE = [1, 2, 98, 327]\n    LIP = [\n        0,\n        61,\n        185,\n        40,\n        39,\n        37,\n        267,\n        269,\n        270,\n        409,\n        291,\n        146,\n        91,\n        181,\n        84,\n        17,\n        314,\n        405,\n        321,\n        375,\n        78,\n        191,\n        80,\n        81,\n        82,\n        13,\n        312,\n        311,\n        310,\n        415,\n        95,\n        88,\n        178,\n        87,\n        14,\n        317,\n        402,\n        318,\n        324,\n        308,\n    ]\n\n    REYE = [33, 7, 163, 144, 145, 153, 154, 155, 133, 246, 161, 160, 159, 158, 157, 173]\n    LEYE = [263, 249, 390, 373, 374, 380, 381, 382, 362, 466, 388, 387, 386, 385, 384, 398]\n\n    LHAND = list(range(468, 489))\n    RHAND = list(range(522, 543))\n\n    LNOSE = [98]\n    RNOSE = [327]\n\n    LLIP = [84, 181, 91, 146, 61, 185, 40, 39, 37, 87, 178, 88, 95, 78, 191, 80, 81, 82]\n    RLIP = [\n        314,\n        405,\n        321,\n        375,\n        291,\n        409,\n        270,\n        269,\n        267,\n        317,\n        402,\n        318,\n        324,\n        308,\n        415,\n        310,\n        311,\n        312,\n    ]\n    POSE = [500, 502, 504, 501, 503, 505, 512, 513]\n    LPOSE = [513, 505, 503, 501]\n    RPOSE = [512, 504, 502, 500]\n\n    POINT_LANDMARKS_PARTS = [LHAND, RHAND, LLIP, RLIP, LPOSE, RPOSE, NOSE, REYE, LEYE]\n    # POINT_LANDMARKS_PARTS = [LHAND, RHAND, NOSE]\n    POINT_LANDMARKS = [item for sublist in POINT_LANDMARKS_PARTS for item in sublist]\n    parts = {\n        \"LLIP\": LLIP,\n        \"RLIP\": RLIP,\n        \"LHAND\": LHAND,\n        \"RHAND\": RHAND,\n        \"LPOSE\": LPOSE,\n        \"RPOSE\": RPOSE,\n        \"LNOSE\": LNOSE,\n        \"RNOSE\": RNOSE,\n        \"REYE\": REYE,\n        \"LEYE\": LEYE,\n    }\n\n    LANDMARK_INDICES = {}  # type: ignore\n    for part in parts:\n        LANDMARK_INDICES[part] = []\n        for landmark in parts[part]:\n            if landmark in POINT_LANDMARKS:\n                LANDMARK_INDICES[part].append(POINT_LANDMARKS.index(landmark))\n\n    CENTER_LANDMARKS = LNOSE + RNOSE\n    CENTER_INDICES = LANDMARK_INDICES[\"LNOSE\"] + LANDMARK_INDICES[\"RNOSE\"]\n\n    NUM_NODES = len(POINT_LANDMARKS)\n    NUM_INPUT_FEATURES = 2 * NUM_NODES\n    CHANNELS = 6 * NUM_NODES\n","metadata":{"execution":{"iopub.status.busy":"2023-07-15T03:20:51.198411Z","iopub.execute_input":"2023-07-15T03:20:51.198827Z","iopub.status.idle":"2023-07-15T03:20:51.261074Z","shell.execute_reply.started":"2023-07-15T03:20:51.198785Z","shell.execute_reply":"2023-07-15T03:20:51.260038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef selected_columns(file_example):\n    df = pd.read_parquet(file_example)\n    selected_x = df.columns[[x + 1 for x in Constants.POINT_LANDMARKS]].tolist()\n    selected_y = [c.replace(\"x\", \"y\") for c in selected_x]\n    selected = []\n    for i in range(Constants.NUM_NODES):\n        selected.append(selected_x[i])\n        selected.append(selected_y[i])\n    return selected  # x1,y1,x2,y2,...\n","metadata":{"execution":{"iopub.status.busy":"2023-07-15T03:20:51.263441Z","iopub.execute_input":"2023-07-15T03:20:51.263914Z","iopub.status.idle":"2023-07-15T03:20:51.271478Z","shell.execute_reply.started":"2023-07-15T03:20:51.263873Z","shell.execute_reply":"2023-07-15T03:20:51.270381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport pandas as pd\nimport os\nimport numpy as np\nimport tensorflow as tf\n\n\ninput_path = \"/kaggle/input/asl-fingerspelling/\"\noutput_path = \"/kaggle/working/\"\n\ndef _float_array_feature(value):\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\n\ndef _int_array_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef preprocess():\n\n    files1 = glob.glob(input_path + \"train_landmarks/*.parquet\")\n    files2 = glob.glob(input_path + \"supplemental_landmarks/*.parquet\")\n    files = files1 + files2\n\n    dtrain1 = pd.read_csv(input_path + \"train.csv\")\n    dtrain2 = pd.read_csv(input_path + \"supplemental_metadata.csv\")\n    dtrain = pd.concat([dtrain1, dtrain2])\n    # print(dtrain[[\"file_id\", \"sequence_id\", \"participant_id\"]].sort_values(by=[\"participant_id\"]))\n    # MAX_STRING_LEN = 43\n\n    os.makedirs(output_path + \"records/\", exist_ok=True)\n    fold = 0\n    options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n    columns = selected_columns(files[0])\n    for file_name in files:\n        print(file_name)\n        fold += 1\n        file_id = file_name.split(\"/\")[-1].split(\".\")[0]\n        df = pd.read_parquet(file_name, columns=columns)\n        labels = dtrain[dtrain[\"file_id\"].astype(str) == file_id]\n        unique_seqs = df.index.unique()\n        output_file = file_name.split(\"/\")[-1].replace(\"parquet\", \"tfrecord\")\n        if \"supp\" in file_name:\n            output_file = \"supp_\" + output_file\n        output_file = output_path + \"records/\" + output_file\n\n        with tf.io.TFRecordWriter(output_file,options=options) as writer:\n            for seq in unique_seqs:\n                phrase = labels[labels[\"sequence_id\"] == seq][\"phrase\"].item()\n                label = [Constants.char_dict[x] for x in phrase]\n                frames = df.loc[seq]\n                # print(file_id, seq, phrase)\n                if frames.empty:\n                    continue\n                frames_numpy = frames.to_numpy().flatten().astype(np.float32)\n                features_dict = {\n                    \"coordinates\": _float_array_feature(frames_numpy),\n                    \"label\": _int_array_feature(label),\n                }\n                features = tf.train.Features(feature=features_dict)\n                example_proto = tf.train.Example(features=features)\n                example = example_proto.SerializeToString()\n                writer.write(example)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-15T03:36:00.710737Z","iopub.execute_input":"2023-07-15T03:36:00.711204Z","iopub.status.idle":"2023-07-15T03:36:00.729602Z","shell.execute_reply.started":"2023-07-15T03:36:00.71117Z","shell.execute_reply":"2023-07-15T03:36:00.728299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess()","metadata":{"execution":{"iopub.status.busy":"2023-07-15T03:36:04.286392Z","iopub.execute_input":"2023-07-15T03:36:04.28691Z","iopub.status.idle":"2023-07-15T04:31:31.003788Z","shell.execute_reply.started":"2023-07-15T03:36:04.286868Z","shell.execute_reply":"2023-07-15T04:31:31.002647Z"},"trusted":true},"execution_count":null,"outputs":[]}]}