{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Google - American Sign Language Fingerspelling Recognition with TensorFlow\n","\n","This notebook walks you through how to train a Transformer model using TensorFlow on the Google - American Sign Language Fingerspelling Recognition dataset made available for this competition.\n","\n","The objective of the model is to predict and translate American Sign Language (ASL) fingerspelling from a set of video frames into text(`phrase`).\n","\n","In this notebook you will learn:\n","\n","- How to load the data\n","- Convert the data to tfrecords to make it faster to re-traing the model\n","- Train a transformer models on the data\n","- Convert the model to TFLite\n","- Create a submission"]},{"cell_type":"markdown","metadata":{},"source":["# Installation\n","\n","Specifically for this competition, you'll need the mediapipe library to work on the data and visualize it"]},{"cell_type":"markdown","metadata":{},"source":["# Import the libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-18T09:04:40.055733Z","iopub.status.busy":"2023-07-18T09:04:40.054786Z","iopub.status.idle":"2023-07-18T09:04:43.829827Z","shell.execute_reply":"2023-07-18T09:04:43.828466Z","shell.execute_reply.started":"2023-07-18T09:04:40.055676Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-07-18 20:07:27.017675: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-07-18 20:07:27.803598: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import random\n","import psutil\n","import glob\n","import gc\n","import math\n","from tensorflow.python.framework.ops import disable_eager_execution\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.832765Z","iopub.status.busy":"2023-07-18T09:04:43.832015Z","iopub.status.idle":"2023-07-18T09:04:43.840057Z","shell.execute_reply":"2023-07-18T09:04:43.83867Z","shell.execute_reply.started":"2023-07-18T09:04:43.832729Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-07-18 20:07:28.670648: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-18 20:07:28.705030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-18 20:07:28.705173: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"]}],"source":["gpus = tf.config.list_physical_devices(\"GPU\")\n","for gpu in gpus:\n","    tf.config.experimental.set_memory_growth(gpu, True)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.841953Z","iopub.status.busy":"2023-07-18T09:04:43.84152Z","iopub.status.idle":"2023-07-18T09:04:43.853631Z","shell.execute_reply":"2023-07-18T09:04:43.852101Z","shell.execute_reply.started":"2023-07-18T09:04:43.84192Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["TensorFlow v2.13.0\n"]}],"source":["print(\"TensorFlow v\" + tf.__version__)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.856382Z","iopub.status.busy":"2023-07-18T09:04:43.855886Z","iopub.status.idle":"2023-07-18T09:04:43.869418Z","shell.execute_reply":"2023-07-18T09:04:43.868074Z","shell.execute_reply.started":"2023-07-18T09:04:43.856337Z"},"trusted":true},"outputs":[],"source":["class MemoryUsageCallbackExtended(tf.keras.callbacks.Callback):\n","    \"\"\"Monitor memory usage on epoch begin and end, collect garbage\"\"\"\n","\n","    # def on_epoch_begin(self, epoch, logs=None):\n","    #    print(\"**Epoch {}**\".format(epoch))\n","    #    print(\n","    #        f\"Memory usage on epoch begin: {int(psutil.Process(os.getpid()).memory_info().rss)/1e9:.2f GB}\"\n","    #    )\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        print(\n","            f\"Memory usage on epoch end: {int(psutil.Process(os.getpid()).memory_info().rss)/1e9:.2f} GB\"\n","        )\n","        gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# Scheduler"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.87415Z","iopub.status.busy":"2023-07-18T09:04:43.873737Z","iopub.status.idle":"2023-07-18T09:04:43.90059Z","shell.execute_reply":"2023-07-18T09:04:43.898986Z","shell.execute_reply.started":"2023-07-18T09:04:43.874119Z"},"trusted":true},"outputs":[],"source":["\n","class CosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    \"\"\"A LearningRateSchedule that uses a cosine decay with optional warmup.\n","\n","    See [Loshchilov & Hutter, ICLR2016](https://arxiv.org/abs/1608.03983),\n","    SGDR: Stochastic Gradient Descent with Warm Restarts.\n","\n","    For the idea of a linear warmup of our learning rate,\n","    see [Goyal et al.](https://arxiv.org/pdf/1706.02677.pdf).\n","\n","    When we begin training a model, we often want an initial increase in our\n","    learning rate followed by a decay. If `warmup_target` is an int, this\n","    schedule applies a linear increase per optimizer step to our learning rate\n","    from `initial_learning_rate` to `warmup_target` for a duration of\n","    `warmup_steps`. Afterwards, it applies a cosine decay function taking our\n","    learning rate from `warmup_target` to `alpha` for a duration of\n","    `decay_steps`. If `warmup_target` is None we skip warmup and our decay\n","    will take our learning rate from `initial_learning_rate` to `alpha`.\n","    It requires a `step` value to  compute the learning rate. You can\n","    just pass a TensorFlow variable that you increment at each training step.\n","\n","    The schedule is a 1-arg callable that produces a warmup followed by a\n","    decayed learning rate when passed the current optimizer step. This can be\n","    useful for changing the learning rate value across different invocations of\n","    optimizer functions.\n","\n","    Our warmup is computed as:\n","\n","    ```python\n","    def warmup_learning_rate(step):\n","        completed_fraction = step / warmup_steps\n","        total_delta = target_warmup - initial_learning_rate\n","        return completed_fraction * total_delta\n","    ```\n","\n","    And our decay is computed as:\n","\n","    ```python\n","    if warmup_target is None:\n","        initial_decay_lr = initial_learning_rate\n","    else:\n","        initial_decay_lr = warmup_target\n","\n","    def decayed_learning_rate(step):\n","        step = min(step, decay_steps)\n","        cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))\n","        decayed = (1 - alpha) * cosine_decay + alpha\n","        return initial_decay_lr * decayed\n","    ```\n","\n","    Example usage without warmup:\n","\n","    ```python\n","    decay_steps = 1000\n","    initial_learning_rate = 0.1\n","    lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n","        initial_learning_rate, decay_steps)\n","    ```\n","\n","    Example usage with warmup:\n","\n","    ```python\n","    decay_steps = 1000\n","    initial_learning_rate = 0\n","    warmup_steps = 1000\n","    target_learning_rate = 0.1\n","    lr_warmup_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n","        initial_learning_rate, decay_steps, warmup_target=target_learning_rate,\n","        warmup_steps=warmup_steps\n","    )\n","    ```\n","\n","    You can pass this schedule directly into a `tf.keras.optimizers.Optimizer`\n","    as the learning rate. The learning rate schedule is also serializable and\n","    deserializable using `tf.keras.optimizers.schedules.serialize` and\n","    `tf.keras.optimizers.schedules.deserialize`.\n","\n","    Returns:\n","      A 1-arg callable learning rate schedule that takes the current optimizer\n","      step and outputs the decayed learning rate, a scalar `Tensor` of the same\n","      type as `initial_learning_rate`.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        initial_learning_rate,\n","        decay_steps,\n","        alpha=0.0,\n","        name=None,\n","        warmup_target=None,\n","        warmup_steps=0,\n","    ):\n","        \"\"\"Applies cosine decay to the learning rate.\n","\n","        Args:\n","          initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a\n","            Python int. The initial learning rate.\n","          decay_steps: A scalar `int32` or `int64` `Tensor` or a Python int.\n","            Number of steps to decay over.\n","          alpha: A scalar `float32` or `float64` `Tensor` or a Python int.\n","            Minimum learning rate value for decay as a fraction of\n","            `initial_learning_rate`.\n","          name: String. Optional name of the operation.  Defaults to\n","            'CosineDecay'.\n","          warmup_target: None or a scalar `float32` or `float64` `Tensor` or a\n","            Python int. The target learning rate for our warmup phase. Will cast\n","            to the `initial_learning_rate` datatype. Setting to None will skip\n","            warmup and begins decay phase from `initial_learning_rate`.\n","            Otherwise scheduler will warmup from `initial_learning_rate` to\n","            `warmup_target`.\n","          warmup_steps: A scalar `int32` or `int64` `Tensor` or a Python int.\n","            Number of steps to warmup over.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.initial_learning_rate = initial_learning_rate\n","        self.decay_steps = decay_steps\n","        self.alpha = alpha\n","        self.name = name\n","        self.warmup_steps = warmup_steps\n","        self.warmup_target = warmup_target\n","\n","    def _decay_function(self, step, decay_steps, decay_from_lr, dtype):\n","        with tf.name_scope(self.name or \"CosineDecay\"):\n","            completed_fraction = step / decay_steps\n","            tf_pi = tf.constant(math.pi, dtype=dtype)\n","            cosine_decayed = 0.5 * (1.0 + tf.cos(tf_pi * completed_fraction))\n","            decayed = (1 - self.alpha) * cosine_decayed + self.alpha\n","            return tf.multiply(decay_from_lr, decayed)\n","\n","    def _warmup_function(self, step, warmup_steps, warmup_target, initial_learning_rate):\n","        with tf.name_scope(self.name or \"CosineDecay\"):\n","            completed_fraction = step / warmup_steps\n","            total_step_delta = warmup_target - initial_learning_rate\n","            return total_step_delta * completed_fraction + initial_learning_rate\n","\n","    def __call__(self, step):\n","        with tf.name_scope(self.name or \"CosineDecay\"):\n","            initial_learning_rate = tf.convert_to_tensor(\n","                self.initial_learning_rate, name=\"initial_learning_rate\"\n","            )\n","            dtype = initial_learning_rate.dtype\n","            decay_steps = tf.cast(self.decay_steps, dtype)\n","            global_step_recomp = tf.cast(step, dtype)\n","\n","            if self.warmup_target is None:\n","                global_step_recomp = tf.minimum(global_step_recomp, decay_steps)\n","                return self._decay_function(\n","                    global_step_recomp,\n","                    decay_steps,\n","                    initial_learning_rate,\n","                    dtype,\n","                )\n","\n","            warmup_target = tf.cast(self.warmup_target, dtype)\n","            warmup_steps = tf.cast(self.warmup_steps, dtype)\n","\n","            global_step_recomp = tf.minimum(global_step_recomp, decay_steps + warmup_steps)\n","\n","            return tf.cond(\n","                global_step_recomp < warmup_steps,\n","                lambda: self._warmup_function(\n","                    global_step_recomp,\n","                    warmup_steps,\n","                    warmup_target,\n","                    initial_learning_rate,\n","                ),\n","                lambda: self._decay_function(\n","                    global_step_recomp - warmup_steps,\n","                    decay_steps,\n","                    warmup_target,\n","                    dtype,\n","                ),\n","            )\n","\n","    def get_config(self):\n","        return {\n","            \"initial_learning_rate\": self.initial_learning_rate,\n","            \"decay_steps\": self.decay_steps,\n","            \"alpha\": self.alpha,\n","            \"name\": self.name,\n","            \"warmup_target\": self.warmup_target,\n","            \"warmup_steps\": self.warmup_steps,\n","        }\n"]},{"cell_type":"markdown","metadata":{},"source":["# Constants"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.902869Z","iopub.status.busy":"2023-07-18T09:04:43.902415Z","iopub.status.idle":"2023-07-18T09:04:43.928871Z","shell.execute_reply":"2023-07-18T09:04:43.927451Z","shell.execute_reply.started":"2023-07-18T09:04:43.902837Z"},"trusted":true},"outputs":[],"source":["def get_char_dict():\n","    char_dict = {\n","        \" \": 0,\n","        \"!\": 1,\n","        \"#\": 2,\n","        \"$\": 3,\n","        \"%\": 4,\n","        \"&\": 5,\n","        \"'\": 6,\n","        \"(\": 7,\n","        \")\": 8,\n","        \"*\": 9,\n","        \"+\": 10,\n","        \",\": 11,\n","        \"-\": 12,\n","        \".\": 13,\n","        \"/\": 14,\n","        \"0\": 15,\n","        \"1\": 16,\n","        \"2\": 17,\n","        \"3\": 18,\n","        \"4\": 19,\n","        \"5\": 20,\n","        \"6\": 21,\n","        \"7\": 22,\n","        \"8\": 23,\n","        \"9\": 24,\n","        \":\": 25,\n","        \";\": 26,\n","        \"=\": 27,\n","        \"?\": 28,\n","        \"@\": 29,\n","        \"[\": 30,\n","        \"_\": 31,\n","        \"a\": 32,\n","        \"b\": 33,\n","        \"c\": 34,\n","        \"d\": 35,\n","        \"e\": 36,\n","        \"f\": 37,\n","        \"g\": 38,\n","        \"h\": 39,\n","        \"i\": 40,\n","        \"j\": 41,\n","        \"k\": 42,\n","        \"l\": 43,\n","        \"m\": 44,\n","        \"n\": 45,\n","        \"o\": 46,\n","        \"p\": 47,\n","        \"q\": 48,\n","        \"r\": 49,\n","        \"s\": 50,\n","        \"t\": 51,\n","        \"u\": 52,\n","        \"v\": 53,\n","        \"w\": 54,\n","        \"x\": 55,\n","        \"y\": 56,\n","        \"z\": 57,\n","        \"~\": 58,\n","    }\n","    char_dict[\"P\"] = 59\n","    char_dict[\"SOS\"] = 60\n","    char_dict[\"EOS\"] = 61\n","    return char_dict\n","\n","\n","class Constants:\n","    ROWS_PER_FRAME = 543\n","    MAX_STRING_LEN = 50\n","    INPUT_PAD = -100.0\n","    char_dict = get_char_dict()\n","    LABEL_PAD = char_dict[\"P\"]\n","    inv_dict = {v: k for k, v in char_dict.items()}\n","    NOSE = [1, 2, 98, 327]\n","\n","    REYE = [33, 7, 163, 144, 145, 153, 154, 155, 133, 246, 161, 160, 159, 158, 157, 173]\n","    LEYE = [263, 249, 390, 373, 374, 380, 381, 382, 362, 466, 388, 387, 386, 385, 384, 398]\n","\n","    LHAND = list(range(468, 489))\n","    RHAND = list(range(522, 543))\n","\n","    LNOSE = [98]\n","    RNOSE = [327]\n","\n","    LLIP = [84, 181, 91, 146, 61, 185, 40, 39, 37, 87, 178, 88, 95, 78, 191, 80, 81, 82]\n","    RLIP = [\n","        314,\n","        405,\n","        321,\n","        375,\n","        291,\n","        409,\n","        270,\n","        269,\n","        267,\n","        317,\n","        402,\n","        318,\n","        324,\n","        308,\n","        415,\n","        310,\n","        311,\n","        312,\n","    ]\n","    POSE = [500, 502, 504, 501, 503, 505, 512, 513]\n","    LPOSE = [513, 505, 503, 501]\n","    RPOSE = [512, 504, 502, 500]\n","\n","    POINT_LANDMARKS_PARTS = [LHAND, RHAND, LLIP, RLIP, LPOSE, RPOSE, NOSE, REYE, LEYE]\n","    # POINT_LANDMARKS_PARTS = [LHAND, RHAND, NOSE]\n","    POINT_LANDMARKS = [item for sublist in POINT_LANDMARKS_PARTS for item in sublist]\n","    parts = {\n","        \"LLIP\": LLIP,\n","        \"RLIP\": RLIP,\n","        \"LHAND\": LHAND,\n","        \"RHAND\": RHAND,\n","        \"LPOSE\": LPOSE,\n","        \"RPOSE\": RPOSE,\n","        \"LNOSE\": LNOSE,\n","        \"RNOSE\": RNOSE,\n","        \"REYE\": REYE,\n","        \"LEYE\": LEYE,\n","    }\n","\n","    LANDMARK_INDICES = {}  # type: ignore\n","    for part in parts:\n","        LANDMARK_INDICES[part] = []\n","        for landmark in parts[part]:\n","            if landmark in POINT_LANDMARKS:\n","                LANDMARK_INDICES[part].append(POINT_LANDMARKS.index(landmark))\n","\n","    CENTER_LANDMARKS = LNOSE + RNOSE\n","    CENTER_INDICES = LANDMARK_INDICES[\"LNOSE\"] + LANDMARK_INDICES[\"RNOSE\"]\n","\n","    NUM_NODES = len(POINT_LANDMARKS)\n","    NUM_INPUT_FEATURES = 2 * NUM_NODES\n","    CHANNELS = 6 * NUM_NODES\n"]},{"cell_type":"markdown","metadata":{},"source":["# Utils"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.930812Z","iopub.status.busy":"2023-07-18T09:04:43.93046Z","iopub.status.idle":"2023-07-18T09:04:43.966369Z","shell.execute_reply":"2023-07-18T09:04:43.964929Z","shell.execute_reply.started":"2023-07-18T09:04:43.930782Z"},"trusted":true},"outputs":[],"source":["\n","# Seed all random number generators\n","def seed_everything(seed=42):\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    tf.random.set_seed(seed)\n","\n","\n","def selected_columns(file_example):\n","    df = pd.read_parquet(file_example)\n","    selected_x = df.columns[[x + 1 for x in Constants.POINT_LANDMARKS]].tolist()\n","    selected_y = [c.replace(\"x\", \"y\") for c in selected_x]\n","    selected = []\n","    for i in range(Constants.NUM_NODES):\n","        selected.append(selected_x[i])\n","        selected.append(selected_y[i])\n","    return selected  # x1,y1,x2,y2,...\n","\n","\n","\n","def num_to_char_fn(y):\n","    return [Constants.inv_dict.get(x, \"\") for x in y]\n","\n","\n","# A callback class to output a few transcriptions during training\n","class CallbackEval(tf.keras.callbacks.Callback):\n","    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n","\n","    def __init__(self, model, dataset):\n","        super().__init__()\n","        self.dataset = dataset\n","        self.model = model\n","\n","    def on_epoch_end(self, epoch: int, logs=None):\n","        predictions = []\n","        targets = []\n","        for batch in self.dataset:\n","            X, y = batch\n","            batch_predictions = self.model(X)\n","            batch_predictions = decode_batch_predictions(batch_predictions)\n","            predictions.extend(batch_predictions)\n","            for label in y:\n","                label = \"\".join(num_to_char_fn(label.numpy()))\n","                targets.append(label)\n","        print(\"-\" * 100)\n","        # for i in np.random.randint(0, len(predictions), 2):\n","        for i in range(10):\n","            print(f\"Target    : {targets[i]}\")\n","            print(f\"Prediction: {predictions[i]}, len: {len(predictions[i])}\")\n","            print(\"-\" * 100)\n","\n","\n","def decode_phrase(pred):\n","    # decode cts prediction by prunning\n","    # (T,CHAR_NUMS)\n","    x = tf.argmax(pred, axis=1)\n","    paddings = tf.constant(\n","        [\n","            [0, 1],\n","        ]\n","    )\n","    x = tf.pad(x, paddings)\n","    diff = tf.not_equal(x[:-1], x[1:])\n","    adjacent_indices = tf.where(diff)[:, 0]\n","    x = tf.gather(x, adjacent_indices)\n","    mask = x != Constants.LABEL_PAD\n","    x = tf.boolean_mask(x, mask, axis=0)\n","    return x\n","\n","\n","# A utility function to decode the output of the network\n","def decode_batch_predictions(pred):\n","    output_text = []\n","    for result in pred:\n","        result = \"\".join(num_to_char_fn(decode_phrase(result).numpy()))\n","        output_text.append(result)\n","    return output_text\n","\n","\n","\n","\n","def code_to_label(label_code):\n","    label = [Constants.inv_dict[x] for x in label_code if Constants.inv_dict[x] != \"P\"]\n","    label = \"\".join(label)\n","    return label\n","\n","\n","def convert_to_strings(batch_label_code):\n","    output = []\n","    for label_code in batch_label_code:\n","        output.append(code_to_label(label_code))\n","    return output\n","\n","\n","def global_metric(val_ds, model):\n","    global_N, global_D = 0, 0\n","    count = 0\n","    metric = LevDistanceMetric()\n","    for batch in val_ds:\n","        count += 1\n","        print(count)\n","        feature, label = batch\n","        logits = model(feature)\n","        _, _, D = batch_edit_distance(label, logits)\n","        metric.update_state(label, logits)\n","   \n","    result = metric.result().numpy()\n","   \n","    return result\n","\n","\n","def sparse_from_dense_ignore_value(dense_tensor):\n","    mask = tf.not_equal(dense_tensor, Constants.LABEL_PAD)\n","    indices = tf.where(mask)\n","    values = tf.boolean_mask(dense_tensor, mask)\n","\n","    return tf.SparseTensor(indices, values, tf.shape(dense_tensor, out_type=tf.int64))\n","\n","\n","def batch_edit_distance(y_true, y_logits):\n","    blank = Constants.LABEL_PAD\n","    B = tf.shape(y_logits)[0]\n","    seq_length = tf.shape(y_logits)[1]\n","    to_decode = tf.transpose(y_logits, perm=[1, 0, 2])\n","    sequence_length = tf.fill(dims=[B], value=seq_length)\n","    hypothesis = tf.nn.ctc_greedy_decoder(\n","        tf.cast(to_decode, tf.float32), sequence_length, blank_index=blank\n","    )[0][\n","        0\n","    ]  # full is [B,...]\n","    truth = sparse_from_dense_ignore_value(y_true)  # full is [B,...]\n","    truth = tf.cast(truth, tf.int64)\n","    edit_dist = tf.edit_distance(hypothesis, truth, normalize=False)\n","\n","    non_ignore_mask = tf.not_equal(y_true, blank)\n","    N = tf.reduce_sum(tf.cast(non_ignore_mask, tf.float32))\n","    D = tf.reduce_sum(edit_dist)\n","    result = (N - D) / N\n","    result = tf.clip_by_value(result, 0.0, 1.0)\n","    return result, N, D\n","\n","\n","class LevDistanceMetric(tf.keras.metrics.Metric):\n","    def __init__(self, name=\"Lev\", **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        self.distance = self.add_weight(name=\"dist\", initializer=\"zeros\")\n","        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n","\n","    def update_state(self, y_true, y_logits, sample_weight=None):\n","        # if using with keras compile, make sure the model outputs logits, not softmax probabilities\n","        _, N, D = batch_edit_distance(y_true, y_logits)\n","        self.distance.assign_add(D)\n","        self.count.assign_add(N)\n","\n","    def result(self):\n","        result = (self.count - self.distance) / self.count\n","        result = tf.clip_by_value(result, 0.0, 1.0)\n","        return result\n","\n","    def reset_state(self):\n","        self.count.assign(0.0)\n","        self.distance.assign(0.0)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.969375Z","iopub.status.busy":"2023-07-18T09:04:43.968934Z","iopub.status.idle":"2023-07-18T09:04:44.024972Z","shell.execute_reply":"2023-07-18T09:04:44.023605Z","shell.execute_reply.started":"2023-07-18T09:04:43.969338Z"},"trusted":true},"outputs":[],"source":["\n","class CTCLoss(tf.keras.losses.Loss):\n","    def __init__(self, pad_token_idx):\n","        self.pad_token_idx = pad_token_idx\n","        super().__init__()\n","\n","    def call(self, labels, logits):\n","        label_length = tf.reduce_sum(tf.cast(labels != self.pad_token_idx, tf.int32), axis=-1)\n","        logit_length = tf.ones(tf.shape(logits)[0], dtype=tf.int32) * tf.shape(logits)[1]\n","\n","        ctc_loss = tf.nn.ctc_loss(\n","            labels=labels,\n","            logits=logits,\n","            label_length=label_length,\n","            logit_length=logit_length,\n","            blank_index=self.pad_token_idx,\n","            logits_time_major=False,\n","        )\n","\n","        return ctc_loss\n","\n","\n","class ECA(tf.keras.layers.Layer):\n","    # Efficient Channel Attention\n","    def __init__(self, kernel_size=5, **kwargs):\n","        super().__init__(**kwargs)\n","        self.supports_masking = True\n","        self.kernel_size = kernel_size\n","        self.conv = tf.keras.layers.Conv1D(\n","            1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False\n","        )\n","\n","    def call(self, inputs, mask=None):\n","        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n","        nn = tf.expand_dims(nn, -1)\n","        nn = self.conv(nn)\n","        nn = tf.squeeze(nn, -1)\n","        nn = tf.nn.sigmoid(nn)\n","        nn = nn[:, None, :]\n","        return inputs * nn\n","\n","\n","class LateDropout(tf.keras.layers.Layer):\n","    def __init__(self, rate, noise_shape=None, start_step=0, **kwargs):\n","        super().__init__(**kwargs)\n","        self.supports_masking = True\n","        self.rate = rate\n","        self.start_step = start_step\n","        self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n","\n","    def build(self, input_shape):\n","        super().build(input_shape)\n","        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n","        self._train_counter = tf.Variable(0, dtype=\"int64\", aggregation=agg, trainable=False)\n","\n","    def call(self, inputs, training=False):\n","        x = tf.cond(\n","            self._train_counter < self.start_step,\n","            lambda: inputs,\n","            lambda: self.dropout(inputs, training=training),\n","        )\n","        if training:\n","            self._train_counter.assign_add(1)\n","        return x\n","\n","\n","class CausalDWConv1D(tf.keras.layers.Layer):\n","    # Causal Depth Wise Convolution\n","    def __init__(\n","        self,\n","        kernel_size=17,\n","        dilation_rate=1,\n","        use_bias=False,\n","        depthwise_initializer=\"glorot_uniform\",\n","        name=\"\",\n","        **kwargs,\n","    ):\n","        super().__init__(name=name, **kwargs)\n","        self.causal_pad = tf.keras.layers.ZeroPadding1D(\n","            (dilation_rate * (kernel_size - 1), 0), name=name + \"_pad\"\n","        )\n","        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n","            kernel_size,\n","            strides=1,\n","            dilation_rate=dilation_rate,\n","            padding=\"valid\",\n","            use_bias=use_bias,\n","            depthwise_initializer=depthwise_initializer,\n","            name=name + \"_dwconv\",\n","        )\n","        self.supports_masking = True\n","\n","    def call(self, inputs):\n","        x = self.causal_pad(inputs)\n","        x = self.dw_conv(x)\n","        return x\n","\n","\n","def Conv1DBlock(\n","    channel_size,\n","    kernel_size,\n","    dilation_rate=1,\n","    drop_rate=0.0,\n","    expand_ratio=2,\n","    # se_ratio=0.25,\n","    activation=\"swish\",\n","    name=None,\n","):\n","    \"\"\"\n","    efficient conv1d block, @hoyso48\n","    \"\"\"\n","    if name is None:\n","        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n","\n","    # Expansion phase\n","    def apply(inputs):\n","        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n","        channels_expand = channels_in * expand_ratio\n","\n","        skip = inputs\n","\n","        x = tf.keras.layers.Dense(\n","            channels_expand, use_bias=True, activation=activation, name=name + \"_expand_conv\"\n","        )(inputs)\n","\n","        # Depthwise Convolution\n","        x = CausalDWConv1D(\n","            kernel_size, dilation_rate=dilation_rate, use_bias=False, name=name + \"_dwconv\"\n","        )(x)\n","\n","        x = tf.keras.layers.LayerNormalization(name=name + \"_bn\")(x)\n","\n","        x = ECA()(x)  # efficient channel attention\n","\n","        x = tf.keras.layers.Dense(channel_size, use_bias=True, name=name + \"_project_conv\")(x)\n","\n","        if drop_rate > 0:\n","            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + \"_drop\")(x)\n","\n","        if channels_in == channel_size:\n","            x = tf.keras.layers.add([x, skip], name=name + \"_add\")\n","        return x\n","\n","    return apply\n","\n","\n","class MultiHeadSelfAttention(tf.keras.layers.Layer):\n","    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n","        super().__init__(**kwargs)\n","        self.dim = dim\n","        self.scale = self.dim**-0.5\n","        self.num_heads = num_heads\n","        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n","        self.drop1 = tf.keras.layers.Dropout(dropout)\n","        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        qkv = self.qkv(inputs)\n","        qkv = tf.keras.layers.Permute((2, 1, 3))(\n","            tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv)\n","        )\n","        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n","\n","        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n","\n","        if mask is not None:\n","            mask = mask[:, None, None, :]\n","\n","        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n","        attn = self.drop1(attn)\n","\n","        x = attn @ v\n","        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n","        x = self.proj(x)\n","        return x\n","\n","\n","def TransformerBlock(\n","    dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation=\"swish\"\n","):\n","    def apply(inputs):\n","        x = inputs\n","        x = tf.keras.layers.LayerNormalization()(x)\n","        x = MultiHeadSelfAttention(dim=dim, num_heads=num_heads, dropout=attn_dropout)(x)\n","        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1))(x)\n","        x = tf.keras.layers.Add()([inputs, x])\n","        attn_out = x\n","\n","        x = tf.keras.layers.LayerNormalization()(x)\n","        x = tf.keras.layers.Dense(dim * expand, use_bias=False, activation=activation)(x)\n","        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n","        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1))(x)\n","        x = tf.keras.layers.Add()([attn_out, x])\n","        return x\n","\n","    return apply\n","\n","\n","def build_model1(\n","    output_dim,\n","    max_len=64,\n","    dropout_step=0,\n","    dim=192,\n","    input_pad=-100,\n","    with_transformer=False,\n","    drop_rate=0.2,\n","):\n","    inp = tf.keras.Input(shape=(max_len, Constants.CHANNELS), dtype=tf.float32, name=\"inputs\")\n","    x = tf.keras.layers.Masking(mask_value=input_pad, input_shape=(max_len, Constants.CHANNELS))(\n","        inp\n","    )\n","    ksize = 17\n","    x = tf.keras.layers.Dense(dim, use_bias=False, name=\"stem_conv\")(x)\n","    x = tf.keras.layers.LayerNormalization(name=\"stem_bn\")(x)\n","\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    if with_transformer:\n","        x = TransformerBlock(dim, expand=2)(x)\n","\n","    x = tf.keras.layers.AvgPool1D(2, 2)(x)\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    if with_transformer:\n","        x = TransformerBlock(dim, expand=2)(x)\n","\n","    if dim == 384:  # for the 4x sized model\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        if with_transformer:\n","            x = TransformerBlock(dim, expand=2)(x)\n","\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        if with_transformer:\n","            x = TransformerBlock(dim, expand=2)(x)\n","\n","    lstm = tf.keras.layers.LSTM(units=output_dim, return_sequences=True)\n","    x = tf.keras.layers.Bidirectional(lstm)(x)\n","    # x = LateDropout(0.8, start_step=dropout_step)(x)\n","    # x = tf.keras.layers.LayerNormalization()(x)\n","\n","    outputs = tf.keras.layers.Dense(output_dim, activation=\"log_softmax\")(x)  # logits\n","\n","    # x = tf.keras.layers.Dense(output_dim)(x)  # logits\n","    # outputs = tf.keras.layers.Activation(\"log_softmax\", dtype=\"float32\")(x)\n","    model = tf.keras.Model(inp, outputs)\n","    return model\n","\n","\n","def get_model(output_dim, max_len, dim, input_pad):\n","   \n","    model = build_model1(output_dim, max_len=max_len, input_pad=input_pad, dim=dim)\n","    return model\n"]},{"cell_type":"markdown","metadata":{},"source":["# Configuration"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.026971Z","iopub.status.busy":"2023-07-18T09:04:44.026546Z","iopub.status.idle":"2023-07-18T09:04:44.044853Z","shell.execute_reply":"2023-07-18T09:04:44.043616Z","shell.execute_reply.started":"2023-07-18T09:04:44.026941Z"},"trusted":true},"outputs":[],"source":["\n","def get_strategy():\n","    logical_devices = tf.config.list_logical_devices()\n","    # Check if TPU is available\n","\n","    gpu_available = any(\"GPU\" in device.name for device in logical_devices)\n","    strategy = None\n","    is_tpu = False\n","    try:\n","        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","        print(\"Running on TPU \", tpu.master())\n","        is_tpu = True\n","    except ValueError:\n","        is_tpu = False\n","\n","    if is_tpu:\n","        tf.config.experimental_connect_to_cluster(tpu)\n","        strategy = tf.distribute.TPUStrategy(tpu)\n","        disable_eager_execution()  # LSTM layer can't use bfloat16 unless we do this.\n","\n","    else:\n","        if gpu_available:\n","          \n","            ngpu = len(gpus)\n","            print(\"Num GPUs Available: \", ngpu)\n","            if ngpu > 1:\n","                strategy = tf.distribute.MirroredStrategy()\n","            else:\n","                strategy = tf.distribute.get_strategy()\n","\n","        else:\n","            print(\"Runing on CPU\")\n","            strategy = tf.distribute.get_strategy()\n","    replicas = strategy.num_replicas_in_sync\n","\n","    print(f\"get strategy replicas: {replicas}\")\n","\n","    return strategy, replicas, is_tpu\n","\n","\n","class CFG:\n","    # These 3 variables are update dynamically later by calling update_config_with_strategy.\n","    strategy = None  # type: ignore\n","    replicas = 1\n","    is_tpu = False\n","\n","    save_output = True\n","    log_path = \"/kaggle/working/\"\n","    input_path = \"/kaggle/input/asl-fingerspelling/\"\n","    output_path = \"/kaggle/working/\"\n","\n","    seed = 42\n","    verbose = 1  # 0) silent 1) progress bar 2) one line per epoch\n","\n","    # max number of frames\n","    #max_len = 256\n","    max_len = 256\n","    replicas = 1\n","    \n","    lr = 5e-4   # 5e-4\n","    weight_decay = 1e-4  # 4e-4\n","    epochs = 80 \n","    batch_size=128\n","    snapshot_epochs = []  # type: ignore\n","    swa_epochs = []  # type: ignore\n","    # list(range(epoch//2,epoch+1))\n","\n","    fp16 = True\n","    \n","    awp = False\n","    awp_lambda = 0.2\n","    awp_start_epoch = 15\n","    dropout_start_epoch = 15\n","    resume = 0\n","    \n","    dim = 384\n","    \n","    comment = f\"model-{dim}-seed{seed}\"\n","    output_dim = 61\n","    num_eval = 6\n","\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.046817Z","iopub.status.busy":"2023-07-18T09:04:44.046403Z","iopub.status.idle":"2023-07-18T09:04:44.063046Z","shell.execute_reply":"2023-07-18T09:04:44.062007Z","shell.execute_reply.started":"2023-07-18T09:04:44.046786Z"},"trusted":true},"outputs":[],"source":["\n","def update_config_with_strategy(config):\n","    # cfg is configuration instance\n","    strategy, replicas, is_tpu = get_strategy()\n","    config.strategy = strategy\n","    config.replicas = replicas\n","    config.is_tpu = is_tpu\n","    config.lr = config.lr * replicas\n","    config.batch_size = config.batch_size * replicas\n","    return config"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.065242Z","iopub.status.busy":"2023-07-18T09:04:44.064883Z","iopub.status.idle":"2023-07-18T09:04:44.114564Z","shell.execute_reply":"2023-07-18T09:04:44.112776Z","shell.execute_reply.started":"2023-07-18T09:04:44.065211Z"},"trusted":true},"outputs":[],"source":["\n","def count_data_items(dataset):\n","    dataset_size = 0\n","    for _ in dataset:\n","        dataset_size += 1\n","    return dataset_size\n","\n","\n","def interp1d_(x, target_len):\n","    target_len = tf.maximum(1, target_len)\n","    x = tf.image.resize(x, (target_len, tf.shape(x)[1]))\n","    return x\n","\n","\n","def tf_nan_mean(x, axis=0, keepdims=False):\n","    return tf.reduce_sum(\n","        tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims\n","    ) / tf.reduce_sum(\n","        tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims\n","    )\n","\n","\n","def tf_nan_std(x, center=None, axis=0, keepdims=False):\n","    if center is None:\n","        center = tf_nan_mean(x, axis=axis, keepdims=True)\n","    d = x - center\n","    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n","\n","\n","def flip_lr(x):\n","    if x.shape[1] == Constants.ROWS_PER_FRAME:\n","        LHAND = Constants.LHAND\n","        RHAND = Constants.RHAND\n","        LLIP = Constants.LLIP\n","        RLIP = Constants.RLIP\n","        LEYE = Constants.LEYE\n","        REYE = Constants.REYE\n","        LNOSE = Constants.LNOSE\n","        RNOSE = Constants.RNOSE\n","        LPOSE = Constants.LPOSE\n","        RPOSE = Constants.RPOSE\n","    else:\n","        LHAND = Constants.LANDMARK_INDICES[\"LHAND\"]\n","        RHAND = Constants.LANDMARK_INDICES[\"RHAND\"]\n","        LLIP = Constants.LANDMARK_INDICES[\"LLIP\"]\n","        RLIP = Constants.LANDMARK_INDICES[\"RLIP\"]\n","        LEYE = Constants.LANDMARK_INDICES[\"LEYE\"]\n","        REYE = Constants.LANDMARK_INDICES[\"REYE\"]\n","        LNOSE = Constants.LANDMARK_INDICES[\"LNOSE\"]\n","        RNOSE = Constants.LANDMARK_INDICES[\"RNOSE\"]\n","        LPOSE = Constants.LANDMARK_INDICES[\"LPOSE\"]\n","        RPOSE = Constants.LANDMARK_INDICES[\"RPOSE\"]\n","\n","    x, y = tf.unstack(x, axis=-1)\n","    x = 1 - x\n","    new_x = tf.stack([x, y], -1)\n","    new_x = tf.transpose(new_x, [1, 0, 2])\n","    lhand = tf.gather(new_x, LHAND, axis=0)\n","    rhand = tf.gather(new_x, RHAND, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LHAND)[..., None], rhand)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RHAND)[..., None], lhand)\n","    llip = tf.gather(new_x, LLIP, axis=0)\n","    rlip = tf.gather(new_x, RLIP, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LLIP)[..., None], rlip)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RLIP)[..., None], llip)\n","    lpose = tf.gather(new_x, LPOSE, axis=0)\n","    rpose = tf.gather(new_x, RPOSE, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LPOSE)[..., None], rpose)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RPOSE)[..., None], lpose)\n","    leye = tf.gather(new_x, LEYE, axis=0)\n","    reye = tf.gather(new_x, REYE, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LEYE)[..., None], reye)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(REYE)[..., None], leye)\n","    lnose = tf.gather(new_x, LNOSE, axis=0)\n","    rnose = tf.gather(new_x, RNOSE, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LNOSE)[..., None], rnose)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RNOSE)[..., None], lnose)\n","    new_x = tf.transpose(new_x, [1, 0, 2])\n","    return new_x\n","\n","\n","def resample(x, rate=(0.8, 1.2)):\n","    rate = tf.random.uniform((), rate[0], rate[1])\n","    length = tf.shape(x)[0]\n","    new_size = tf.cast(rate * tf.cast(length, tf.float32), tf.int32)\n","    new_x = interp1d_(x, new_size)\n","    return new_x\n","\n","\n","def spatial_random_affine(\n","    xyz,\n","    scale=(0.8, 1.2),\n","    shear=(-0.1, 0.1),\n","    shift=(-0.1, 0.1),\n","    degree=(-20, 20),\n","):\n","    center = tf.constant([0.5, 0.5])\n","    if degree is not None:\n","        xy = xyz[..., :2]\n","        z = xyz[..., 2:]\n","        xy -= center\n","        degree = tf.random.uniform((), *degree)\n","        radian = degree / 180 * np.pi\n","        c = tf.math.cos(radian)\n","        s = tf.math.sin(radian)\n","        rotate_mat = tf.identity(\n","            [\n","                [c, s],\n","                [-s, c],\n","            ]\n","        )\n","        xy = xy @ rotate_mat\n","        xy = xy + center\n","        xyz = tf.concat([xy, z], axis=-1)\n","\n","    if scale is not None:\n","        scale = tf.random.uniform((), *scale)\n","        xyz = scale * xyz\n","\n","    if shear is not None:\n","        xy = xyz[..., :2]\n","        z = xyz[..., 2:]\n","        shear_x = shear_y = tf.random.uniform((), *shear)\n","        if tf.random.uniform(()) < 0.5:\n","            shear_x = 0.0\n","        else:\n","            shear_y = 0.0\n","        shear_mat = tf.identity([[1.0, shear_x], [shear_y, 1.0]])\n","        xy = xy @ shear_mat\n","        xyz = tf.concat([xy, z], axis=-1)\n","\n","    if shift is not None:\n","        shift = tf.random.uniform((), *shift)\n","        xyz = xyz + shift\n","\n","    return xyz\n","\n","\n","def temporal_mask(x, size=[1, 10], mask_value=float(\"nan\")):\n","    l0 = tf.shape(x)[0]\n","    if size[1] > l0 // 8:\n","        size[1] = l0 // 8\n","        if size[1] <= 1:\n","            size[1] = 2\n","    mask_size = tf.random.uniform((), *size, dtype=tf.int32)\n","    mask_offset = tf.random.uniform((), 0, tf.clip_by_value(l0 - mask_size, 1, l0), dtype=tf.int32)\n","    x = tf.tensor_scatter_nd_update(\n","        x,\n","        tf.range(mask_offset, mask_offset + mask_size)[..., None],\n","        tf.fill([mask_size, tf.shape(x)[1], 2], mask_value),\n","    )\n","    return x\n","\n","\n","def spatial_mask(x, size=(0.05, 0.2), mask_value=float(\"nan\")):\n","    mask_offset_y = tf.random.uniform(())\n","    mask_offset_x = tf.random.uniform(())\n","    mask_size = tf.random.uniform((), *size)\n","    mask_x = (mask_offset_x < x[..., 0]) & (x[..., 0] < mask_offset_x + mask_size)\n","    mask_y = (mask_offset_y < x[..., 1]) & (x[..., 1] < mask_offset_y + mask_size)\n","    mask = mask_x & mask_y\n","    x = tf.where(mask[..., None], mask_value, x)\n","    return x\n","\n","\n","#@tf.function()\n","def augment_fn(x):\n","    # shape (T,F)\n","    x = tf.reshape(x, (tf.shape(x)[0], -1, 2))\n","    if tf.random.uniform(()) < 0.4:\n","        x = resample(x, (0.5, 1.5))\n","    if tf.random.uniform(()) < 0.4:\n","        x = flip_lr(x)\n","    if tf.random.uniform(()) < 0.4:\n","        x = spatial_random_affine(x)\n","    if tf.random.uniform(()) < 0.2:\n","        x = temporal_mask(x)\n","    if tf.random.uniform(()) < 0.2:\n","        x = spatial_mask(x)\n","    x = tf.reshape(x, (tf.shape(x)[0], -1))\n","    return x\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.117655Z","iopub.status.busy":"2023-07-18T09:04:44.117104Z","iopub.status.idle":"2023-07-18T09:04:44.142302Z","shell.execute_reply":"2023-07-18T09:04:44.140662Z","shell.execute_reply.started":"2023-07-18T09:04:44.117609Z"},"trusted":true},"outputs":[],"source":["\n","class Preprocess(tf.keras.layers.Layer):\n","    def __init__(self, max_len, normalize=False, **kwargs):\n","        super().__init__(**kwargs)\n","        self.max_len = max_len\n","        self.center = Constants.CENTER_INDICES\n","        self.normalize = normalize\n","\n","    # preprocess a batch of data\n","    def call(self, x):\n","        # rank is 3: [B,T,F]\n","        # if your input is just [T,F], extend its dimesnion before calling.\n","\n","        x = tf.reshape(x, (tf.shape(x)[0], tf.shape(x)[1], -1, 2))\n","        # dimensions now are [B,T,F//2,2]\n","\n","        x_selected = x\n","        if self.normalize:\n","            mean = tf_nan_mean(tf.gather(x, self.center, axis=2), axis=[1, 2], keepdims=True)\n","            mean = tf.where(tf.math.is_nan(mean), tf.constant(0.5, x.dtype), mean)\n","            std = tf_nan_std(x_selected, center=mean, axis=[1, 2], keepdims=True)\n","            x = (x_selected - mean) / std\n","        else:\n","            x = x_selected\n","\n","        dx = tf.cond(\n","            tf.shape(x)[1] > 1,\n","            lambda: tf.pad(x[:, 1:] - x[:, :-1], [[0, 0], [0, 1], [0, 0], [0, 0]]),\n","            lambda: tf.zeros_like(x),\n","        )\n","\n","        dx2 = tf.cond(\n","            tf.shape(x)[1] > 2,\n","            lambda: tf.pad(x[:, 2:] - x[:, :-2], [[0, 0], [0, 2], [0, 0], [0, 0]]),\n","            lambda: tf.zeros_like(x),\n","        )\n","        length = tf.shape(x)[1]\n","\n","        x = tf.concat(\n","            [\n","                tf.reshape(x, (-1, length, 2 * Constants.NUM_NODES)),  # x1,y1,x2,y2,...\n","                tf.reshape(dx, (-1, length, 2 * Constants.NUM_NODES)),\n","                tf.reshape(dx2, (-1, length, 2 * Constants.NUM_NODES)),\n","            ],\n","            axis=-1,\n","        )\n","\n","        # x1,y1,x2,y2,...dx1,dy1,dx2,dy2,...\n","        x = tf.where(tf.math.is_nan(x), tf.constant(0.0, x.dtype), x)\n","        return x\n","\n","\n","def pad_if_short(x, max_len):\n","    # shape (T,F)\n","    pad_len = max_len - tf.shape(x)[0]\n","    padding = tf.ones((pad_len, tf.shape(x)[1]), dtype=x.dtype) * Constants.INPUT_PAD\n","    x = tf.concat([x, padding], axis=0)\n","    return x\n","\n","\n","def shrink_if_long(x, max_len):\n","    # shape is [T,F]\n","    if tf.shape(x)[0] > max_len:\n","        # we need to extend the dimension to [T,F,channels]  for tf.image.resize\n","        x = tf.image.resize(x[..., None], (max_len, tf.shape(x)[1]))\n","        x = tf.squeeze(x, axis=2)\n","    return x\n","\n","def preprocess(x, max_len, do_pad=True):\n","    # shape (T,F)\n","    x = shrink_if_long(x, max_len=max_len)\n","    # Preprocess expects a batch, so we extend the dimension to (None,T,F), then reduce the output back to (T,F).\n","    x = tf.cast(Preprocess(max_len=max_len)(x[None, ...])[0], tf.float32)\n","\n","    if do_pad:  # we can avoid this step if there is batch padding\n","        x = pad_if_short(x, max_len=max_len)\n","\n","    return x"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.144369Z","iopub.status.busy":"2023-07-18T09:04:44.143941Z","iopub.status.idle":"2023-07-18T09:04:44.160043Z","shell.execute_reply":"2023-07-18T09:04:44.159041Z","shell.execute_reply.started":"2023-07-18T09:04:44.144335Z"},"trusted":true},"outputs":[],"source":["\n","def decode_tfrec(record_bytes):\n","    features = tf.io.parse_single_example(\n","        record_bytes,\n","        {\n","            \"coordinates\": tf.io.VarLenFeature(tf.float32),\n","            \"label\": tf.io.VarLenFeature(tf.int64),\n","        },\n","    )\n","    coords = tf.sparse.to_dense(features[\"coordinates\"])\n","    coords = tf.reshape(coords, (-1, Constants.NUM_INPUT_FEATURES))\n","    label = tf.sparse.to_dense(features[\"label\"])\n","\n","    return (coords, label)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.167121Z","iopub.status.busy":"2023-07-18T09:04:44.166717Z","iopub.status.idle":"2023-07-18T09:04:44.179273Z","shell.execute_reply":"2023-07-18T09:04:44.17748Z","shell.execute_reply.started":"2023-07-18T09:04:44.167092Z"},"trusted":true},"outputs":[],"source":["\n","def get_dataset(\n","    filenames,\n","    input_path,\n","    max_len,\n","    batch_size=64,\n","    drop_remainder=False,\n","    augment=False,\n","    shuffle_buffer=None,\n","    repeat=False,\n","    use_tfrecords=True,\n","):\n","    ignore_order = tf.data.Options()\n","    ignore_order.experimental_deterministic = False\n","\n","    \n","    ds = tf.data.TFRecordDataset(\n","        filenames, num_parallel_reads=tf.data.AUTOTUNE, compression_type=\"GZIP\"\n","    )\n","    ds = ds.map(decode_tfrec, tf.data.AUTOTUNE)\n","  \n","    ds.with_options(ignore_order)\n","    \n","    if augment:\n","        ds = ds.map(lambda x, y: (augment_fn(x), y), tf.data.AUTOTUNE)\n","    \n","    ds = ds.map(lambda x, y: (preprocess(x, max_len=max_len, do_pad=False), y), tf.data.AUTOTUNE)\n","    #if repeat:\n","    #    ds = ds.repeat()\n","    \n","    if shuffle_buffer is not None:\n","        ds = ds.shuffle(shuffle_buffer)\n","  \n","    ds = ds.padded_batch(\n","        batch_size,\n","        padding_values=(\n","            tf.constant(Constants.INPUT_PAD, dtype=tf.float32),\n","            tf.constant(Constants.LABEL_PAD, dtype=tf.int64),\n","        ),\n","        padded_shapes=([max_len, Constants.CHANNELS], [Constants.MAX_STRING_LEN]),\n","        drop_remainder=drop_remainder,\n","    )\n","    \n","    #tf.data.experimental.assert_cardinality(len(labels) // BATCH_SIZE)\n","\n","    ds = ds.prefetch(tf.data.AUTOTUNE)\n","\n","    return ds\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.182143Z","iopub.status.busy":"2023-07-18T09:04:44.181727Z","iopub.status.idle":"2023-07-18T09:04:44.208936Z","shell.execute_reply":"2023-07-18T09:04:44.207184Z","shell.execute_reply.started":"2023-07-18T09:04:44.182112Z"},"trusted":true},"outputs":[],"source":["\n","def train_run(train_files, valid_files, config, num_train, experiment_id=0, use_tfrecords=True,summary=False):\n","    gc.collect()\n","    tf.keras.backend.clear_session()\n","    # tf.config.optimizer.set_jit(\"autoclustering\")\n","\n","    if config.fp16:\n","        if config.is_tpu:\n","            policy = \"mixed_bfloat16\"\n","        else:\n","            policy = \"mixed_float16\"\n","    else:\n","        policy = \"float32\"\n","    tf.keras.mixed_precision.set_global_policy(policy)\n","\n","    augment_train= True\n","    repeat_train = True\n","\n","    shuffle_buffer = 4096\n","    train_ds = get_dataset(\n","        train_files,\n","        input_path=config.input_path,\n","        max_len=config.max_len,\n","        batch_size=config.batch_size,\n","        drop_remainder=True,\n","        augment=augment_train,\n","        repeat=repeat_train,\n","        shuffle_buffer=shuffle_buffer,\n","        use_tfrecords=True,\n","    )\n","    if valid_files is not None:\n","        valid_ds = get_dataset(\n","            valid_files,\n","            input_path=config.input_path,\n","            max_len=config.max_len,\n","            batch_size=config.batch_size,\n","            use_tfrecords=True,\n","        )\n","    else:\n","        valid_ds = None\n","        valid_files = []\n","\n","    # num_train = count_data_items(train_ds)\n","    # num_valid = count_data_items(valid_ds)\n","    # print(num_train, num_valid, config.batch_size)\n","    # exit()\n","\n","    steps_per_epoch = num_train // config.batch_size\n","    strategy = config.strategy\n","    with strategy.scope():\n","        model = get_model(\n","            max_len=config.max_len,\n","            output_dim=config.output_dim,\n","            input_pad=Constants.INPUT_PAD,\n","            dim=config.dim,\n","        )\n"," \n","        base_lr = config.lr\n","        lr_schedule = CosineDecay(\n","            initial_learning_rate=base_lr / 10,\n","            decay_steps=int(0.95 * steps_per_epoch * config.epochs),\n","            alpha=0.02,\n","            name=None,\n","            warmup_target=base_lr,\n","            warmup_steps=int(0.05 * steps_per_epoch * config.epochs),\n","        )\n","        \n","        opt = tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=config.weight_decay)\n","        #opt = tf.keras.optimizers.AdamW(learning_rate=2e-4)\n","        loss = CTCLoss(pad_token_idx=Constants.LABEL_PAD)\n","\n","        model.compile(\n","            optimizer=opt,\n","            loss=loss,\n","            metrics=[\n","                LevDistanceMetric(),\n","            ],\n","            #jit_compile= not config.is_tpu # Should be False on TPU!!\n","        )\n","\n","    if summary:\n","        print()\n","        model.summary()\n","        print()\n","        print(train_ds, valid_ds)\n","        print()\n","    print(f\"---------experiment {experiment_id}---------\")\n","    print(f\"train:{num_train} \")\n","    print()\n","\n","    if config.resume:\n","        print(f\"resume from epoch{config.resume}\")\n","        model.load_weights(f\"{config.log_path}/{config.comment}-exp{experiment_id}-last.h5\")\n","        if train_ds is not None:\n","            model.evaluate(train_ds.take(steps_per_epoch))\n","        if valid_ds is not None:\n","            model.evaluate(valid_ds)\n","\n","    tb_logger = tf.keras.callbacks.TensorBoard(\n","        log_dir=\"config.log_path\", histogram_freq=0, write_graph=True, write_images=True\n","    )\n","    sv_loss = tf.keras.callbacks.ModelCheckpoint(\n","        f\"{config.log_path}/{config.comment}-exp{experiment_id}-best.h5\",\n","        monitor=\"val_loss\",\n","        verbose=1,\n","        save_best_only=True,\n","        save_weights_only=True,\n","        mode=\"min\",\n","        save_freq=\"epoch\",\n","    )\n","  \n","    # Callback function to check transcription on the val set.\n","    # validation_callback = CallbackEval(model, valid_ds)\n","    memory_usage = MemoryUsageCallbackExtended()\n","    callbacks = []\n","    if config.save_output:\n","        callbacks.append(tb_logger)\n","        # callbacks.append(swa)\n","        callbacks.append(sv_loss)\n","    callbacks.append(memory_usage)\n","    callbacks.append(tf.keras.callbacks.TerminateOnNaN())\n","    # callbacks.append(validation_callback)\n","\n","    history = model.fit(\n","        train_ds,\n","        epochs=config.epochs - config.resume,\n","        #steps_per_epoch=steps_per_epoch,\n","        callbacks=callbacks,\n","        validation_data=valid_ds,\n","        verbose=config.verbose,\n","        # validation_steps=None,\n","    )\n","\n","    if config.save_output:  # reload the saved best weights checkpoint\n","        saved_based_model = f\"{config.log_path}/{config.comment}-exp{experiment_id}-best.h5\"\n","        if os.path.exists(saved_based_model):\n","            model.load_weights(saved_based_model)\n","        else:\n","            print(f\"Warning: could not find {saved_based_model}\")\n","    if valid_ds is not None:\n","        cv = model.evaluate(valid_ds, verbose=config.verbose)\n","    else:\n","        cv = None\n","    return model, cv, history\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.210911Z","iopub.status.busy":"2023-07-18T09:04:44.21051Z","iopub.status.idle":"2023-07-18T09:04:44.227683Z","shell.execute_reply":"2023-07-18T09:04:44.226169Z","shell.execute_reply.started":"2023-07-18T09:04:44.21088Z"},"trusted":true},"outputs":[],"source":["\n","def train(cfg=CFG, experiment_id=0, use_supplemental=True):\n","    tf.keras.backend.clear_session()\n","    config = cfg()\n","    update_config_with_strategy(config)\n","    print(f\"using {config.replicas} replicas\")\n","    print(f\"batch size {config.batch_size}\")\n","    print(f\"fp16={config.fp16}\")\n","    seed_everything(config.seed)\n","    \n","    all_filenames = sorted(glob.glob(\"/kaggle/input/asl-preprocessing/records/*.tfrecord\"))\n","    regular = [x for x in all_filenames if \"supp\" not in x]\n","    supp = [x for x in all_filenames if \"supp\" in x]\n","    \n","    data_filenames = regular\n","    if use_supplemental:\n","        data_filenames += supp\n","    print(\"Using TFRECORDS\")\n","    \n","  \n","    valid_files = data_filenames[: config.num_eval]  # first part in list\n","    train_files = data_filenames[config.num_eval :]\n","    random.shuffle(train_files)\n","    \n","    \n","    df1 = pd.read_csv(config.input_path + \"train.csv\")\n","    df2 = pd.read_csv(config.input_path + \"supplemental_metadata.csv\")\n","    df_info = pd.concat([df1, df2])\n","    \n","    #ds = get_dataset(train_files, CFG.input_path,max_len=CFG.max_len, augment=False, batch_size=64)\n","    #print(ds)\n","    #for x,y in ds:\n","    #    print(x,y)\n","    #raise\n","    \n","    if use_supplemental:\n","        num_train = 3567 * 32  # with supplemental\n","    else:\n","        num_train = 1912 * 32  # without supplemental\n","  \n","    train_run(\n","        train_files,\n","        valid_files,\n","        config,\n","        num_train,\n","        summary=False,\n","        experiment_id=experiment_id,\n","        use_tfrecords=True,\n","    )\n","    \n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.231071Z","iopub.status.busy":"2023-07-18T09:04:44.229724Z","iopub.status.idle":"2023-07-18T09:04:44.497144Z","shell.execute_reply":"2023-07-18T09:04:44.495773Z","shell.execute_reply.started":"2023-07-18T09:04:44.231015Z"},"trusted":true},"outputs":[],"source":["gc.collect()\n","tf.keras.backend.clear_session()"]},{"cell_type":"markdown","metadata":{},"source":["# Train It!"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.499483Z","iopub.status.busy":"2023-07-18T09:04:44.499005Z","iopub.status.idle":"2023-07-18T09:04:44.505927Z","shell.execute_reply":"2023-07-18T09:04:44.504277Z","shell.execute_reply.started":"2023-07-18T09:04:44.49944Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-07-18 20:07:28.997593: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-18 20:07:28.997826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-18 20:07:28.997979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-18 20:07:29.062861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-18 20:07:29.063000: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-18 20:07:29.063095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-18 20:07:29.063167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6000 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"]},{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  1\n","get strategy replicas: 1\n","using 1 replicas\n","batch size 128\n","fp16=True\n","Using TFRECORDS\n","INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n","Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3060 Ti, compute capability 8.6\n"]},{"name":"stderr","output_type":"stream","text":["2023-07-18 20:07:29.273072: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"]},{"name":"stdout","output_type":"stream","text":["---------experiment 0---------\n","train:114144 \n","\n","Epoch 1/80\n","WARNING:tensorflow:From /home/sronen/code/.venv/lib/python3.10/site-packages/tensorflow/python/ops/ctc_ops.py:1514: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\n","WARNING:tensorflow:From /home/sronen/code/.venv/lib/python3.10/site-packages/tensorflow/python/ops/ctc_ops.py:1497: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\n"]},{"name":"stderr","output_type":"stream","text":["2023-07-18 20:07:46.812989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8801\n","2023-07-18 20:07:46.932745: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"]},{"name":"stdout","output_type":"stream","text":["      5/Unknown - 17s 354ms/step - loss: 420.5000 - Lev: 0.0000e+00"]},{"name":"stderr","output_type":"stream","text":["2023-07-18 20:07:49.770777: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f090f8b74c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2023-07-18 20:07:49.770800: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Ti, Compute Capability 8.6\n","2023-07-18 20:07:49.774916: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","2023-07-18 20:07:49.826909: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n","2023-07-18 20:07:49.869338: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"]},{"name":"stdout","output_type":"stream","text":["    891/Unknown - 343s 368ms/step - loss: 85.8993 - Lev: 0.0069"]},{"name":"stderr","output_type":"stream","text":["2023-07-18 20:13:15.583406: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16253687722023668478\n","2023-07-18 20:13:15.583435: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14285955576222201784\n","2023-07-18 20:13:15.583442: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10843003463023075631\n","2023-07-18 20:13:15.583448: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7384413572668943933\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 1: val_loss improved from inf to 66.36849, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n"]},{"name":"stderr","output_type":"stream","text":["2023-07-18 20:13:27.453018: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6890535081491427376\n","2023-07-18 20:13:27.453050: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10395432177698698251\n","2023-07-18 20:13:27.453066: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15529229740936111770\n","2023-07-18 20:13:27.453077: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11698758912837572279\n"]},{"name":"stdout","output_type":"stream","text":["Memory usage on epoch end: 5.10 GB\n","891/891 [==============================] - 356s 382ms/step - loss: 85.8993 - Lev: 0.0069 - val_loss: 66.3685 - val_Lev: 0.0240\n","Epoch 2/80\n","891/891 [==============================] - ETA: 0s - loss: 77.5680 - Lev: 0.0320\n","Epoch 2: val_loss improved from 66.36849 to 65.81821, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.63 GB\n","891/891 [==============================] - 334s 373ms/step - loss: 77.5680 - Lev: 0.0320 - val_loss: 65.8182 - val_Lev: 0.0394\n","Epoch 3/80\n","891/891 [==============================] - ETA: 0s - loss: 75.2758 - Lev: 0.0678\n","Epoch 3: val_loss improved from 65.81821 to 63.68873, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.71 GB\n","891/891 [==============================] - 339s 379ms/step - loss: 75.2758 - Lev: 0.0678 - val_loss: 63.6887 - val_Lev: 0.0347\n","Epoch 4/80\n","891/891 [==============================] - ETA: 0s - loss: 71.1295 - Lev: 0.1319\n","Epoch 4: val_loss improved from 63.68873 to 57.51970, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.62 GB\n","891/891 [==============================] - 341s 381ms/step - loss: 71.1295 - Lev: 0.1319 - val_loss: 57.5197 - val_Lev: 0.1185\n","Epoch 5/80\n","891/891 [==============================] - ETA: 0s - loss: 67.5372 - Lev: 0.1662\n","Epoch 5: val_loss improved from 57.51970 to 54.58958, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.62 GB\n","891/891 [==============================] - 344s 384ms/step - loss: 67.5372 - Lev: 0.1662 - val_loss: 54.5896 - val_Lev: 0.1411\n","Epoch 6/80\n","891/891 [==============================] - ETA: 0s - loss: 64.0001 - Lev: 0.1946\n","Epoch 6: val_loss improved from 54.58958 to 48.39626, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.41 GB\n","891/891 [==============================] - 340s 380ms/step - loss: 64.0001 - Lev: 0.1946 - val_loss: 48.3963 - val_Lev: 0.1971\n","Epoch 7/80\n","891/891 [==============================] - ETA: 0s - loss: 46.4201 - Lev: 0.3973\n","Epoch 7: val_loss improved from 48.39626 to 29.96429, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.59 GB\n","891/891 [==============================] - 342s 383ms/step - loss: 46.4201 - Lev: 0.3973 - val_loss: 29.9643 - val_Lev: 0.5431\n","Epoch 8/80\n","891/891 [==============================] - ETA: 0s - loss: 36.9216 - Lev: 0.5386\n","Epoch 8: val_loss improved from 29.96429 to 27.06764, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.60 GB\n","891/891 [==============================] - 340s 380ms/step - loss: 36.9216 - Lev: 0.5386 - val_loss: 27.0676 - val_Lev: 0.5996\n","Epoch 9/80\n","891/891 [==============================] - ETA: 0s - loss: 34.2016 - Lev: 0.5749\n","Epoch 9: val_loss improved from 27.06764 to 25.12347, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.67 GB\n","891/891 [==============================] - 335s 374ms/step - loss: 34.2016 - Lev: 0.5749 - val_loss: 25.1235 - val_Lev: 0.6194\n","Epoch 10/80\n","891/891 [==============================] - ETA: 0s - loss: 32.6204 - Lev: 0.5948\n","Epoch 10: val_loss improved from 25.12347 to 24.22880, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.86 GB\n","891/891 [==============================] - 336s 375ms/step - loss: 32.6204 - Lev: 0.5948 - val_loss: 24.2288 - val_Lev: 0.6323\n","Epoch 11/80\n","891/891 [==============================] - ETA: 0s - loss: 31.3799 - Lev: 0.6107\n","Epoch 11: val_loss improved from 24.22880 to 23.01206, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.04 GB\n","891/891 [==============================] - 338s 377ms/step - loss: 31.3799 - Lev: 0.6107 - val_loss: 23.0121 - val_Lev: 0.6510\n","Epoch 12/80\n","891/891 [==============================] - ETA: 0s - loss: 30.3816 - Lev: 0.6236\n","Epoch 12: val_loss improved from 23.01206 to 22.77960, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.60 GB\n","891/891 [==============================] - 341s 381ms/step - loss: 30.3816 - Lev: 0.6236 - val_loss: 22.7796 - val_Lev: 0.6579\n","Epoch 13/80\n","891/891 [==============================] - ETA: 0s - loss: 29.5661 - Lev: 0.6340\n","Epoch 13: val_loss improved from 22.77960 to 21.73123, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.50 GB\n","891/891 [==============================] - 344s 384ms/step - loss: 29.5661 - Lev: 0.6340 - val_loss: 21.7312 - val_Lev: 0.6704\n","Epoch 14/80\n","891/891 [==============================] - ETA: 0s - loss: 28.9091 - Lev: 0.6427\n","Epoch 14: val_loss improved from 21.73123 to 21.43986, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.78 GB\n","891/891 [==============================] - 341s 380ms/step - loss: 28.9091 - Lev: 0.6427 - val_loss: 21.4399 - val_Lev: 0.6770\n","Epoch 15/80\n","891/891 [==============================] - ETA: 0s - loss: 28.2151 - Lev: 0.6516\n","Epoch 15: val_loss improved from 21.43986 to 21.20751, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.55 GB\n","891/891 [==============================] - 336s 376ms/step - loss: 28.2151 - Lev: 0.6516 - val_loss: 21.2075 - val_Lev: 0.6766\n","Epoch 16/80\n","891/891 [==============================] - ETA: 0s - loss: 27.6497 - Lev: 0.6585\n","Epoch 16: val_loss improved from 21.20751 to 20.75104, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.74 GB\n","891/891 [==============================] - 338s 377ms/step - loss: 27.6497 - Lev: 0.6585 - val_loss: 20.7510 - val_Lev: 0.6888\n","Epoch 17/80\n","891/891 [==============================] - ETA: 0s - loss: 27.1070 - Lev: 0.6661\n","Epoch 17: val_loss improved from 20.75104 to 20.59911, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.65 GB\n","891/891 [==============================] - 333s 372ms/step - loss: 27.1070 - Lev: 0.6661 - val_loss: 20.5991 - val_Lev: 0.6919\n","Epoch 18/80\n","891/891 [==============================] - ETA: 0s - loss: 26.6269 - Lev: 0.6716\n","Epoch 18: val_loss improved from 20.59911 to 19.95934, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.06 GB\n","891/891 [==============================] - 339s 379ms/step - loss: 26.6269 - Lev: 0.6716 - val_loss: 19.9593 - val_Lev: 0.6989\n","Epoch 19/80\n","891/891 [==============================] - ETA: 0s - loss: 26.1845 - Lev: 0.6777\n","Epoch 19: val_loss improved from 19.95934 to 19.87767, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.45 GB\n","891/891 [==============================] - 339s 379ms/step - loss: 26.1845 - Lev: 0.6777 - val_loss: 19.8777 - val_Lev: 0.6997\n","Epoch 20/80\n","891/891 [==============================] - ETA: 0s - loss: 25.7070 - Lev: 0.6835\n","Epoch 20: val_loss did not improve from 19.87767\n","Memory usage on epoch end: 6.52 GB\n","891/891 [==============================] - 333s 372ms/step - loss: 25.7070 - Lev: 0.6835 - val_loss: 19.9437 - val_Lev: 0.7049\n","Epoch 21/80\n","891/891 [==============================] - ETA: 0s - loss: 25.4054 - Lev: 0.6870\n","Epoch 21: val_loss improved from 19.87767 to 19.45091, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.67 GB\n","891/891 [==============================] - 337s 376ms/step - loss: 25.4054 - Lev: 0.6870 - val_loss: 19.4509 - val_Lev: 0.7122\n","Epoch 22/80\n","891/891 [==============================] - ETA: 0s - loss: 24.9623 - Lev: 0.6924\n","Epoch 22: val_loss improved from 19.45091 to 19.41870, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.65 GB\n","891/891 [==============================] - 335s 374ms/step - loss: 24.9623 - Lev: 0.6924 - val_loss: 19.4187 - val_Lev: 0.7123\n","Epoch 23/80\n","891/891 [==============================] - ETA: 0s - loss: 24.6549 - Lev: 0.6962\n","Epoch 23: val_loss improved from 19.41870 to 19.20846, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.59 GB\n","891/891 [==============================] - 349s 390ms/step - loss: 24.6549 - Lev: 0.6962 - val_loss: 19.2085 - val_Lev: 0.7160\n","Epoch 24/80\n","891/891 [==============================] - ETA: 0s - loss: 24.3701 - Lev: 0.7000\n","Epoch 24: val_loss did not improve from 19.20846\n","Memory usage on epoch end: 6.88 GB\n","891/891 [==============================] - 339s 379ms/step - loss: 24.3701 - Lev: 0.7000 - val_loss: 19.3228 - val_Lev: 0.7162\n","Epoch 25/80\n","891/891 [==============================] - ETA: 0s - loss: 23.9465 - Lev: 0.7052\n","Epoch 25: val_loss improved from 19.20846 to 18.87995, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.77 GB\n","891/891 [==============================] - 345s 386ms/step - loss: 23.9465 - Lev: 0.7052 - val_loss: 18.8800 - val_Lev: 0.7209\n","Epoch 26/80\n","891/891 [==============================] - ETA: 0s - loss: 23.6737 - Lev: 0.7078\n","Epoch 26: val_loss improved from 18.87995 to 18.77861, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.58 GB\n","891/891 [==============================] - 335s 374ms/step - loss: 23.6737 - Lev: 0.7078 - val_loss: 18.7786 - val_Lev: 0.7199\n","Epoch 27/80\n","891/891 [==============================] - ETA: 0s - loss: 23.3368 - Lev: 0.7124\n","Epoch 27: val_loss improved from 18.77861 to 18.57658, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.66 GB\n","891/891 [==============================] - 335s 374ms/step - loss: 23.3368 - Lev: 0.7124 - val_loss: 18.5766 - val_Lev: 0.7216\n","Epoch 28/80\n","891/891 [==============================] - ETA: 0s - loss: 23.0957 - Lev: 0.7152\n","Epoch 28: val_loss improved from 18.57658 to 18.44712, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.82 GB\n","891/891 [==============================] - 332s 371ms/step - loss: 23.0957 - Lev: 0.7152 - val_loss: 18.4471 - val_Lev: 0.7280\n","Epoch 29/80\n","891/891 [==============================] - ETA: 0s - loss: 22.8289 - Lev: 0.7186\n","Epoch 29: val_loss improved from 18.44712 to 18.25827, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.62 GB\n","891/891 [==============================] - 338s 377ms/step - loss: 22.8289 - Lev: 0.7186 - val_loss: 18.2583 - val_Lev: 0.7290\n","Epoch 30/80\n","891/891 [==============================] - ETA: 0s - loss: 22.5468 - Lev: 0.7220\n","Epoch 30: val_loss improved from 18.25827 to 18.25689, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.48 GB\n","891/891 [==============================] - 337s 377ms/step - loss: 22.5468 - Lev: 0.7220 - val_loss: 18.2569 - val_Lev: 0.7275\n","Epoch 31/80\n","891/891 [==============================] - ETA: 0s - loss: 22.3008 - Lev: 0.7246\n","Epoch 31: val_loss did not improve from 18.25689\n","Memory usage on epoch end: 5.36 GB\n","891/891 [==============================] - 338s 378ms/step - loss: 22.3008 - Lev: 0.7246 - val_loss: 18.4815 - val_Lev: 0.7288\n","Epoch 32/80\n","891/891 [==============================] - ETA: 0s - loss: 22.0456 - Lev: 0.7275\n","Epoch 32: val_loss improved from 18.25689 to 17.98138, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.22 GB\n","891/891 [==============================] - 337s 377ms/step - loss: 22.0456 - Lev: 0.7275 - val_loss: 17.9814 - val_Lev: 0.7357\n","Epoch 33/80\n","891/891 [==============================] - ETA: 0s - loss: 21.8035 - Lev: 0.7305\n","Epoch 33: val_loss did not improve from 17.98138\n","Memory usage on epoch end: 5.03 GB\n","891/891 [==============================] - 341s 381ms/step - loss: 21.8035 - Lev: 0.7305 - val_loss: 18.0538 - val_Lev: 0.7336\n","Epoch 34/80\n","891/891 [==============================] - ETA: 0s - loss: 21.5424 - Lev: 0.7334\n","Epoch 34: val_loss improved from 17.98138 to 17.90553, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.10 GB\n","891/891 [==============================] - 338s 377ms/step - loss: 21.5424 - Lev: 0.7334 - val_loss: 17.9055 - val_Lev: 0.7362\n","Epoch 35/80\n","891/891 [==============================] - ETA: 0s - loss: 21.3661 - Lev: 0.7356\n","Epoch 35: val_loss improved from 17.90553 to 17.82601, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.10 GB\n","891/891 [==============================] - 342s 382ms/step - loss: 21.3661 - Lev: 0.7356 - val_loss: 17.8260 - val_Lev: 0.7348\n","Epoch 36/80\n","891/891 [==============================] - ETA: 0s - loss: 21.1438 - Lev: 0.7380\n","Epoch 36: val_loss improved from 17.82601 to 17.67700, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.24 GB\n","891/891 [==============================] - 336s 376ms/step - loss: 21.1438 - Lev: 0.7380 - val_loss: 17.6770 - val_Lev: 0.7391\n","Epoch 37/80\n","891/891 [==============================] - ETA: 0s - loss: 20.8431 - Lev: 0.7418\n","Epoch 37: val_loss improved from 17.67700 to 17.61162, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.06 GB\n","891/891 [==============================] - 341s 380ms/step - loss: 20.8431 - Lev: 0.7418 - val_loss: 17.6116 - val_Lev: 0.7420\n","Epoch 38/80\n","891/891 [==============================] - ETA: 0s - loss: 20.6470 - Lev: 0.7440\n","Epoch 38: val_loss improved from 17.61162 to 17.60860, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.31 GB\n","891/891 [==============================] - 346s 386ms/step - loss: 20.6470 - Lev: 0.7440 - val_loss: 17.6086 - val_Lev: 0.7417\n","Epoch 39/80\n","891/891 [==============================] - ETA: 0s - loss: 20.4524 - Lev: 0.7461\n","Epoch 39: val_loss improved from 17.60860 to 17.56590, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.40 GB\n","891/891 [==============================] - 358s 399ms/step - loss: 20.4524 - Lev: 0.7461 - val_loss: 17.5659 - val_Lev: 0.7437\n","Epoch 40/80\n","891/891 [==============================] - ETA: 0s - loss: 20.2388 - Lev: 0.7488\n","Epoch 40: val_loss improved from 17.56590 to 17.53608, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.21 GB\n","891/891 [==============================] - 356s 397ms/step - loss: 20.2388 - Lev: 0.7488 - val_loss: 17.5361 - val_Lev: 0.7437\n","Epoch 41/80\n","891/891 [==============================] - ETA: 0s - loss: 20.1093 - Lev: 0.7502\n","Epoch 41: val_loss improved from 17.53608 to 17.48793, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.26 GB\n","891/891 [==============================] - 343s 383ms/step - loss: 20.1093 - Lev: 0.7502 - val_loss: 17.4879 - val_Lev: 0.7458\n","Epoch 42/80\n","891/891 [==============================] - ETA: 0s - loss: 19.8653 - Lev: 0.7531\n","Epoch 42: val_loss did not improve from 17.48793\n","Memory usage on epoch end: 6.31 GB\n","891/891 [==============================] - 339s 379ms/step - loss: 19.8653 - Lev: 0.7531 - val_loss: 17.5347 - val_Lev: 0.7440\n","Epoch 43/80\n","891/891 [==============================] - ETA: 0s - loss: 19.6762 - Lev: 0.7552\n","Epoch 43: val_loss improved from 17.48793 to 17.43706, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.32 GB\n","891/891 [==============================] - 339s 378ms/step - loss: 19.6762 - Lev: 0.7552 - val_loss: 17.4371 - val_Lev: 0.7471\n","Epoch 44/80\n","891/891 [==============================] - ETA: 0s - loss: 19.5044 - Lev: 0.7574\n","Epoch 44: val_loss improved from 17.43706 to 17.30048, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.20 GB\n","891/891 [==============================] - 337s 377ms/step - loss: 19.5044 - Lev: 0.7574 - val_loss: 17.3005 - val_Lev: 0.7484\n","Epoch 45/80\n","891/891 [==============================] - ETA: 0s - loss: 19.2970 - Lev: 0.7596\n","Epoch 45: val_loss improved from 17.30048 to 17.27765, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.29 GB\n","891/891 [==============================] - 342s 382ms/step - loss: 19.2970 - Lev: 0.7596 - val_loss: 17.2776 - val_Lev: 0.7497\n","Epoch 46/80\n","891/891 [==============================] - ETA: 0s - loss: 19.1411 - Lev: 0.7616\n","Epoch 46: val_loss improved from 17.27765 to 17.21395, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.20 GB\n","891/891 [==============================] - 340s 380ms/step - loss: 19.1411 - Lev: 0.7616 - val_loss: 17.2140 - val_Lev: 0.7504\n","Epoch 47/80\n","891/891 [==============================] - ETA: 0s - loss: 18.9568 - Lev: 0.7633\n","Epoch 47: val_loss improved from 17.21395 to 17.20454, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.22 GB\n","891/891 [==============================] - 340s 380ms/step - loss: 18.9568 - Lev: 0.7633 - val_loss: 17.2045 - val_Lev: 0.7524\n","Epoch 48/80\n","891/891 [==============================] - ETA: 0s - loss: 18.8078 - Lev: 0.7652\n","Epoch 48: val_loss improved from 17.20454 to 17.12785, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.10 GB\n","891/891 [==============================] - 338s 377ms/step - loss: 18.8078 - Lev: 0.7652 - val_loss: 17.1278 - val_Lev: 0.7522\n","Epoch 49/80\n","891/891 [==============================] - ETA: 0s - loss: 18.6591 - Lev: 0.7669\n","Epoch 49: val_loss improved from 17.12785 to 17.10565, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.03 GB\n","891/891 [==============================] - 341s 380ms/step - loss: 18.6591 - Lev: 0.7669 - val_loss: 17.1057 - val_Lev: 0.7528\n","Epoch 50/80\n","891/891 [==============================] - ETA: 0s - loss: 18.5259 - Lev: 0.7686\n","Epoch 50: val_loss improved from 17.10565 to 16.95145, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.69 GB\n","891/891 [==============================] - 334s 372ms/step - loss: 18.5259 - Lev: 0.7686 - val_loss: 16.9515 - val_Lev: 0.7536\n","Epoch 51/80\n","891/891 [==============================] - ETA: 0s - loss: 18.3199 - Lev: 0.7712\n","Epoch 51: val_loss did not improve from 16.95145\n","Memory usage on epoch end: 5.16 GB\n","891/891 [==============================] - 339s 379ms/step - loss: 18.3199 - Lev: 0.7712 - val_loss: 17.0396 - val_Lev: 0.7545\n","Epoch 52/80\n","891/891 [==============================] - ETA: 0s - loss: 18.1397 - Lev: 0.7731\n","Epoch 52: val_loss improved from 16.95145 to 16.88970, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.48 GB\n","891/891 [==============================] - 337s 376ms/step - loss: 18.1397 - Lev: 0.7731 - val_loss: 16.8897 - val_Lev: 0.7551\n","Epoch 53/80\n","891/891 [==============================] - ETA: 0s - loss: 18.0621 - Lev: 0.7739\n","Epoch 53: val_loss did not improve from 16.88970\n","Memory usage on epoch end: 5.42 GB\n","891/891 [==============================] - 342s 382ms/step - loss: 18.0621 - Lev: 0.7739 - val_loss: 17.1027 - val_Lev: 0.7559\n","Epoch 54/80\n","891/891 [==============================] - ETA: 0s - loss: 17.9012 - Lev: 0.7758\n","Epoch 54: val_loss did not improve from 16.88970\n","Memory usage on epoch end: 5.32 GB\n","891/891 [==============================] - 340s 379ms/step - loss: 17.9012 - Lev: 0.7758 - val_loss: 17.0119 - val_Lev: 0.7566\n","Epoch 55/80\n","891/891 [==============================] - ETA: 0s - loss: 17.7138 - Lev: 0.7778\n","Epoch 55: val_loss did not improve from 16.88970\n","Memory usage on epoch end: 5.49 GB\n","891/891 [==============================] - 338s 377ms/step - loss: 17.7138 - Lev: 0.7778 - val_loss: 16.9462 - val_Lev: 0.7570\n","Epoch 56/80\n","891/891 [==============================] - ETA: 0s - loss: 17.6134 - Lev: 0.7792\n","Epoch 56: val_loss did not improve from 16.88970\n","Memory usage on epoch end: 5.37 GB\n","891/891 [==============================] - 336s 375ms/step - loss: 17.6134 - Lev: 0.7792 - val_loss: 16.9735 - val_Lev: 0.7583\n","Epoch 57/80\n","891/891 [==============================] - ETA: 0s - loss: 17.4882 - Lev: 0.7805\n","Epoch 57: val_loss improved from 16.88970 to 16.84444, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.18 GB\n","891/891 [==============================] - 339s 378ms/step - loss: 17.4882 - Lev: 0.7805 - val_loss: 16.8444 - val_Lev: 0.7570\n","Epoch 58/80\n","891/891 [==============================] - ETA: 0s - loss: 17.3741 - Lev: 0.7818\n","Epoch 58: val_loss improved from 16.84444 to 16.80808, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.24 GB\n","891/891 [==============================] - 338s 378ms/step - loss: 17.3741 - Lev: 0.7818 - val_loss: 16.8081 - val_Lev: 0.7594\n","Epoch 59/80\n","891/891 [==============================] - ETA: 0s - loss: 17.2552 - Lev: 0.7835\n","Epoch 59: val_loss did not improve from 16.80808\n","Memory usage on epoch end: 5.47 GB\n","891/891 [==============================] - 338s 378ms/step - loss: 17.2552 - Lev: 0.7835 - val_loss: 16.8887 - val_Lev: 0.7584\n","Epoch 60/80\n","891/891 [==============================] - ETA: 0s - loss: 17.1566 - Lev: 0.7844\n","Epoch 60: val_loss did not improve from 16.80808\n","Memory usage on epoch end: 5.36 GB\n","891/891 [==============================] - 335s 374ms/step - loss: 17.1566 - Lev: 0.7844 - val_loss: 16.9348 - val_Lev: 0.7590\n","Epoch 61/80\n","891/891 [==============================] - ETA: 0s - loss: 16.9526 - Lev: 0.7868\n","Epoch 61: val_loss did not improve from 16.80808\n","Memory usage on epoch end: 5.06 GB\n","891/891 [==============================] - 332s 370ms/step - loss: 16.9526 - Lev: 0.7868 - val_loss: 16.8680 - val_Lev: 0.7601\n","Epoch 62/80\n","891/891 [==============================] - ETA: 0s - loss: 16.9252 - Lev: 0.7872\n","Epoch 62: val_loss did not improve from 16.80808\n","Memory usage on epoch end: 5.02 GB\n","891/891 [==============================] - 335s 374ms/step - loss: 16.9252 - Lev: 0.7872 - val_loss: 16.8763 - val_Lev: 0.7599\n","Epoch 63/80\n","891/891 [==============================] - ETA: 0s - loss: 16.8319 - Lev: 0.7883\n","Epoch 63: val_loss did not improve from 16.80808\n","Memory usage on epoch end: 4.97 GB\n","891/891 [==============================] - 332s 371ms/step - loss: 16.8319 - Lev: 0.7883 - val_loss: 16.8642 - val_Lev: 0.7611\n","Epoch 64/80\n","891/891 [==============================] - ETA: 0s - loss: 16.7558 - Lev: 0.7891\n","Epoch 64: val_loss did not improve from 16.80808\n","Memory usage on epoch end: 4.61 GB\n","891/891 [==============================] - 333s 371ms/step - loss: 16.7558 - Lev: 0.7891 - val_loss: 16.8473 - val_Lev: 0.7609\n","Epoch 65/80\n","891/891 [==============================] - ETA: 0s - loss: 16.6599 - Lev: 0.7905\n","Epoch 65: val_loss did not improve from 16.80808\n","Memory usage on epoch end: 5.25 GB\n","891/891 [==============================] - 333s 371ms/step - loss: 16.6599 - Lev: 0.7905 - val_loss: 16.8327 - val_Lev: 0.7611\n","Epoch 66/80\n","891/891 [==============================] - ETA: 0s - loss: 16.5746 - Lev: 0.7914\n","Epoch 66: val_loss did not improve from 16.80808\n","Memory usage on epoch end: 5.03 GB\n","891/891 [==============================] - 330s 368ms/step - loss: 16.5746 - Lev: 0.7914 - val_loss: 16.8466 - val_Lev: 0.7616\n","Epoch 67/80\n","891/891 [==============================] - ETA: 0s - loss: 16.5195 - Lev: 0.7917\n","Epoch 67: val_loss improved from 16.80808 to 16.77874, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.60 GB\n","891/891 [==============================] - 335s 374ms/step - loss: 16.5195 - Lev: 0.7917 - val_loss: 16.7787 - val_Lev: 0.7611\n","Epoch 68/80\n","891/891 [==============================] - ETA: 0s - loss: 16.4509 - Lev: 0.7927\n","Epoch 68: val_loss did not improve from 16.77874\n","Memory usage on epoch end: 4.55 GB\n","891/891 [==============================] - 341s 381ms/step - loss: 16.4509 - Lev: 0.7927 - val_loss: 16.8080 - val_Lev: 0.7626\n","Epoch 69/80\n","891/891 [==============================] - ETA: 0s - loss: 16.3929 - Lev: 0.7931\n","Epoch 69: val_loss did not improve from 16.77874\n","Memory usage on epoch end: 5.29 GB\n","891/891 [==============================] - 334s 372ms/step - loss: 16.3929 - Lev: 0.7931 - val_loss: 16.7965 - val_Lev: 0.7620\n","Epoch 70/80\n","358/891 [===========>..................] - ETA: 3:12 - loss: 16.2088 - Lev: 0.7945"]}],"source":["train(use_supplemental=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.507582Z","iopub.status.busy":"2023-07-18T09:04:44.507205Z","iopub.status.idle":"2023-07-18T09:04:44.521779Z","shell.execute_reply":"2023-07-18T09:04:44.520195Z","shell.execute_reply.started":"2023-07-18T09:04:44.507536Z"},"trusted":true},"outputs":[],"source":["# Inference "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.523717Z","iopub.status.busy":"2023-07-18T09:04:44.523335Z","iopub.status.idle":"2023-07-18T09:04:44.53874Z","shell.execute_reply":"2023-07-18T09:04:44.536985Z","shell.execute_reply.started":"2023-07-18T09:04:44.523687Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.541198Z","iopub.status.busy":"2023-07-18T09:04:44.540385Z","iopub.status.idle":"2023-07-18T09:04:44.556953Z","shell.execute_reply":"2023-07-18T09:04:44.555702Z","shell.execute_reply.started":"2023-07-18T09:04:44.541158Z"},"trusted":true},"outputs":[],"source":["class InferModel(tf.Module):\n","    def __init__(self, model,config=CFG):\n","        super().__init__()\n","\n","        self.model = model\n","        self.max_len=config.max_len\n","\n","    @tf.function(\n","        input_signature=[tf.TensorSpec(shape=(None,Constants.NUM_INPUT_FEATURES), dtype=tf.float32, name=\"inputs\")]\n","    )\n","    def __call__(self, inputs):\n","        \"\"\"\n","        Applies the feature generation model and main model to the input tensor.\n","\n","        Args:\n","            inputs: Input tensor with shape (T, F).\n","\n","        Returns:\n","            A dictionary with a single key 'outputs' and corresponding output tensor.\n","        \"\"\"\n","        x=tf.cast(inputs,tf.float32)\n","        x = x[None] # trick to deal with empty frames\n","        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, Constants.NUM_INPUT_FEATURES)), lambda: tf.identity(x))\n","        x = x[0]\n","        x = preprocess(x,max_len=self.max_len)\n","      \n","        x = self.model(x[None],training=False)[0]\n","                    \n","        x=decode_phrase(x)        \n","        x = tf.cond(tf.shape(x)[0] == 0, lambda: tf.zeros(1, tf.int64), lambda: tf.identity(x))                   \n","                    \n","        outputs=tf.one_hot(x,depth=59,dtype=tf.float32)\n","        return {\"outputs\": outputs}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.558943Z","iopub.status.busy":"2023-07-18T09:04:44.558502Z","iopub.status.idle":"2023-07-18T09:04:48.116622Z","shell.execute_reply":"2023-07-18T09:04:48.11492Z","shell.execute_reply.started":"2023-07-18T09:04:44.55891Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-07-18 20:03:04.039269: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-18 20:03:04.039430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-18 20:03:04.039522: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-18 20:03:04.087722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-18 20:03:04.087875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-18 20:03:04.087972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-18 20:03:04.088045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1487 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"]},{"ename":"FileNotFoundError","evalue":"[Errno 2] Unable to open file (unable to open file: name = '/kaggle/input/weights-from-12h-run/model-384-seed42-exp0-best.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m#saved_based_model = f\"/kaggle/input/weights/{config.comment}-exp{experiment_id}-best.h5\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m saved_based_model \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/kaggle/input/weights-from-12h-run/\u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39mcomment\u001b[39m}\u001b[39;00m\u001b[39m-exp\u001b[39m\u001b[39m{\u001b[39;00mexperiment_id\u001b[39m}\u001b[39;00m\u001b[39m-best.h5\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m model\u001b[39m.\u001b[39;49mload_weights(saved_based_model)\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel with weights \u001b[39m\u001b[39m{\u001b[39;00msaved_based_model\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/code/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m~/code/.venv/lib/python3.10/site-packages/h5py/_hl/files.py:567\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    558\u001b[0m     fapl \u001b[39m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    559\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    560\u001b[0m                      alignment_threshold\u001b[39m=\u001b[39malignment_threshold,\n\u001b[1;32m    561\u001b[0m                      alignment_interval\u001b[39m=\u001b[39malignment_interval,\n\u001b[1;32m    562\u001b[0m                      meta_block_size\u001b[39m=\u001b[39mmeta_block_size,\n\u001b[1;32m    563\u001b[0m                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    564\u001b[0m     fcpl \u001b[39m=\u001b[39m make_fcpl(track_order\u001b[39m=\u001b[39mtrack_order, fs_strategy\u001b[39m=\u001b[39mfs_strategy,\n\u001b[1;32m    565\u001b[0m                      fs_persist\u001b[39m=\u001b[39mfs_persist, fs_threshold\u001b[39m=\u001b[39mfs_threshold,\n\u001b[1;32m    566\u001b[0m                      fs_page_size\u001b[39m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 567\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[39m=\u001b[39;49mswmr)\n\u001b[1;32m    569\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    570\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libver \u001b[39m=\u001b[39m libver\n","File \u001b[0;32m~/code/.venv/lib/python3.10/site-packages/h5py/_hl/files.py:231\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m swmr \u001b[39mand\u001b[39;00m swmr_support:\n\u001b[1;32m    230\u001b[0m         flags \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 231\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39;49mopen(name, flags, fapl\u001b[39m=\u001b[39;49mfapl)\n\u001b[1;32m    232\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    233\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mopen(name, h5f\u001b[39m.\u001b[39mACC_RDWR, fapl\u001b[39m=\u001b[39mfapl)\n","File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mh5py/h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '/kaggle/input/weights-from-12h-run/model-384-seed42-exp0-best.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"]}],"source":["\n","config=CFG\n","\n","model = get_model(\n","    max_len=config.max_len,\n","    output_dim=config.output_dim,\n","    dim=config.dim,\n","    input_pad=Constants.INPUT_PAD,\n",")\n","experiment_id=0\n","\n","saved_based_model = f\"/kaggle/input/weights/{config.comment}-exp{experiment_id}-best.h5\"\n","#saved_based_model = f\"/kaggle/input/weights-from-12h-run/{config.comment}-exp{experiment_id}-best.h5\"\n","model.load_weights(saved_based_model)\n","print(f\"model with weights {saved_based_model}\")"]},{"cell_type":"markdown","metadata":{},"source":["#Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:48.118699Z","iopub.status.busy":"2023-07-18T09:04:48.1183Z","iopub.status.idle":"2023-07-18T09:04:55.78829Z","shell.execute_reply":"2023-07-18T09:04:55.784098Z","shell.execute_reply.started":"2023-07-18T09:04:48.118667Z"},"trusted":true},"outputs":[],"source":["# Sanity Check\n","import json\n","with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n","    character_map = json.load(f)\n","rev_character_map = {j:i for i,j in character_map.items()}\n","\n","infer_keras_model=InferModel(model)\n","\n","main_dir = '/kaggle/input/asl-fingerspelling/'\n","path = f'{main_dir}train_landmarks/5414471.parquet'\n","cols=selected_columns(path)\n","df = pd.read_parquet(path, engine = 'auto', columns = cols)\n","seq_id=1816796431\n","seq=df.loc[seq_id]\n","data = seq[cols].to_numpy()\n","print(f'input shape: {data.shape}, dtype: {data.dtype}')\n","output = infer_keras_model(data)[\"outputs\"]\n","prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output, axis=1)])\n","\n","print(prediction_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:55.79275Z","iopub.status.busy":"2023-07-18T09:04:55.792139Z","iopub.status.idle":"2023-07-18T09:05:47.089729Z","shell.execute_reply":"2023-07-18T09:05:47.088743Z","shell.execute_reply.started":"2023-07-18T09:04:55.792699Z"},"trusted":true},"outputs":[],"source":["SAVED_MODEL_PATH=\"/kaggle/working/infer_model\"\n","\n","tf.saved_model.save(infer_keras_model,SAVED_MODEL_PATH)\n","keras_model_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_PATH)\n","keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","#keras_model_converter.target_spec.supported_types = [tf.float16]\n","#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","#converter.allow_custom_ops=True\n","tflite_model = keras_model_converter.convert()\n","TFLITE_FILE_PATH=\"/kaggle/working/model.tflite\"\n","with open(TFLITE_FILE_PATH, \"wb\") as f:\n","    f.write(tflite_model)\n","\n","with open('/kaggle/working/inference_args.json', 'w') as f:\n","     json.dump({ 'selected_columns': cols }, f)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:05:47.092039Z","iopub.status.busy":"2023-07-18T09:05:47.091681Z","iopub.status.idle":"2023-07-18T09:05:47.257675Z","shell.execute_reply":"2023-07-18T09:05:47.255994Z","shell.execute_reply.started":"2023-07-18T09:05:47.092009Z"},"trusted":true},"outputs":[],"source":["interpreter = tf.lite.Interpreter(TFLITE_FILE_PATH)\n","REQUIRED_SIGNATURE = \"serving_default\"\n","REQUIRED_OUTPUT = \"outputs\"\n","found_signatures = list(interpreter.get_signature_list().keys())\n","if REQUIRED_SIGNATURE not in found_signatures:\n","    print(\"Required input signature not found.\")\n","\n","prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n","output = prediction_fn(inputs=data)\n","prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n","print(prediction_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:05:47.26006Z","iopub.status.busy":"2023-07-18T09:05:47.25933Z","iopub.status.idle":"2023-07-18T09:05:48.830171Z","shell.execute_reply":"2023-07-18T09:05:48.828963Z","shell.execute_reply.started":"2023-07-18T09:05:47.260006Z"},"trusted":true},"outputs":[],"source":["!zip submission.zip \"/kaggle/working/model.tflite\" \"/kaggle/working/inference_args.json\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:05:48.832628Z","iopub.status.busy":"2023-07-18T09:05:48.832209Z","iopub.status.idle":"2023-07-18T09:05:48.840337Z","shell.execute_reply":"2023-07-18T09:05:48.838608Z","shell.execute_reply.started":"2023-07-18T09:05:48.832593Z"},"trusted":true},"outputs":[],"source":["#!pip install /kaggle/input/tflite-wheels-2140/tflite_runtime_nightly-2.14.0.dev20230508-cp310-cp310-manylinux2014_x86_64.whl"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:05:48.84292Z","iopub.status.busy":"2023-07-18T09:05:48.842282Z","iopub.status.idle":"2023-07-18T09:05:48.857104Z","shell.execute_reply":"2023-07-18T09:05:48.856148Z","shell.execute_reply.started":"2023-07-18T09:05:48.842886Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","import json\n","import pandas as pd\n","import tflite_runtime.interpreter as tflite\n","import numpy as np\n","import time\n","from tqdm import tqdm\n","import Levenshtein as Lev\n","import glob\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:05:48.859108Z","iopub.status.busy":"2023-07-18T09:05:48.858802Z","iopub.status.idle":"2023-07-18T09:05:48.878044Z","shell.execute_reply":"2023-07-18T09:05:48.876595Z","shell.execute_reply.started":"2023-07-18T09:05:48.859082Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","SEL_FEATURES = json.load(open('/kaggle/working/inference_args.json'))['selected_columns']\n","\n","def load_relevant_data_subset(pq_path):\n","        return pd.read_parquet(pq_path, columns=SEL_FEATURES) #selected_columns)\n","\n","with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n","    character_map = json.load(f)\n","rev_character_map = {j:i for i,j in character_map.items()}\n","\n","\n","df = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\n","\n","idx = 0\n","sample = df.loc[idx]\n","loaded = load_relevant_data_subset('/kaggle/input/asl-fingerspelling/' + sample['path'])\n","loaded = loaded[loaded.index==sample['sequence_id']].values\n","print(loaded.shape)\n","frames = loaded\n","\n","def wer__(s1, s2):\n","    w1 = len(s1.split())\n","    lvd = Lev.distance(s1, s2)\n","    return lvd / w1\n","\n","interpreter = tflite.Interpreter('model.tflite')\n","found_signatures = list(interpreter.get_signature_list().keys())\n","\n","REQUIRED_SIGNATURE = 'serving_default'\n","REQUIRED_OUTPUT = 'outputs'\n","if REQUIRED_SIGNATURE not in found_signatures:\n","    raise KernelEvalException('Required input signature not found.')\n","\n","prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n","output_lite = prediction_fn(inputs=frames)\n","prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output_lite[REQUIRED_OUTPUT], axis=1)])\n","print(prediction_str)\n","\n","\n","st = time.time()\n","count=0\n","model_time = 0\n","\n","levs = []\n","\n","files=glob.glob('/kaggle/input/asl-fingerspelling/train_landmarks/*.parquet')\n","for f in files:\n","    df = load_relevant_data_subset(f)\n","    seq=df.index.drop_duplicates()\n","    for ind in tqdm(seq):\n","        loaded = df.loc[ind].values\n","        count+=1\n","        md_st = time.time()\n","        output_ = prediction_fn(inputs=loaded)\n","        out= output_[REQUIRED_OUTPUT]\n","        assert out.ndim==2\n","        assert out.shape[1]==59\n","        assert out.dtype==np.float32\n","        assert np.all(np.isfinite(out))\n","        \n","        prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output_[REQUIRED_OUTPUT], axis=1)])\n","        model_time += time.time() - md_st\n","    \n","        #cur_lev = wer__(sample['phrase'], prediction_str) \n","        #print(sample['phrase'], '|', prediction_str, '|', cur_lev)\n","        #print()\n","\n","        #levs.append(cur_lev)\n","\n","#print(f'WER: {np.mean(levs):.5f}')\n","print(f'Mean time: {(time.time() - st)/count:.2f}')\n","print(f'Mean time only infer: {model_time/count:.2f}')\n","\n","out=prediction_fn(inputs=np.empty(0,dtype=np.float32))[\"outputs\"]\n","print(out.shape,output_.dtype)\n","\"\"\" "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":4}
