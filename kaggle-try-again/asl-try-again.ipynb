{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Google - American Sign Language Fingerspelling Recognition with TensorFlow\n","\n","This notebook walks you through how to train a Transformer model using TensorFlow on the Google - American Sign Language Fingerspelling Recognition dataset made available for this competition.\n","\n","The objective of the model is to predict and translate American Sign Language (ASL) fingerspelling from a set of video frames into text(`phrase`).\n","\n","In this notebook you will learn:\n","\n","- How to load the data\n","- Convert the data to tfrecords to make it faster to re-traing the model\n","- Train a transformer models on the data\n","- Convert the model to TFLite\n","- Create a submission"]},{"cell_type":"markdown","metadata":{},"source":["# Installation\n","\n","Specifically for this competition, you'll need the mediapipe library to work on the data and visualize it"]},{"cell_type":"markdown","metadata":{},"source":["# Import the libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-18T09:04:40.055733Z","iopub.status.busy":"2023-07-18T09:04:40.054786Z","iopub.status.idle":"2023-07-18T09:04:43.829827Z","shell.execute_reply":"2023-07-18T09:04:43.828466Z","shell.execute_reply.started":"2023-07-18T09:04:40.055676Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-07-19 10:21:40.910340: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:8893] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-07-19 10:21:40.910367: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-07-19 10:21:40.911977: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-07-19 10:21:41.173180: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-07-19 10:21:42.359777: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From /home/sronen/code/.venv/lib/python3.10/site-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n","Instructions for updating:\n","The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n","WARNING:tensorflow:From /home/sronen/code/.venv/lib/python3.10/site-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n","Instructions for updating:\n","The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import random\n","import psutil\n","import glob\n","import gc\n","import math\n","from tensorflow.python.framework.ops import disable_eager_execution\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.832765Z","iopub.status.busy":"2023-07-18T09:04:43.832015Z","iopub.status.idle":"2023-07-18T09:04:43.840057Z","shell.execute_reply":"2023-07-18T09:04:43.83867Z","shell.execute_reply.started":"2023-07-18T09:04:43.832729Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-07-19 10:21:43.603012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-19 10:21:43.666539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-19 10:21:43.666754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"]}],"source":["gpus = tf.config.list_physical_devices(\"GPU\")\n","for gpu in gpus:\n","    tf.config.experimental.set_memory_growth(gpu, True)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.841953Z","iopub.status.busy":"2023-07-18T09:04:43.84152Z","iopub.status.idle":"2023-07-18T09:04:43.853631Z","shell.execute_reply":"2023-07-18T09:04:43.852101Z","shell.execute_reply.started":"2023-07-18T09:04:43.84192Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["TensorFlow v2.14.0-dev20230718\n"]}],"source":["print(\"TensorFlow v\" + tf.__version__)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.856382Z","iopub.status.busy":"2023-07-18T09:04:43.855886Z","iopub.status.idle":"2023-07-18T09:04:43.869418Z","shell.execute_reply":"2023-07-18T09:04:43.868074Z","shell.execute_reply.started":"2023-07-18T09:04:43.856337Z"},"trusted":true},"outputs":[],"source":["class MemoryUsageCallbackExtended(tf.keras.callbacks.Callback):\n","    \"\"\"Monitor memory usage on epoch begin and end, collect garbage\"\"\"\n","\n","    # def on_epoch_begin(self, epoch, logs=None):\n","    #    print(\"**Epoch {}**\".format(epoch))\n","    #    print(\n","    #        f\"Memory usage on epoch begin: {int(psutil.Process(os.getpid()).memory_info().rss)/1e9:.2f GB}\"\n","    #    )\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        print(\n","            f\"Memory usage on epoch end: {int(psutil.Process(os.getpid()).memory_info().rss)/1e9:.2f} GB\"\n","        )\n","        gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["# Scheduler"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.87415Z","iopub.status.busy":"2023-07-18T09:04:43.873737Z","iopub.status.idle":"2023-07-18T09:04:43.90059Z","shell.execute_reply":"2023-07-18T09:04:43.898986Z","shell.execute_reply.started":"2023-07-18T09:04:43.874119Z"},"trusted":true},"outputs":[],"source":["\n","class CosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    \"\"\"A LearningRateSchedule that uses a cosine decay with optional warmup.\n","\n","    See [Loshchilov & Hutter, ICLR2016](https://arxiv.org/abs/1608.03983),\n","    SGDR: Stochastic Gradient Descent with Warm Restarts.\n","\n","    For the idea of a linear warmup of our learning rate,\n","    see [Goyal et al.](https://arxiv.org/pdf/1706.02677.pdf).\n","\n","    When we begin training a model, we often want an initial increase in our\n","    learning rate followed by a decay. If `warmup_target` is an int, this\n","    schedule applies a linear increase per optimizer step to our learning rate\n","    from `initial_learning_rate` to `warmup_target` for a duration of\n","    `warmup_steps`. Afterwards, it applies a cosine decay function taking our\n","    learning rate from `warmup_target` to `alpha` for a duration of\n","    `decay_steps`. If `warmup_target` is None we skip warmup and our decay\n","    will take our learning rate from `initial_learning_rate` to `alpha`.\n","    It requires a `step` value to  compute the learning rate. You can\n","    just pass a TensorFlow variable that you increment at each training step.\n","\n","    The schedule is a 1-arg callable that produces a warmup followed by a\n","    decayed learning rate when passed the current optimizer step. This can be\n","    useful for changing the learning rate value across different invocations of\n","    optimizer functions.\n","\n","    Our warmup is computed as:\n","\n","    ```python\n","    def warmup_learning_rate(step):\n","        completed_fraction = step / warmup_steps\n","        total_delta = target_warmup - initial_learning_rate\n","        return completed_fraction * total_delta\n","    ```\n","\n","    And our decay is computed as:\n","\n","    ```python\n","    if warmup_target is None:\n","        initial_decay_lr = initial_learning_rate\n","    else:\n","        initial_decay_lr = warmup_target\n","\n","    def decayed_learning_rate(step):\n","        step = min(step, decay_steps)\n","        cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))\n","        decayed = (1 - alpha) * cosine_decay + alpha\n","        return initial_decay_lr * decayed\n","    ```\n","\n","    Example usage without warmup:\n","\n","    ```python\n","    decay_steps = 1000\n","    initial_learning_rate = 0.1\n","    lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n","        initial_learning_rate, decay_steps)\n","    ```\n","\n","    Example usage with warmup:\n","\n","    ```python\n","    decay_steps = 1000\n","    initial_learning_rate = 0\n","    warmup_steps = 1000\n","    target_learning_rate = 0.1\n","    lr_warmup_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n","        initial_learning_rate, decay_steps, warmup_target=target_learning_rate,\n","        warmup_steps=warmup_steps\n","    )\n","    ```\n","\n","    You can pass this schedule directly into a `tf.keras.optimizers.Optimizer`\n","    as the learning rate. The learning rate schedule is also serializable and\n","    deserializable using `tf.keras.optimizers.schedules.serialize` and\n","    `tf.keras.optimizers.schedules.deserialize`.\n","\n","    Returns:\n","      A 1-arg callable learning rate schedule that takes the current optimizer\n","      step and outputs the decayed learning rate, a scalar `Tensor` of the same\n","      type as `initial_learning_rate`.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        initial_learning_rate,\n","        decay_steps,\n","        alpha=0.0,\n","        name=None,\n","        warmup_target=None,\n","        warmup_steps=0,\n","    ):\n","        \"\"\"Applies cosine decay to the learning rate.\n","\n","        Args:\n","          initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a\n","            Python int. The initial learning rate.\n","          decay_steps: A scalar `int32` or `int64` `Tensor` or a Python int.\n","            Number of steps to decay over.\n","          alpha: A scalar `float32` or `float64` `Tensor` or a Python int.\n","            Minimum learning rate value for decay as a fraction of\n","            `initial_learning_rate`.\n","          name: String. Optional name of the operation.  Defaults to\n","            'CosineDecay'.\n","          warmup_target: None or a scalar `float32` or `float64` `Tensor` or a\n","            Python int. The target learning rate for our warmup phase. Will cast\n","            to the `initial_learning_rate` datatype. Setting to None will skip\n","            warmup and begins decay phase from `initial_learning_rate`.\n","            Otherwise scheduler will warmup from `initial_learning_rate` to\n","            `warmup_target`.\n","          warmup_steps: A scalar `int32` or `int64` `Tensor` or a Python int.\n","            Number of steps to warmup over.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.initial_learning_rate = initial_learning_rate\n","        self.decay_steps = decay_steps\n","        self.alpha = alpha\n","        self.name = name\n","        self.warmup_steps = warmup_steps\n","        self.warmup_target = warmup_target\n","\n","    def _decay_function(self, step, decay_steps, decay_from_lr, dtype):\n","        with tf.name_scope(self.name or \"CosineDecay\"):\n","            completed_fraction = step / decay_steps\n","            tf_pi = tf.constant(math.pi, dtype=dtype)\n","            cosine_decayed = 0.5 * (1.0 + tf.cos(tf_pi * completed_fraction))\n","            decayed = (1 - self.alpha) * cosine_decayed + self.alpha\n","            return tf.multiply(decay_from_lr, decayed)\n","\n","    def _warmup_function(self, step, warmup_steps, warmup_target, initial_learning_rate):\n","        with tf.name_scope(self.name or \"CosineDecay\"):\n","            completed_fraction = step / warmup_steps\n","            total_step_delta = warmup_target - initial_learning_rate\n","            return total_step_delta * completed_fraction + initial_learning_rate\n","\n","    def __call__(self, step):\n","        with tf.name_scope(self.name or \"CosineDecay\"):\n","            initial_learning_rate = tf.convert_to_tensor(\n","                self.initial_learning_rate, name=\"initial_learning_rate\"\n","            )\n","            dtype = initial_learning_rate.dtype\n","            decay_steps = tf.cast(self.decay_steps, dtype)\n","            global_step_recomp = tf.cast(step, dtype)\n","\n","            if self.warmup_target is None:\n","                global_step_recomp = tf.minimum(global_step_recomp, decay_steps)\n","                return self._decay_function(\n","                    global_step_recomp,\n","                    decay_steps,\n","                    initial_learning_rate,\n","                    dtype,\n","                )\n","\n","            warmup_target = tf.cast(self.warmup_target, dtype)\n","            warmup_steps = tf.cast(self.warmup_steps, dtype)\n","\n","            global_step_recomp = tf.minimum(global_step_recomp, decay_steps + warmup_steps)\n","\n","            return tf.cond(\n","                global_step_recomp < warmup_steps,\n","                lambda: self._warmup_function(\n","                    global_step_recomp,\n","                    warmup_steps,\n","                    warmup_target,\n","                    initial_learning_rate,\n","                ),\n","                lambda: self._decay_function(\n","                    global_step_recomp - warmup_steps,\n","                    decay_steps,\n","                    warmup_target,\n","                    dtype,\n","                ),\n","            )\n","\n","    def get_config(self):\n","        return {\n","            \"initial_learning_rate\": self.initial_learning_rate,\n","            \"decay_steps\": self.decay_steps,\n","            \"alpha\": self.alpha,\n","            \"name\": self.name,\n","            \"warmup_target\": self.warmup_target,\n","            \"warmup_steps\": self.warmup_steps,\n","        }\n"]},{"cell_type":"markdown","metadata":{},"source":["# Constants"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.902869Z","iopub.status.busy":"2023-07-18T09:04:43.902415Z","iopub.status.idle":"2023-07-18T09:04:43.928871Z","shell.execute_reply":"2023-07-18T09:04:43.927451Z","shell.execute_reply.started":"2023-07-18T09:04:43.902837Z"},"trusted":true},"outputs":[],"source":["def get_char_dict():\n","    char_dict = {\n","        \" \": 0,\n","        \"!\": 1,\n","        \"#\": 2,\n","        \"$\": 3,\n","        \"%\": 4,\n","        \"&\": 5,\n","        \"'\": 6,\n","        \"(\": 7,\n","        \")\": 8,\n","        \"*\": 9,\n","        \"+\": 10,\n","        \",\": 11,\n","        \"-\": 12,\n","        \".\": 13,\n","        \"/\": 14,\n","        \"0\": 15,\n","        \"1\": 16,\n","        \"2\": 17,\n","        \"3\": 18,\n","        \"4\": 19,\n","        \"5\": 20,\n","        \"6\": 21,\n","        \"7\": 22,\n","        \"8\": 23,\n","        \"9\": 24,\n","        \":\": 25,\n","        \";\": 26,\n","        \"=\": 27,\n","        \"?\": 28,\n","        \"@\": 29,\n","        \"[\": 30,\n","        \"_\": 31,\n","        \"a\": 32,\n","        \"b\": 33,\n","        \"c\": 34,\n","        \"d\": 35,\n","        \"e\": 36,\n","        \"f\": 37,\n","        \"g\": 38,\n","        \"h\": 39,\n","        \"i\": 40,\n","        \"j\": 41,\n","        \"k\": 42,\n","        \"l\": 43,\n","        \"m\": 44,\n","        \"n\": 45,\n","        \"o\": 46,\n","        \"p\": 47,\n","        \"q\": 48,\n","        \"r\": 49,\n","        \"s\": 50,\n","        \"t\": 51,\n","        \"u\": 52,\n","        \"v\": 53,\n","        \"w\": 54,\n","        \"x\": 55,\n","        \"y\": 56,\n","        \"z\": 57,\n","        \"~\": 58,\n","    }\n","    char_dict[\"P\"] = 59\n","    char_dict[\"SOS\"] = 60\n","    char_dict[\"EOS\"] = 61\n","    return char_dict\n","\n","\n","class Constants:\n","    ROWS_PER_FRAME = 543\n","    MAX_STRING_LEN = 50\n","    INPUT_PAD = -100.0\n","    char_dict = get_char_dict()\n","    LABEL_PAD = char_dict[\"P\"]\n","    inv_dict = {v: k for k, v in char_dict.items()}\n","    NOSE = [1, 2, 98, 327]\n","\n","    REYE = [33, 7, 163, 144, 145, 153, 154, 155, 133, 246, 161, 160, 159, 158, 157, 173]\n","    LEYE = [263, 249, 390, 373, 374, 380, 381, 382, 362, 466, 388, 387, 386, 385, 384, 398]\n","\n","    LHAND = list(range(468, 489))\n","    RHAND = list(range(522, 543))\n","\n","    LNOSE = [98]\n","    RNOSE = [327]\n","\n","    LLIP = [84, 181, 91, 146, 61, 185, 40, 39, 37, 87, 178, 88, 95, 78, 191, 80, 81, 82]\n","    RLIP = [\n","        314,\n","        405,\n","        321,\n","        375,\n","        291,\n","        409,\n","        270,\n","        269,\n","        267,\n","        317,\n","        402,\n","        318,\n","        324,\n","        308,\n","        415,\n","        310,\n","        311,\n","        312,\n","    ]\n","    POSE = [500, 502, 504, 501, 503, 505, 512, 513]\n","    LPOSE = [513, 505, 503, 501]\n","    RPOSE = [512, 504, 502, 500]\n","\n","    POINT_LANDMARKS_PARTS = [LHAND, RHAND, LLIP, RLIP, LPOSE, RPOSE, NOSE, REYE, LEYE]\n","    # POINT_LANDMARKS_PARTS = [LHAND, RHAND, NOSE]\n","    POINT_LANDMARKS = [item for sublist in POINT_LANDMARKS_PARTS for item in sublist]\n","    parts = {\n","        \"LLIP\": LLIP,\n","        \"RLIP\": RLIP,\n","        \"LHAND\": LHAND,\n","        \"RHAND\": RHAND,\n","        \"LPOSE\": LPOSE,\n","        \"RPOSE\": RPOSE,\n","        \"LNOSE\": LNOSE,\n","        \"RNOSE\": RNOSE,\n","        \"REYE\": REYE,\n","        \"LEYE\": LEYE,\n","    }\n","\n","    LANDMARK_INDICES = {}  # type: ignore\n","    for part in parts:\n","        LANDMARK_INDICES[part] = []\n","        for landmark in parts[part]:\n","            if landmark in POINT_LANDMARKS:\n","                LANDMARK_INDICES[part].append(POINT_LANDMARKS.index(landmark))\n","\n","    CENTER_LANDMARKS = LNOSE + RNOSE\n","    CENTER_INDICES = LANDMARK_INDICES[\"LNOSE\"] + LANDMARK_INDICES[\"RNOSE\"]\n","\n","    NUM_NODES = len(POINT_LANDMARKS)\n","    NUM_INPUT_FEATURES = 2 * NUM_NODES\n","    CHANNELS = 6 * NUM_NODES\n"]},{"cell_type":"markdown","metadata":{},"source":["# Utils"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.930812Z","iopub.status.busy":"2023-07-18T09:04:43.93046Z","iopub.status.idle":"2023-07-18T09:04:43.966369Z","shell.execute_reply":"2023-07-18T09:04:43.964929Z","shell.execute_reply.started":"2023-07-18T09:04:43.930782Z"},"trusted":true},"outputs":[],"source":["\n","# Seed all random number generators\n","def seed_everything(seed=42):\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    tf.random.set_seed(seed)\n","\n","\n","def selected_columns(file_example):\n","    df = pd.read_parquet(file_example)\n","    selected_x = df.columns[[x + 1 for x in Constants.POINT_LANDMARKS]].tolist()\n","    selected_y = [c.replace(\"x\", \"y\") for c in selected_x]\n","    selected = []\n","    for i in range(Constants.NUM_NODES):\n","        selected.append(selected_x[i])\n","        selected.append(selected_y[i])\n","    return selected  # x1,y1,x2,y2,...\n","\n","\n","\n","def num_to_char_fn(y):\n","    return [Constants.inv_dict.get(x, \"\") for x in y]\n","\n","\n","# A callback class to output a few transcriptions during training\n","class CallbackEval(tf.keras.callbacks.Callback):\n","    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n","\n","    def __init__(self, model, dataset):\n","        super().__init__()\n","        self.dataset = dataset\n","        self.model = model\n","\n","    def on_epoch_end(self, epoch: int, logs=None):\n","        predictions = []\n","        targets = []\n","        for batch in self.dataset:\n","            X, y = batch\n","            batch_predictions = self.model(X)\n","            batch_predictions = decode_batch_predictions(batch_predictions)\n","            predictions.extend(batch_predictions)\n","            for label in y:\n","                label = \"\".join(num_to_char_fn(label.numpy()))\n","                targets.append(label)\n","        print(\"-\" * 100)\n","        # for i in np.random.randint(0, len(predictions), 2):\n","        for i in range(10):\n","            print(f\"Target    : {targets[i]}\")\n","            print(f\"Prediction: {predictions[i]}, len: {len(predictions[i])}\")\n","            print(\"-\" * 100)\n","\n","\n","def decode_phrase(pred):\n","    # decode cts prediction by prunning\n","    # (T,CHAR_NUMS)\n","    x = tf.argmax(pred, axis=1)\n","    paddings = tf.constant(\n","        [\n","            [0, 1],\n","        ]\n","    )\n","    x = tf.pad(x, paddings)\n","    diff = tf.not_equal(x[:-1], x[1:])\n","    adjacent_indices = tf.where(diff)[:, 0]\n","    x = tf.gather(x, adjacent_indices)\n","    mask = x != Constants.LABEL_PAD\n","    x = tf.boolean_mask(x, mask, axis=0)\n","    return x\n","\n","\n","# A utility function to decode the output of the network\n","def decode_batch_predictions(pred):\n","    output_text = []\n","    for result in pred:\n","        result = \"\".join(num_to_char_fn(decode_phrase(result).numpy()))\n","        output_text.append(result)\n","    return output_text\n","\n","\n","\n","\n","def code_to_label(label_code):\n","    label = [Constants.inv_dict[x] for x in label_code if Constants.inv_dict[x] != \"P\"]\n","    label = \"\".join(label)\n","    return label\n","\n","\n","def convert_to_strings(batch_label_code):\n","    output = []\n","    for label_code in batch_label_code:\n","        output.append(code_to_label(label_code))\n","    return output\n","\n","\n","def global_metric(val_ds, model):\n","    global_N, global_D = 0, 0\n","    count = 0\n","    metric = LevDistanceMetric()\n","    for batch in val_ds:\n","        count += 1\n","        print(count)\n","        feature, label = batch\n","        logits = model(feature)\n","        _, _, D = batch_edit_distance(label, logits)\n","        metric.update_state(label, logits)\n","   \n","    result = metric.result().numpy()\n","   \n","    return result\n","\n","\n","def sparse_from_dense_ignore_value(dense_tensor):\n","    mask = tf.not_equal(dense_tensor, Constants.LABEL_PAD)\n","    indices = tf.where(mask)\n","    values = tf.boolean_mask(dense_tensor, mask)\n","\n","    return tf.SparseTensor(indices, values, tf.shape(dense_tensor, out_type=tf.int64))\n","\n","\n","def batch_edit_distance(y_true, y_logits):\n","    blank = Constants.LABEL_PAD\n","    B = tf.shape(y_logits)[0]\n","    seq_length = tf.shape(y_logits)[1]\n","    to_decode = tf.transpose(y_logits, perm=[1, 0, 2])\n","    sequence_length = tf.fill(dims=[B], value=seq_length)\n","    hypothesis = tf.nn.ctc_greedy_decoder(\n","        tf.cast(to_decode, tf.float32), sequence_length, blank_index=blank\n","    )[0][\n","        0\n","    ]  # full is [B,...]\n","    truth = sparse_from_dense_ignore_value(y_true)  # full is [B,...]\n","    truth = tf.cast(truth, tf.int64)\n","    edit_dist = tf.edit_distance(hypothesis, truth, normalize=False)\n","\n","    non_ignore_mask = tf.not_equal(y_true, blank)\n","    N = tf.reduce_sum(tf.cast(non_ignore_mask, tf.float32))\n","    D = tf.reduce_sum(edit_dist)\n","    result = (N - D) / N\n","    result = tf.clip_by_value(result, 0.0, 1.0)\n","    return result, N, D\n","\n","\n","class LevDistanceMetric(tf.keras.metrics.Metric):\n","    def __init__(self, name=\"Lev\", **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        self.distance = self.add_weight(name=\"dist\", initializer=\"zeros\")\n","        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n","\n","    def update_state(self, y_true, y_logits, sample_weight=None):\n","        # if using with keras compile, make sure the model outputs logits, not softmax probabilities\n","        _, N, D = batch_edit_distance(y_true, y_logits)\n","        self.distance.assign_add(D)\n","        self.count.assign_add(N)\n","\n","    def result(self):\n","        result = (self.count - self.distance) / self.count\n","        result = tf.clip_by_value(result, 0.0, 1.0)\n","        return result\n","\n","    def reset_state(self):\n","        self.count.assign(0.0)\n","        self.distance.assign(0.0)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.969375Z","iopub.status.busy":"2023-07-18T09:04:43.968934Z","iopub.status.idle":"2023-07-18T09:04:44.024972Z","shell.execute_reply":"2023-07-18T09:04:44.023605Z","shell.execute_reply.started":"2023-07-18T09:04:43.969338Z"},"trusted":true},"outputs":[],"source":["\n","class CTCLoss(tf.keras.losses.Loss):\n","    def __init__(self, pad_token_idx):\n","        self.pad_token_idx = pad_token_idx\n","        super().__init__()\n","\n","    def call(self, labels, logits):\n","        label_length = tf.reduce_sum(tf.cast(labels != self.pad_token_idx, tf.int32), axis=-1)\n","        logit_length = tf.ones(tf.shape(logits)[0], dtype=tf.int32) * tf.shape(logits)[1]\n","\n","        ctc_loss = tf.nn.ctc_loss(\n","            labels=labels,\n","            logits=logits,\n","            label_length=label_length,\n","            logit_length=logit_length,\n","            blank_index=self.pad_token_idx,\n","            logits_time_major=False,\n","        )\n","\n","        return ctc_loss\n","\n","\n","class ECA(tf.keras.layers.Layer):\n","    # Efficient Channel Attention\n","    def __init__(self, kernel_size=5, **kwargs):\n","        super().__init__(**kwargs)\n","        self.supports_masking = True\n","        self.kernel_size = kernel_size\n","        self.conv = tf.keras.layers.Conv1D(\n","            1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False\n","        )\n","\n","    def call(self, inputs, mask=None):\n","        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n","        nn = tf.expand_dims(nn, -1)\n","        nn = self.conv(nn)\n","        nn = tf.squeeze(nn, -1)\n","        nn = tf.nn.sigmoid(nn)\n","        nn = nn[:, None, :]\n","        return inputs * nn\n","\n","\n","class LateDropout(tf.keras.layers.Layer):\n","    def __init__(self, rate, noise_shape=None, start_step=0, **kwargs):\n","        super().__init__(**kwargs)\n","        self.supports_masking = True\n","        self.rate = rate\n","        self.start_step = start_step\n","        self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n","\n","    def build(self, input_shape):\n","        super().build(input_shape)\n","        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n","        self._train_counter = tf.Variable(0, dtype=\"int64\", aggregation=agg, trainable=False)\n","\n","    def call(self, inputs, training=False):\n","        x = tf.cond(\n","            self._train_counter < self.start_step,\n","            lambda: inputs,\n","            lambda: self.dropout(inputs, training=training),\n","        )\n","        if training:\n","            self._train_counter.assign_add(1)\n","        return x\n","\n","\n","class CausalDWConv1D(tf.keras.layers.Layer):\n","    # Causal Depth Wise Convolution\n","    def __init__(\n","        self,\n","        kernel_size=17,\n","        dilation_rate=1,\n","        use_bias=False,\n","        depthwise_initializer=\"glorot_uniform\",\n","        name=\"\",\n","        **kwargs,\n","    ):\n","        super().__init__(name=name, **kwargs)\n","        self.causal_pad = tf.keras.layers.ZeroPadding1D(\n","            (dilation_rate * (kernel_size - 1), 0), name=name + \"_pad\"\n","        )\n","        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n","            kernel_size,\n","            strides=1,\n","            dilation_rate=dilation_rate,\n","            padding=\"valid\",\n","            use_bias=use_bias,\n","            depthwise_initializer=depthwise_initializer,\n","            name=name + \"_dwconv\",\n","        )\n","        self.supports_masking = True\n","\n","    def call(self, inputs):\n","        x = self.causal_pad(inputs)\n","        x = self.dw_conv(x)\n","        return x\n","\n","\n","def Conv1DBlock(\n","    channel_size,\n","    kernel_size,\n","    dilation_rate=1,\n","    drop_rate=0.0,\n","    expand_ratio=2,\n","    # se_ratio=0.25,\n","    activation=\"swish\",\n","    name=None,\n","):\n","    \"\"\"\n","    efficient conv1d block, @hoyso48\n","    \"\"\"\n","    if name is None:\n","        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n","\n","    # Expansion phase\n","    def apply(inputs):\n","        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n","        channels_expand = channels_in * expand_ratio\n","\n","        skip = inputs\n","\n","        x = tf.keras.layers.Dense(\n","            channels_expand, use_bias=True, activation=activation, name=name + \"_expand_conv\"\n","        )(inputs)\n","\n","        # Depthwise Convolution\n","        x = CausalDWConv1D(\n","            kernel_size, dilation_rate=dilation_rate, use_bias=False, name=name + \"_dwconv\"\n","        )(x)\n","\n","        x = tf.keras.layers.LayerNormalization(name=name + \"_bn\")(x)\n","\n","        x = ECA()(x)  # efficient channel attention\n","\n","        x = tf.keras.layers.Dense(channel_size, use_bias=True, name=name + \"_project_conv\")(x)\n","\n","        if drop_rate > 0:\n","            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + \"_drop\")(x)\n","\n","        if channels_in == channel_size:\n","            x = tf.keras.layers.add([x, skip], name=name + \"_add\")\n","        return x\n","\n","    return apply\n","\n","\n","class MultiHeadSelfAttention(tf.keras.layers.Layer):\n","    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n","        super().__init__(**kwargs)\n","        self.dim = dim\n","        self.scale = self.dim**-0.5\n","        self.num_heads = num_heads\n","        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n","        self.drop1 = tf.keras.layers.Dropout(dropout)\n","        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        qkv = self.qkv(inputs)\n","        qkv = tf.keras.layers.Permute((2, 1, 3))(\n","            tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv)\n","        )\n","        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n","\n","        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n","\n","        if mask is not None:\n","            mask = mask[:, None, None, :]\n","\n","        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n","        attn = self.drop1(attn)\n","\n","        x = attn @ v\n","        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n","        x = self.proj(x)\n","        return x\n","\n","\n","def TransformerBlock(\n","    dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation=\"swish\"\n","):\n","    def apply(inputs):\n","        x = inputs\n","        x = tf.keras.layers.LayerNormalization()(x)\n","        x = MultiHeadSelfAttention(dim=dim, num_heads=num_heads, dropout=attn_dropout)(x)\n","        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1))(x)\n","        x = tf.keras.layers.Add()([inputs, x])\n","        attn_out = x\n","\n","        x = tf.keras.layers.LayerNormalization()(x)\n","        x = tf.keras.layers.Dense(dim * expand, use_bias=False, activation=activation)(x)\n","        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n","        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1))(x)\n","        x = tf.keras.layers.Add()([attn_out, x])\n","        return x\n","\n","    return apply\n","\n","\n","def build_model1(\n","    output_dim,\n","    max_len=64,\n","    dropout_step=0,\n","    dim=192,\n","    input_pad=-100,\n","    with_transformer=False,\n","    drop_rate=0.2,\n","):\n","    inp = tf.keras.Input(shape=(max_len, Constants.CHANNELS), dtype=tf.float32, name=\"inputs\")\n","    x = tf.keras.layers.Masking(mask_value=input_pad, input_shape=(max_len, Constants.CHANNELS))(\n","        inp\n","    )\n","    ksize = 17\n","    x = tf.keras.layers.Dense(dim, use_bias=False, name=\"stem_conv\")(x)\n","    x = tf.keras.layers.LayerNormalization(name=\"stem_bn\")(x)\n","\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    if with_transformer:\n","        x = TransformerBlock(dim, expand=2)(x)\n","\n","    #x = tf.keras.layers.AvgPool1D(2, 2)(x)\n"," \n","\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    if with_transformer:\n","        x = TransformerBlock(dim, expand=2)(x)\n","\n","    x = tf.keras.layers.AvgPool1D(2, 2)(x)\n","    x2=tf.keras.layers.Dense(dim)(x)\n","    lstm2 = tf.keras.layers.LSTM(units=output_dim, return_sequences=True)\n","    x3 = tf.keras.layers.Bidirectional(lstm2)(x2)\n","    x3 = tf.keras.layers.Dense(output_dim)(x3)  \n","    soft = tf.keras.activations.softmax(x3)\n","    logsoft=tf.nn.log_softmax(x3)\n","    x2=tf.keras.layers.Dense(dim)(soft)+x2\n","\n","    if dim == 384:  # for the 4x sized model\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        if with_transformer:\n","            x = TransformerBlock(dim, expand=2)(x)\n","\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        if with_transformer:\n","            x = TransformerBlock(dim, expand=2)(x)\n","\n","    lstm = tf.keras.layers.LSTM(units=output_dim, return_sequences=True)\n","    x = tf.keras.layers.Bidirectional(lstm)(x)\n","    x = LateDropout(0.3, start_step=dropout_step)(x)\n","    # x = tf.keras.layers.LayerNormalization()(x)\n","\n","    output = tf.keras.layers.Dense(output_dim, activation=\"log_softmax\")(x)  # logits\n","\n","    # x = tf.keras.layers.Dense(output_dim)(x)  # logits\n","    # outputs = tf.keras.layers.Activation(\"log_softmax\", dtype=\"float32\")(x)\n","    model = tf.keras.Model(inp, outputs=[output,logsoft])\n","    return model\n","\n","\n","def get_model(output_dim, max_len, dim, input_pad):\n","   \n","    model = build_model1(output_dim, max_len=max_len, input_pad=input_pad, dim=dim)\n","    return model\n"]},{"cell_type":"markdown","metadata":{},"source":["# Configuration"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.026971Z","iopub.status.busy":"2023-07-18T09:04:44.026546Z","iopub.status.idle":"2023-07-18T09:04:44.044853Z","shell.execute_reply":"2023-07-18T09:04:44.043616Z","shell.execute_reply.started":"2023-07-18T09:04:44.026941Z"},"trusted":true},"outputs":[],"source":["\n","def get_strategy():\n","    logical_devices = tf.config.list_logical_devices()\n","    # Check if TPU is available\n","\n","    gpu_available = any(\"GPU\" in device.name for device in logical_devices)\n","    strategy = None\n","    is_tpu = False\n","    try:\n","        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","        print(\"Running on TPU \", tpu.master())\n","        is_tpu = True\n","    except ValueError:\n","        is_tpu = False\n","\n","    if is_tpu:\n","        tf.config.experimental_connect_to_cluster(tpu)\n","        strategy = tf.distribute.TPUStrategy(tpu)\n","        disable_eager_execution()  # LSTM layer can't use bfloat16 unless we do this.\n","\n","    else:\n","        if gpu_available:\n","          \n","            ngpu = len(gpus)\n","            print(\"Num GPUs Available: \", ngpu)\n","            if ngpu > 1:\n","                strategy = tf.distribute.MirroredStrategy()\n","            else:\n","                strategy = tf.distribute.get_strategy()\n","\n","        else:\n","            print(\"Runing on CPU\")\n","            strategy = tf.distribute.get_strategy()\n","    replicas = strategy.num_replicas_in_sync\n","\n","    print(f\"get strategy replicas: {replicas}\")\n","\n","    return strategy, replicas, is_tpu\n","\n","\n","class CFG:\n","    # These 3 variables are update dynamically later by calling update_config_with_strategy.\n","    strategy = None  # type: ignore\n","    replicas = 1\n","    is_tpu = False\n","\n","    save_output = True\n","    log_path = \"/kaggle/working/\"\n","    input_path = \"/kaggle/input/asl-fingerspelling/\"\n","    output_path = \"/kaggle/working/\"\n","\n","    seed = 42\n","    verbose = 1  # 0) silent 1) progress bar 2) one line per epoch\n","\n","    # max number of frames\n","    #max_len = 256\n","    max_len = 256\n","    replicas = 1\n","    \n","    lr = 5e-4   # 5e-4\n","    weight_decay = 1e-4  # 4e-4\n","    epochs = 80 \n","    batch_size=128\n","    snapshot_epochs = []  # type: ignore\n","    swa_epochs = []  # type: ignore\n","    # list(range(epoch//2,epoch+1))\n","\n","    fp16 = True\n","    \n","    awp = False\n","    awp_lambda = 0.2\n","    awp_start_epoch = 15\n","    dropout_start_epoch = 15\n","    resume = 0\n","    \n","    dim = 384\n","    \n","    comment = f\"model-{dim}-seed{seed}\"\n","    output_dim = 61\n","    num_eval = 6\n","\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.046817Z","iopub.status.busy":"2023-07-18T09:04:44.046403Z","iopub.status.idle":"2023-07-18T09:04:44.063046Z","shell.execute_reply":"2023-07-18T09:04:44.062007Z","shell.execute_reply.started":"2023-07-18T09:04:44.046786Z"},"trusted":true},"outputs":[],"source":["\n","def update_config_with_strategy(config):\n","    # cfg is configuration instance\n","    strategy, replicas, is_tpu = get_strategy()\n","    config.strategy = strategy\n","    config.replicas = replicas\n","    config.is_tpu = is_tpu\n","    config.lr = config.lr * replicas\n","    config.batch_size = config.batch_size * replicas\n","    return config"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.065242Z","iopub.status.busy":"2023-07-18T09:04:44.064883Z","iopub.status.idle":"2023-07-18T09:04:44.114564Z","shell.execute_reply":"2023-07-18T09:04:44.112776Z","shell.execute_reply.started":"2023-07-18T09:04:44.065211Z"},"trusted":true},"outputs":[],"source":["\n","def count_data_items(dataset):\n","    dataset_size = 0\n","    for _ in dataset:\n","        dataset_size += 1\n","    return dataset_size\n","\n","\n","def interp1d_(x, target_len):\n","    target_len = tf.maximum(1, target_len)\n","    x = tf.image.resize(x, (target_len, tf.shape(x)[1]))\n","    return x\n","\n","\n","def tf_nan_mean(x, axis=0, keepdims=False):\n","    return tf.reduce_sum(\n","        tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims\n","    ) / tf.reduce_sum(\n","        tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims\n","    )\n","\n","\n","def tf_nan_std(x, center=None, axis=0, keepdims=False):\n","    if center is None:\n","        center = tf_nan_mean(x, axis=axis, keepdims=True)\n","    d = x - center\n","    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n","\n","\n","def flip_lr(x):\n","    if x.shape[1] == Constants.ROWS_PER_FRAME:\n","        LHAND = Constants.LHAND\n","        RHAND = Constants.RHAND\n","        LLIP = Constants.LLIP\n","        RLIP = Constants.RLIP\n","        LEYE = Constants.LEYE\n","        REYE = Constants.REYE\n","        LNOSE = Constants.LNOSE\n","        RNOSE = Constants.RNOSE\n","        LPOSE = Constants.LPOSE\n","        RPOSE = Constants.RPOSE\n","    else:\n","        LHAND = Constants.LANDMARK_INDICES[\"LHAND\"]\n","        RHAND = Constants.LANDMARK_INDICES[\"RHAND\"]\n","        LLIP = Constants.LANDMARK_INDICES[\"LLIP\"]\n","        RLIP = Constants.LANDMARK_INDICES[\"RLIP\"]\n","        LEYE = Constants.LANDMARK_INDICES[\"LEYE\"]\n","        REYE = Constants.LANDMARK_INDICES[\"REYE\"]\n","        LNOSE = Constants.LANDMARK_INDICES[\"LNOSE\"]\n","        RNOSE = Constants.LANDMARK_INDICES[\"RNOSE\"]\n","        LPOSE = Constants.LANDMARK_INDICES[\"LPOSE\"]\n","        RPOSE = Constants.LANDMARK_INDICES[\"RPOSE\"]\n","\n","    x, y = tf.unstack(x, axis=-1)\n","    x = 1 - x\n","    new_x = tf.stack([x, y], -1)\n","    new_x = tf.transpose(new_x, [1, 0, 2])\n","    lhand = tf.gather(new_x, LHAND, axis=0)\n","    rhand = tf.gather(new_x, RHAND, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LHAND)[..., None], rhand)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RHAND)[..., None], lhand)\n","    llip = tf.gather(new_x, LLIP, axis=0)\n","    rlip = tf.gather(new_x, RLIP, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LLIP)[..., None], rlip)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RLIP)[..., None], llip)\n","    lpose = tf.gather(new_x, LPOSE, axis=0)\n","    rpose = tf.gather(new_x, RPOSE, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LPOSE)[..., None], rpose)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RPOSE)[..., None], lpose)\n","    leye = tf.gather(new_x, LEYE, axis=0)\n","    reye = tf.gather(new_x, REYE, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LEYE)[..., None], reye)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(REYE)[..., None], leye)\n","    lnose = tf.gather(new_x, LNOSE, axis=0)\n","    rnose = tf.gather(new_x, RNOSE, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LNOSE)[..., None], rnose)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RNOSE)[..., None], lnose)\n","    new_x = tf.transpose(new_x, [1, 0, 2])\n","    return new_x\n","\n","\n","def resample(x, rate=(0.8, 1.2)):\n","    rate = tf.random.uniform((), rate[0], rate[1])\n","    length = tf.shape(x)[0]\n","    new_size = tf.cast(rate * tf.cast(length, tf.float32), tf.int32)\n","    new_x = interp1d_(x, new_size)\n","    return new_x\n","\n","\n","def spatial_random_affine(\n","    xyz,\n","    scale=(0.8, 1.2),\n","    shear=(-0.1, 0.1),\n","    shift=(-0.1, 0.1),\n","    degree=(-20, 20),\n","):\n","    center = tf.constant([0.5, 0.5])\n","    if degree is not None:\n","        xy = xyz[..., :2]\n","        z = xyz[..., 2:]\n","        xy -= center\n","        degree = tf.random.uniform((), *degree)\n","        radian = degree / 180 * np.pi\n","        c = tf.math.cos(radian)\n","        s = tf.math.sin(radian)\n","        rotate_mat = tf.identity(\n","            [\n","                [c, s],\n","                [-s, c],\n","            ]\n","        )\n","        xy = xy @ rotate_mat\n","        xy = xy + center\n","        xyz = tf.concat([xy, z], axis=-1)\n","\n","    if scale is not None:\n","        scale = tf.random.uniform((), *scale)\n","        xyz = scale * xyz\n","\n","    if shear is not None:\n","        xy = xyz[..., :2]\n","        z = xyz[..., 2:]\n","        shear_x = shear_y = tf.random.uniform((), *shear)\n","        if tf.random.uniform(()) < 0.5:\n","            shear_x = 0.0\n","        else:\n","            shear_y = 0.0\n","        shear_mat = tf.identity([[1.0, shear_x], [shear_y, 1.0]])\n","        xy = xy @ shear_mat\n","        xyz = tf.concat([xy, z], axis=-1)\n","\n","    if shift is not None:\n","        shift = tf.random.uniform((), *shift)\n","        xyz = xyz + shift\n","\n","    return xyz\n","\n","\n","def temporal_mask(x, size=[1, 15], mask_value=float(\"nan\")):\n","    l0 = tf.shape(x)[0]\n","    if size[1] > l0 // 8:\n","        size[1] = l0 // 8\n","        if size[1] <= 1:\n","            size[1] = 2\n","    mask_size = tf.random.uniform((), *size, dtype=tf.int32)\n","    mask_offset = tf.random.uniform((), 0, tf.clip_by_value(l0 - mask_size, 1, l0), dtype=tf.int32)\n","    x = tf.tensor_scatter_nd_update(\n","        x,\n","        tf.range(mask_offset, mask_offset + mask_size)[..., None],\n","        tf.fill([mask_size, tf.shape(x)[1], 2], mask_value),\n","    )\n","    return x\n","\n","\n","def spatial_mask(x, size=(0.05, 0.2), mask_value=float(\"nan\")):\n","    mask_offset_y = tf.random.uniform(())\n","    mask_offset_x = tf.random.uniform(())\n","    mask_size = tf.random.uniform((), *size)\n","    mask_x = (mask_offset_x < x[..., 0]) & (x[..., 0] < mask_offset_x + mask_size)\n","    mask_y = (mask_offset_y < x[..., 1]) & (x[..., 1] < mask_offset_y + mask_size)\n","    mask = mask_x & mask_y\n","    x = tf.where(mask[..., None], mask_value, x)\n","    return x\n","\n","\n","#@tf.function()\n","def augment_fn(x):\n","    # shape (T,F)\n","    x = tf.reshape(x, (tf.shape(x)[0], -1, 2))\n","    if tf.random.uniform(()) < 0.5:\n","        x = resample(x, (0.5, 1.5))\n","    if tf.random.uniform(()) < 0.5:\n","        x = flip_lr(x)\n","    if tf.random.uniform(()) < 0.5:\n","        x = spatial_random_affine(x)\n","    if tf.random.uniform(()) < 0.3:\n","        x = temporal_mask(x)\n","    if tf.random.uniform(()) < 0.3:\n","        x = spatial_mask(x)\n","    x = tf.reshape(x, (tf.shape(x)[0], -1))\n","    return x\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.117655Z","iopub.status.busy":"2023-07-18T09:04:44.117104Z","iopub.status.idle":"2023-07-18T09:04:44.142302Z","shell.execute_reply":"2023-07-18T09:04:44.140662Z","shell.execute_reply.started":"2023-07-18T09:04:44.117609Z"},"trusted":true},"outputs":[],"source":["\n","class Preprocess(tf.keras.layers.Layer):\n","    def __init__(self, max_len, normalize=False, **kwargs):\n","        super().__init__(**kwargs)\n","        self.max_len = max_len\n","        self.center = Constants.CENTER_INDICES\n","        self.normalize = normalize\n","\n","    # preprocess a batch of data\n","    def call(self, x):\n","        # rank is 3: [B,T,F]\n","        # if your input is just [T,F], extend its dimesnion before calling.\n","\n","        x = tf.reshape(x, (tf.shape(x)[0], tf.shape(x)[1], -1, 2))\n","        # dimensions now are [B,T,F//2,2]\n","\n","        x_selected = x\n","        if self.normalize:\n","            mean = tf_nan_mean(tf.gather(x, self.center, axis=2), axis=[1, 2], keepdims=True)\n","            mean = tf.where(tf.math.is_nan(mean), tf.constant(0.5, x.dtype), mean)\n","            std = tf_nan_std(x_selected, center=mean, axis=[1, 2], keepdims=True)\n","            x = (x_selected - mean) / std\n","        else:\n","            x = x_selected\n","\n","        dx = tf.cond(\n","            tf.shape(x)[1] > 1,\n","            lambda: tf.pad(x[:, 1:] - x[:, :-1], [[0, 0], [0, 1], [0, 0], [0, 0]]),\n","            lambda: tf.zeros_like(x),\n","        )\n","\n","        dx2 = tf.cond(\n","            tf.shape(x)[1] > 2,\n","            lambda: tf.pad(x[:, 2:] - x[:, :-2], [[0, 0], [0, 2], [0, 0], [0, 0]]),\n","            lambda: tf.zeros_like(x),\n","        )\n","        length = tf.shape(x)[1]\n","\n","        x = tf.concat(\n","            [\n","                tf.reshape(x, (-1, length, 2 * Constants.NUM_NODES)),  # x1,y1,x2,y2,...\n","                tf.reshape(dx, (-1, length, 2 * Constants.NUM_NODES)),\n","                tf.reshape(dx2, (-1, length, 2 * Constants.NUM_NODES)),\n","            ],\n","            axis=-1,\n","        )\n","\n","        # x1,y1,x2,y2,...dx1,dy1,dx2,dy2,...\n","        x = tf.where(tf.math.is_nan(x), tf.constant(0.0, x.dtype), x)\n","        return x\n","\n","\n","def pad_if_short(x, max_len):\n","    # shape (T,F)\n","    pad_len = max_len - tf.shape(x)[0]\n","    padding = tf.ones((pad_len, tf.shape(x)[1]), dtype=x.dtype) * Constants.INPUT_PAD\n","    x = tf.concat([x, padding], axis=0)\n","    return x\n","\n","\n","def shrink_if_long(x, max_len):\n","    # shape is [T,F]\n","    if tf.shape(x)[0] > max_len:\n","        # we need to extend the dimension to [T,F,channels]  for tf.image.resize\n","        x = tf.image.resize(x[..., None], (max_len, tf.shape(x)[1]))\n","        x = tf.squeeze(x, axis=2)\n","    return x\n","\n","def preprocess(x, max_len, do_pad=True):\n","    # shape (T,F)\n","    x = shrink_if_long(x, max_len=max_len)\n","    # Preprocess expects a batch, so we extend the dimension to (None,T,F), then reduce the output back to (T,F).\n","    x = tf.cast(Preprocess(max_len=max_len)(x[None, ...])[0], tf.float32)\n","\n","    if do_pad:  # we can avoid this step if there is batch padding\n","        x = pad_if_short(x, max_len=max_len)\n","\n","    return x"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.144369Z","iopub.status.busy":"2023-07-18T09:04:44.143941Z","iopub.status.idle":"2023-07-18T09:04:44.160043Z","shell.execute_reply":"2023-07-18T09:04:44.159041Z","shell.execute_reply.started":"2023-07-18T09:04:44.144335Z"},"trusted":true},"outputs":[],"source":["\n","def decode_tfrec(record_bytes):\n","    features = tf.io.parse_single_example(\n","        record_bytes,\n","        {\n","            \"coordinates\": tf.io.VarLenFeature(tf.float32),\n","            \"label\": tf.io.VarLenFeature(tf.int64),\n","        },\n","    )\n","    coords = tf.sparse.to_dense(features[\"coordinates\"])\n","    coords = tf.reshape(coords, (-1, Constants.NUM_INPUT_FEATURES))\n","    label = tf.sparse.to_dense(features[\"label\"])\n","\n","    return (coords, label)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.167121Z","iopub.status.busy":"2023-07-18T09:04:44.166717Z","iopub.status.idle":"2023-07-18T09:04:44.179273Z","shell.execute_reply":"2023-07-18T09:04:44.17748Z","shell.execute_reply.started":"2023-07-18T09:04:44.167092Z"},"trusted":true},"outputs":[],"source":["\n","def get_dataset(\n","    filenames,\n","    input_path,\n","    max_len,\n","    batch_size=64,\n","    drop_remainder=False,\n","    augment=False,\n","    shuffle_buffer=None,\n","    repeat=False,\n","    use_tfrecords=True,\n","):\n","    ignore_order = tf.data.Options()\n","    ignore_order.experimental_deterministic = False\n","\n","    \n","    ds = tf.data.TFRecordDataset(\n","        filenames, num_parallel_reads=tf.data.AUTOTUNE, compression_type=\"GZIP\"\n","    )\n","    ds = ds.map(decode_tfrec, tf.data.AUTOTUNE)\n","  \n","    ds.with_options(ignore_order)\n","    \n","    if augment:\n","        ds = ds.map(lambda x, y: (augment_fn(x), y), tf.data.AUTOTUNE)\n","    \n","    ds = ds.map(lambda x, y: (preprocess(x, max_len=max_len, do_pad=False), y), tf.data.AUTOTUNE)\n","    #if repeat:\n","    #    ds = ds.repeat()\n","    \n","    if shuffle_buffer is not None:\n","        ds = ds.shuffle(shuffle_buffer)\n","  \n","    ds = ds.padded_batch(\n","        batch_size,\n","        padding_values=(\n","            tf.constant(Constants.INPUT_PAD, dtype=tf.float32),\n","            tf.constant(Constants.LABEL_PAD, dtype=tf.int64),\n","        ),\n","        padded_shapes=([max_len, Constants.CHANNELS], [Constants.MAX_STRING_LEN]),\n","        drop_remainder=drop_remainder,\n","    )\n","    \n","    #tf.data.experimental.assert_cardinality(len(labels) // BATCH_SIZE)\n","\n","    ds = ds.prefetch(tf.data.AUTOTUNE)\n","\n","    return ds\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.182143Z","iopub.status.busy":"2023-07-18T09:04:44.181727Z","iopub.status.idle":"2023-07-18T09:04:44.208936Z","shell.execute_reply":"2023-07-18T09:04:44.207184Z","shell.execute_reply.started":"2023-07-18T09:04:44.182112Z"},"trusted":true},"outputs":[],"source":["\n","def train_run(train_files, valid_files, config, num_train, experiment_id=0, use_tfrecords=True,summary=False):\n","    gc.collect()\n","    tf.keras.backend.clear_session()\n","    # tf.config.optimizer.set_jit(\"autoclustering\")\n","\n","    if config.fp16:\n","        if config.is_tpu:\n","            policy = \"mixed_bfloat16\"\n","        else:\n","            policy = \"mixed_float16\"\n","    else:\n","        policy = \"float32\"\n","    tf.keras.mixed_precision.set_global_policy(policy)\n","\n","    augment_train= True\n","    repeat_train = True\n","\n","    shuffle_buffer = 4096\n","    train_ds = get_dataset(\n","        train_files,\n","        input_path=config.input_path,\n","        max_len=config.max_len,\n","        batch_size=config.batch_size,\n","        drop_remainder=True,\n","        augment=augment_train,\n","        repeat=repeat_train,\n","        shuffle_buffer=shuffle_buffer,\n","        use_tfrecords=True,\n","    )\n","    if valid_files is not None:\n","        valid_ds = get_dataset(\n","            valid_files,\n","            input_path=config.input_path,\n","            max_len=config.max_len,\n","            batch_size=config.batch_size,\n","            use_tfrecords=True,\n","        )\n","    else:\n","        valid_ds = None\n","        valid_files = []\n","\n","    # num_train = count_data_items(train_ds)\n","    # num_valid = count_data_items(valid_ds)\n","    # print(num_train, num_valid, config.batch_size)\n","    # exit()\n","\n","    steps_per_epoch = num_train // config.batch_size\n","    strategy = config.strategy\n","    with strategy.scope():\n","        model = get_model(\n","            max_len=config.max_len,\n","            output_dim=config.output_dim,\n","            input_pad=Constants.INPUT_PAD,\n","            dim=config.dim,\n","        )\n"," \n","        base_lr = config.lr\n","        lr_schedule = CosineDecay(\n","            initial_learning_rate=base_lr / 10,\n","            decay_steps=int(0.95 * steps_per_epoch * config.epochs),\n","            alpha=0.02,\n","            name=None,\n","            warmup_target=base_lr,\n","            warmup_steps=int(0.05 * steps_per_epoch * config.epochs),\n","        )\n","        \n","        opt = tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=config.weight_decay)\n","        #opt = tf.keras.optimizers.AdamW(learning_rate=2e-4)\n","        ctc_loss = CTCLoss(pad_token_idx=Constants.LABEL_PAD)\n","\n","        model.compile(\n","            optimizer=opt,\n","            loss=[ctc_loss,ctc_loss],\n","            loss_weights=[0.5,0.5],\n","            metrics=[\n","                LevDistanceMetric(),\n","            ],\n","            #jit_compile= not config.is_tpu # Should be False on TPU!!\n","        )\n","\n","    if summary:\n","        print()\n","        model.summary()\n","        print()\n","        print(train_ds, valid_ds)\n","        print()\n","    print(f\"---------experiment {experiment_id}---------\")\n","    print(f\"train:{num_train} \")\n","    print()\n","\n","    if config.resume:\n","        print(f\"resume from epoch{config.resume}\")\n","        model.load_weights(f\"{config.log_path}/{config.comment}-exp{experiment_id}-last.h5\")\n","        if train_ds is not None:\n","            model.evaluate(train_ds.take(steps_per_epoch))\n","        if valid_ds is not None:\n","            model.evaluate(valid_ds)\n","\n","    tb_logger = tf.keras.callbacks.TensorBoard(\n","        log_dir=\"config.log_path\", histogram_freq=0, write_graph=True, write_images=True\n","    )\n","    sv_loss = tf.keras.callbacks.ModelCheckpoint(\n","        f\"{config.log_path}/{config.comment}-exp{experiment_id}-best.h5\",\n","        monitor=\"val_loss\",\n","        verbose=1,\n","        save_best_only=True,\n","        save_weights_only=True,\n","        mode=\"min\",\n","        save_freq=\"epoch\",\n","    )\n","  \n","    # Callback function to check transcription on the val set.\n","    # validation_callback = CallbackEval(model, valid_ds)\n","    memory_usage = MemoryUsageCallbackExtended()\n","    callbacks = []\n","    if config.save_output:\n","        callbacks.append(tb_logger)\n","        # callbacks.append(swa)\n","        callbacks.append(sv_loss)\n","    callbacks.append(memory_usage)\n","    callbacks.append(tf.keras.callbacks.TerminateOnNaN())\n","    # callbacks.append(validation_callback)\n","\n","    history = model.fit(\n","        train_ds,\n","        epochs=config.epochs - config.resume,\n","        #steps_per_epoch=steps_per_epoch,\n","        callbacks=callbacks,\n","        validation_data=valid_ds,\n","        verbose=config.verbose,\n","        # validation_steps=None,\n","    )\n","\n","    if config.save_output:  # reload the saved best weights checkpoint\n","        saved_based_model = f\"{config.log_path}/{config.comment}-exp{experiment_id}-best.h5\"\n","        if os.path.exists(saved_based_model):\n","            model.load_weights(saved_based_model)\n","        else:\n","            print(f\"Warning: could not find {saved_based_model}\")\n","    if valid_ds is not None:\n","        cv = model.evaluate(valid_ds, verbose=config.verbose)\n","    else:\n","        cv = None\n","    return model, cv, history\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.210911Z","iopub.status.busy":"2023-07-18T09:04:44.21051Z","iopub.status.idle":"2023-07-18T09:04:44.227683Z","shell.execute_reply":"2023-07-18T09:04:44.226169Z","shell.execute_reply.started":"2023-07-18T09:04:44.21088Z"},"trusted":true},"outputs":[],"source":["\n","def train(cfg=CFG, experiment_id=0, use_supplemental=True):\n","    tf.keras.backend.clear_session()\n","    config = cfg()\n","    update_config_with_strategy(config)\n","    print(f\"using {config.replicas} replicas\")\n","    print(f\"batch size {config.batch_size}\")\n","    print(f\"fp16={config.fp16}\")\n","    seed_everything(config.seed)\n","    \n","    all_filenames = sorted(glob.glob(\"/kaggle/input/asl-preprocessing/records/*.tfrecord\"))\n","    regular = [x for x in all_filenames if \"supp\" not in x]\n","    supp = [x for x in all_filenames if \"supp\" in x]\n","    \n","    data_filenames = regular\n","    if use_supplemental:\n","        data_filenames += supp\n","    print(\"Using TFRECORDS\")\n","    \n","  \n","    valid_files = data_filenames[: config.num_eval]  # first part in list\n","    train_files = data_filenames[config.num_eval :]\n","    random.shuffle(train_files)\n","    \n","    \n","    df1 = pd.read_csv(config.input_path + \"train.csv\")\n","    df2 = pd.read_csv(config.input_path + \"supplemental_metadata.csv\")\n","    df_info = pd.concat([df1, df2])\n","    \n","    #ds = get_dataset(train_files, CFG.input_path,max_len=CFG.max_len, augment=False, batch_size=64)\n","    #print(ds)\n","    #for x,y in ds:\n","    #    print(x,y)\n","    #raise\n","    \n","    if use_supplemental:\n","        num_train = 3567 * 32  # with supplemental\n","    else:\n","        num_train = 1912 * 32  # without supplemental\n","  \n","    train_run(\n","        train_files,\n","        valid_files,\n","        config,\n","        num_train,\n","        summary=False,\n","        experiment_id=experiment_id,\n","        use_tfrecords=True,\n","    )\n","    \n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.231071Z","iopub.status.busy":"2023-07-18T09:04:44.229724Z","iopub.status.idle":"2023-07-18T09:04:44.497144Z","shell.execute_reply":"2023-07-18T09:04:44.495773Z","shell.execute_reply.started":"2023-07-18T09:04:44.231015Z"},"trusted":true},"outputs":[],"source":["gc.collect()\n","tf.keras.backend.clear_session()"]},{"cell_type":"markdown","metadata":{},"source":["# Train It!"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.499483Z","iopub.status.busy":"2023-07-18T09:04:44.499005Z","iopub.status.idle":"2023-07-18T09:04:44.505927Z","shell.execute_reply":"2023-07-18T09:04:44.504277Z","shell.execute_reply.started":"2023-07-18T09:04:44.49944Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  1\n","get strategy replicas: 1\n","using 1 replicas\n","batch size 128\n","fp16=True\n","Using TFRECORDS\n","INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n","Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3060 Ti, compute capability 8.6\n"]},{"name":"stderr","output_type":"stream","text":["2023-07-19 10:21:43.971263: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-19 10:21:43.971412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-19 10:21:43.971502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-19 10:21:44.875243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-19 10:21:44.875450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-19 10:21:44.875620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-19 10:21:44.875736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1831] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5608 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n","2023-07-19 10:21:45.051692: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-19 10:21:46.281261: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"]},{"name":"stdout","output_type":"stream","text":["---------experiment 0---------\n","train:114144 \n","\n","Epoch 1/80\n","WARNING:tensorflow:From /home/sronen/code/.venv/lib/python3.10/site-packages/tensorflow/python/ops/ctc_ops.py:1514: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\n","WARNING:tensorflow:From /home/sronen/code/.venv/lib/python3.10/site-packages/tensorflow/python/ops/ctc_ops.py:1497: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\n"]},{"name":"stderr","output_type":"stream","text":["2023-07-19 10:22:07.822380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:440] Loaded cuDNN version 8901\n","2023-07-19 10:22:08.174291: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"]},{"name":"stdout","output_type":"stream","text":["      5/Unknown - 23s 494ms/step - loss: 427.1500 - dense_3_loss: 430.0500 - tf.nn.log_softmax_loss: 424.2000 - dense_3_Lev: 0.0000e+00 - tf.nn.log_softmax_Lev: 0.0000e+00"]},{"name":"stderr","output_type":"stream","text":["2023-07-19 10:22:13.054185: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f471c057ec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2023-07-19 10:22:13.054205: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Ti, Compute Capability 8.6\n","2023-07-19 10:22:13.062410: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","2023-07-19 10:22:13.150228: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"]},{"name":"stdout","output_type":"stream","text":["    891/Unknown - 482s 517ms/step - loss: 90.2392 - dense_3_loss: 87.1317 - tf.nn.log_softmax_loss: 93.3462 - dense_3_Lev: 0.0000e+00 - tf.nn.log_softmax_Lev: 0.0017"]},{"name":"stderr","output_type":"stream","text":["2023-07-19 10:29:51.082736: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14636731949007016663\n","2023-07-19 10:29:51.082762: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5898872308319091555\n","2023-07-19 10:29:51.082769: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14325292773810307957\n","2023-07-19 10:29:51.082774: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14233161986450140093\n","2023-07-19 10:29:51.082780: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6673920992748748862\n","2023-07-19 10:29:51.082807: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10803775347702043644\n","2023-07-19 10:30:09.054276: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8981036001774190784\n","2023-07-19 10:30:09.054308: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 445244564130325033\n","2023-07-19 10:30:09.054323: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9623511319840123901\n","2023-07-19 10:30:09.054330: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14457791103803209183\n","2023-07-19 10:30:09.054334: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13831977509210365623\n","2023-07-19 10:30:09.054341: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3465288978825197219\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 1: val_loss improved from inf to 66.48195, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.76 GB\n","891/891 [==============================] - 500s 538ms/step - loss: 90.2392 - dense_3_loss: 87.1317 - tf.nn.log_softmax_loss: 93.3462 - dense_3_Lev: 0.0000e+00 - tf.nn.log_softmax_Lev: 0.0017 - val_loss: 66.4819 - val_dense_3_loss: 66.0638 - val_tf.nn.log_softmax_loss: 66.9088 - val_dense_3_Lev: 0.0000e+00 - val_tf.nn.log_softmax_Lev: 0.0000e+00\n","Epoch 2/80\n","891/891 [==============================] - ETA: 0s - loss: 78.1285 - dense_3_loss: 78.2075 - tf.nn.log_softmax_loss: 78.0492 - dense_3_Lev: 0.0018 - tf.nn.log_softmax_Lev: 3.8363e-07\n","Epoch 2: val_loss improved from 66.48195 to 65.83914, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 8.74 GB\n","891/891 [==============================] - 475s 531ms/step - loss: 78.1285 - dense_3_loss: 78.2075 - tf.nn.log_softmax_loss: 78.0492 - dense_3_Lev: 0.0018 - tf.nn.log_softmax_Lev: 3.8363e-07 - val_loss: 65.8391 - val_dense_3_loss: 65.4957 - val_tf.nn.log_softmax_loss: 66.1804 - val_dense_3_Lev: 0.0000e+00 - val_tf.nn.log_softmax_Lev: 0.0000e+00\n","Epoch 3/80\n","891/891 [==============================] - ETA: 0s - loss: 76.3474 - dense_3_loss: 76.3179 - tf.nn.log_softmax_loss: 76.3776 - dense_3_Lev: 0.0140 - tf.nn.log_softmax_Lev: 0.0036\n","Epoch 3: val_loss improved from 65.83914 to 62.44859, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.99 GB\n","891/891 [==============================] - 460s 514ms/step - loss: 76.3474 - dense_3_loss: 76.3179 - tf.nn.log_softmax_loss: 76.3776 - dense_3_Lev: 0.0140 - tf.nn.log_softmax_Lev: 0.0036 - val_loss: 62.4486 - val_dense_3_loss: 62.0025 - val_tf.nn.log_softmax_loss: 62.8879 - val_dense_3_Lev: 0.0157 - val_tf.nn.log_softmax_Lev: 0.0036\n","Epoch 4/80\n","891/891 [==============================] - ETA: 0s - loss: 71.8198 - dense_3_loss: 71.7123 - tf.nn.log_softmax_loss: 71.9250 - dense_3_Lev: 0.0447 - tf.nn.log_softmax_Lev: 0.0199\n","Epoch 4: val_loss improved from 62.44859 to 58.40902, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 7.56 GB\n","891/891 [==============================] - 467s 522ms/step - loss: 71.8198 - dense_3_loss: 71.7123 - tf.nn.log_softmax_loss: 71.9250 - dense_3_Lev: 0.0447 - tf.nn.log_softmax_Lev: 0.0199 - val_loss: 58.4090 - val_dense_3_loss: 58.4958 - val_tf.nn.log_softmax_loss: 58.3221 - val_dense_3_Lev: 0.0421 - val_tf.nn.log_softmax_Lev: 0.0310\n","Epoch 5/80\n","891/891 [==============================] - ETA: 0s - loss: 59.4439 - dense_3_loss: 60.2043 - tf.nn.log_softmax_loss: 58.6834 - dense_3_Lev: 0.1875 - tf.nn.log_softmax_Lev: 0.1801\n","Epoch 5: val_loss improved from 58.40902 to 35.10353, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.38 GB\n","891/891 [==============================] - 478s 534ms/step - loss: 59.4439 - dense_3_loss: 60.2043 - tf.nn.log_softmax_loss: 58.6834 - dense_3_Lev: 0.1875 - tf.nn.log_softmax_Lev: 0.1801 - val_loss: 35.1035 - val_dense_3_loss: 34.8904 - val_tf.nn.log_softmax_loss: 35.3164 - val_dense_3_Lev: 0.4277 - val_tf.nn.log_softmax_Lev: 0.4463\n","Epoch 6/80\n","891/891 [==============================] - ETA: 0s - loss: 42.0912 - dense_3_loss: 41.7295 - tf.nn.log_softmax_loss: 42.4524 - dense_3_Lev: 0.4818 - tf.nn.log_softmax_Lev: 0.4581\n","Epoch 6: val_loss improved from 35.10353 to 28.47371, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 7.12 GB\n","891/891 [==============================] - 470s 525ms/step - loss: 42.0912 - dense_3_loss: 41.7295 - tf.nn.log_softmax_loss: 42.4524 - dense_3_Lev: 0.4818 - tf.nn.log_softmax_Lev: 0.4581 - val_loss: 28.4737 - val_dense_3_loss: 27.6986 - val_tf.nn.log_softmax_loss: 29.2491 - val_dense_3_Lev: 0.5844 - val_tf.nn.log_softmax_Lev: 0.5582\n","Epoch 7/80\n","891/891 [==============================] - ETA: 0s - loss: 37.7764 - dense_3_loss: 37.1506 - tf.nn.log_softmax_loss: 38.4020 - dense_3_Lev: 0.5410 - tf.nn.log_softmax_Lev: 0.5128\n","Epoch 7: val_loss improved from 28.47371 to 26.48931, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.96 GB\n","891/891 [==============================] - 462s 517ms/step - loss: 37.7764 - dense_3_loss: 37.1506 - tf.nn.log_softmax_loss: 38.4020 - dense_3_Lev: 0.5410 - tf.nn.log_softmax_Lev: 0.5128 - val_loss: 26.4893 - val_dense_3_loss: 25.7128 - val_tf.nn.log_softmax_loss: 27.2675 - val_dense_3_Lev: 0.6094 - val_tf.nn.log_softmax_Lev: 0.5812\n","Epoch 8/80\n","891/891 [==============================] - ETA: 0s - loss: 35.6642 - dense_3_loss: 34.9318 - tf.nn.log_softmax_loss: 36.3970 - dense_3_Lev: 0.5687 - tf.nn.log_softmax_Lev: 0.5390\n","Epoch 8: val_loss improved from 26.48931 to 25.73403, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 7.05 GB\n","891/891 [==============================] - 473s 529ms/step - loss: 35.6642 - dense_3_loss: 34.9318 - tf.nn.log_softmax_loss: 36.3970 - dense_3_Lev: 0.5687 - tf.nn.log_softmax_Lev: 0.5390 - val_loss: 25.7340 - val_dense_3_loss: 24.8178 - val_tf.nn.log_softmax_loss: 26.6482 - val_dense_3_Lev: 0.6242 - val_tf.nn.log_softmax_Lev: 0.5998\n","Epoch 9/80\n","891/891 [==============================] - ETA: 0s - loss: 34.3227 - dense_3_loss: 33.5102 - tf.nn.log_softmax_loss: 35.1350 - dense_3_Lev: 0.5863 - tf.nn.log_softmax_Lev: 0.5556\n","Epoch 9: val_loss improved from 25.73403 to 24.50388, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.51 GB\n","891/891 [==============================] - 474s 530ms/step - loss: 34.3227 - dense_3_loss: 33.5102 - tf.nn.log_softmax_loss: 35.1350 - dense_3_Lev: 0.5863 - tf.nn.log_softmax_Lev: 0.5556 - val_loss: 24.5039 - val_dense_3_loss: 23.6162 - val_tf.nn.log_softmax_loss: 25.3915 - val_dense_3_Lev: 0.6379 - val_tf.nn.log_softmax_Lev: 0.6122\n","Epoch 10/80\n","891/891 [==============================] - ETA: 0s - loss: 33.2291 - dense_3_loss: 32.3528 - tf.nn.log_softmax_loss: 34.1049 - dense_3_Lev: 0.6004 - tf.nn.log_softmax_Lev: 0.5686\n","Epoch 10: val_loss improved from 24.50388 to 24.20822, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 7.10 GB\n","891/891 [==============================] - 486s 544ms/step - loss: 33.2291 - dense_3_loss: 32.3528 - tf.nn.log_softmax_loss: 34.1049 - dense_3_Lev: 0.6004 - tf.nn.log_softmax_Lev: 0.5686 - val_loss: 24.2082 - val_dense_3_loss: 23.2815 - val_tf.nn.log_softmax_loss: 25.1370 - val_dense_3_Lev: 0.6499 - val_tf.nn.log_softmax_Lev: 0.6200\n","Epoch 11/80\n","891/891 [==============================] - ETA: 0s - loss: 32.3346 - dense_3_loss: 31.4052 - tf.nn.log_softmax_loss: 33.2636 - dense_3_Lev: 0.6123 - tf.nn.log_softmax_Lev: 0.5799\n","Epoch 11: val_loss improved from 24.20822 to 23.32994, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.06 GB\n","891/891 [==============================] - 488s 545ms/step - loss: 32.3346 - dense_3_loss: 31.4052 - tf.nn.log_softmax_loss: 33.2636 - dense_3_Lev: 0.6123 - tf.nn.log_softmax_Lev: 0.5799 - val_loss: 23.3299 - val_dense_3_loss: 22.4318 - val_tf.nn.log_softmax_loss: 24.2307 - val_dense_3_Lev: 0.6588 - val_tf.nn.log_softmax_Lev: 0.6313\n","Epoch 12/80\n","891/891 [==============================] - ETA: 0s - loss: 31.5465 - dense_3_loss: 30.5599 - tf.nn.log_softmax_loss: 32.5325 - dense_3_Lev: 0.6224 - tf.nn.log_softmax_Lev: 0.5891\n","Epoch 12: val_loss improved from 23.32994 to 22.51265, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.67 GB\n","891/891 [==============================] - 475s 531ms/step - loss: 31.5465 - dense_3_loss: 30.5599 - tf.nn.log_softmax_loss: 32.5325 - dense_3_Lev: 0.6224 - tf.nn.log_softmax_Lev: 0.5891 - val_loss: 22.5127 - val_dense_3_loss: 21.6122 - val_tf.nn.log_softmax_loss: 23.4142 - val_dense_3_Lev: 0.6695 - val_tf.nn.log_softmax_Lev: 0.6430\n","Epoch 13/80\n","891/891 [==============================] - ETA: 0s - loss: 30.8715 - dense_3_loss: 29.8542 - tf.nn.log_softmax_loss: 31.8896 - dense_3_Lev: 0.6316 - tf.nn.log_softmax_Lev: 0.5979\n","Epoch 13: val_loss improved from 22.51265 to 22.12369, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.88 GB\n","891/891 [==============================] - 476s 533ms/step - loss: 30.8715 - dense_3_loss: 29.8542 - tf.nn.log_softmax_loss: 31.8896 - dense_3_Lev: 0.6316 - tf.nn.log_softmax_Lev: 0.5979 - val_loss: 22.1237 - val_dense_3_loss: 21.2321 - val_tf.nn.log_softmax_loss: 23.0114 - val_dense_3_Lev: 0.6777 - val_tf.nn.log_softmax_Lev: 0.6555\n","Epoch 14/80\n","891/891 [==============================] - ETA: 0s - loss: 30.3273 - dense_3_loss: 29.2700 - tf.nn.log_softmax_loss: 31.3849 - dense_3_Lev: 0.6388 - tf.nn.log_softmax_Lev: 0.6047\n","Epoch 14: val_loss improved from 22.12369 to 21.87647, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.82 GB\n","891/891 [==============================] - 474s 530ms/step - loss: 30.3273 - dense_3_loss: 29.2700 - tf.nn.log_softmax_loss: 31.3849 - dense_3_Lev: 0.6388 - tf.nn.log_softmax_Lev: 0.6047 - val_loss: 21.8765 - val_dense_3_loss: 21.0655 - val_tf.nn.log_softmax_loss: 22.6891 - val_dense_3_Lev: 0.6788 - val_tf.nn.log_softmax_Lev: 0.6496\n","Epoch 15/80\n","891/891 [==============================] - ETA: 0s - loss: 29.7694 - dense_3_loss: 28.6774 - tf.nn.log_softmax_loss: 30.8613 - dense_3_Lev: 0.6465 - tf.nn.log_softmax_Lev: 0.6120\n","Epoch 15: val_loss improved from 21.87647 to 21.38185, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.64 GB\n","891/891 [==============================] - 477s 534ms/step - loss: 29.7694 - dense_3_loss: 28.6774 - tf.nn.log_softmax_loss: 30.8613 - dense_3_Lev: 0.6465 - tf.nn.log_softmax_Lev: 0.6120 - val_loss: 21.3818 - val_dense_3_loss: 20.6038 - val_tf.nn.log_softmax_loss: 22.1592 - val_dense_3_Lev: 0.6847 - val_tf.nn.log_softmax_Lev: 0.6586\n","Epoch 16/80\n","891/891 [==============================] - ETA: 0s - loss: 29.3154 - dense_3_loss: 28.1845 - tf.nn.log_softmax_loss: 30.4467 - dense_3_Lev: 0.6519 - tf.nn.log_softmax_Lev: 0.6174\n","Epoch 16: val_loss improved from 21.38185 to 20.98933, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.72 GB\n","891/891 [==============================] - 474s 530ms/step - loss: 29.3154 - dense_3_loss: 28.1845 - tf.nn.log_softmax_loss: 30.4467 - dense_3_Lev: 0.6519 - tf.nn.log_softmax_Lev: 0.6174 - val_loss: 20.9893 - val_dense_3_loss: 20.1318 - val_tf.nn.log_softmax_loss: 21.8479 - val_dense_3_Lev: 0.6933 - val_tf.nn.log_softmax_Lev: 0.6646\n","Epoch 17/80\n","891/891 [==============================] - ETA: 0s - loss: 28.8167 - dense_3_loss: 27.6702 - tf.nn.log_softmax_loss: 29.9631 - dense_3_Lev: 0.6584 - tf.nn.log_softmax_Lev: 0.6238\n","Epoch 17: val_loss did not improve from 20.98933\n","Memory usage on epoch end: 5.43 GB\n","891/891 [==============================] - 475s 532ms/step - loss: 28.8167 - dense_3_loss: 27.6702 - tf.nn.log_softmax_loss: 29.9631 - dense_3_Lev: 0.6584 - tf.nn.log_softmax_Lev: 0.6238 - val_loss: 21.1446 - val_dense_3_loss: 20.1763 - val_tf.nn.log_softmax_loss: 22.1139 - val_dense_3_Lev: 0.6948 - val_tf.nn.log_softmax_Lev: 0.6727\n","Epoch 18/80\n","891/891 [==============================] - ETA: 0s - loss: 28.3857 - dense_3_loss: 27.1967 - tf.nn.log_softmax_loss: 29.5748 - dense_3_Lev: 0.6641 - tf.nn.log_softmax_Lev: 0.6291\n","Epoch 18: val_loss improved from 20.98933 to 20.49385, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.36 GB\n","891/891 [==============================] - 473s 529ms/step - loss: 28.3857 - dense_3_loss: 27.1967 - tf.nn.log_softmax_loss: 29.5748 - dense_3_Lev: 0.6641 - tf.nn.log_softmax_Lev: 0.6291 - val_loss: 20.4939 - val_dense_3_loss: 19.6354 - val_tf.nn.log_softmax_loss: 21.3533 - val_dense_3_Lev: 0.7021 - val_tf.nn.log_softmax_Lev: 0.6784\n","Epoch 19/80\n","891/891 [==============================] - ETA: 0s - loss: 28.0343 - dense_3_loss: 26.8287 - tf.nn.log_softmax_loss: 29.2402 - dense_3_Lev: 0.6685 - tf.nn.log_softmax_Lev: 0.6339\n","Epoch 19: val_loss improved from 20.49385 to 20.32875, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.52 GB\n","891/891 [==============================] - 475s 531ms/step - loss: 28.0343 - dense_3_loss: 26.8287 - tf.nn.log_softmax_loss: 29.2402 - dense_3_Lev: 0.6685 - tf.nn.log_softmax_Lev: 0.6339 - val_loss: 20.3288 - val_dense_3_loss: 19.6041 - val_tf.nn.log_softmax_loss: 21.0530 - val_dense_3_Lev: 0.7037 - val_tf.nn.log_softmax_Lev: 0.6798\n","Epoch 20/80\n","891/891 [==============================] - ETA: 0s - loss: 27.6853 - dense_3_loss: 26.4515 - tf.nn.log_softmax_loss: 28.9190 - dense_3_Lev: 0.6732 - tf.nn.log_softmax_Lev: 0.6383\n","Epoch 20: val_loss did not improve from 20.32875\n","Memory usage on epoch end: 6.37 GB\n","891/891 [==============================] - 476s 532ms/step - loss: 27.6853 - dense_3_loss: 26.4515 - tf.nn.log_softmax_loss: 28.9190 - dense_3_Lev: 0.6732 - tf.nn.log_softmax_Lev: 0.6383 - val_loss: 20.4962 - val_dense_3_loss: 19.6645 - val_tf.nn.log_softmax_loss: 21.3279 - val_dense_3_Lev: 0.7065 - val_tf.nn.log_softmax_Lev: 0.6838\n","Epoch 21/80\n","891/891 [==============================] - ETA: 0s - loss: 27.3266 - dense_3_loss: 26.0677 - tf.nn.log_softmax_loss: 28.5849 - dense_3_Lev: 0.6781 - tf.nn.log_softmax_Lev: 0.6428\n","Epoch 21: val_loss improved from 20.32875 to 19.95628, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.44 GB\n","891/891 [==============================] - 477s 533ms/step - loss: 27.3266 - dense_3_loss: 26.0677 - tf.nn.log_softmax_loss: 28.5849 - dense_3_Lev: 0.6781 - tf.nn.log_softmax_Lev: 0.6428 - val_loss: 19.9563 - val_dense_3_loss: 19.1251 - val_tf.nn.log_softmax_loss: 20.7852 - val_dense_3_Lev: 0.7154 - val_tf.nn.log_softmax_Lev: 0.6898\n","Epoch 22/80\n","891/891 [==============================] - ETA: 0s - loss: 26.9627 - dense_3_loss: 25.6812 - tf.nn.log_softmax_loss: 28.2441 - dense_3_Lev: 0.6829 - tf.nn.log_softmax_Lev: 0.6471\n","Epoch 22: val_loss improved from 19.95628 to 19.72706, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.40 GB\n","891/891 [==============================] - 477s 534ms/step - loss: 26.9627 - dense_3_loss: 25.6812 - tf.nn.log_softmax_loss: 28.2441 - dense_3_Lev: 0.6829 - tf.nn.log_softmax_Lev: 0.6471 - val_loss: 19.7271 - val_dense_3_loss: 18.9745 - val_tf.nn.log_softmax_loss: 20.4803 - val_dense_3_Lev: 0.7135 - val_tf.nn.log_softmax_Lev: 0.6894\n","Epoch 23/80\n","891/891 [==============================] - ETA: 0s - loss: 26.6399 - dense_3_loss: 25.3336 - tf.nn.log_softmax_loss: 27.9464 - dense_3_Lev: 0.6868 - tf.nn.log_softmax_Lev: 0.6511\n","Epoch 23: val_loss improved from 19.72706 to 19.47687, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.07 GB\n","891/891 [==============================] - 474s 530ms/step - loss: 26.6399 - dense_3_loss: 25.3336 - tf.nn.log_softmax_loss: 27.9464 - dense_3_Lev: 0.6868 - tf.nn.log_softmax_Lev: 0.6511 - val_loss: 19.4769 - val_dense_3_loss: 18.7896 - val_tf.nn.log_softmax_loss: 20.1654 - val_dense_3_Lev: 0.7166 - val_tf.nn.log_softmax_Lev: 0.6915\n","Epoch 24/80\n","891/891 [==============================] - ETA: 0s - loss: 26.3600 - dense_3_loss: 25.0178 - tf.nn.log_softmax_loss: 27.7020 - dense_3_Lev: 0.6908 - tf.nn.log_softmax_Lev: 0.6542\n","Epoch 24: val_loss did not improve from 19.47687\n","Memory usage on epoch end: 6.60 GB\n","891/891 [==============================] - 475s 531ms/step - loss: 26.3600 - dense_3_loss: 25.0178 - tf.nn.log_softmax_loss: 27.7020 - dense_3_Lev: 0.6908 - tf.nn.log_softmax_Lev: 0.6542 - val_loss: 19.5974 - val_dense_3_loss: 18.7571 - val_tf.nn.log_softmax_loss: 20.4377 - val_dense_3_Lev: 0.7210 - val_tf.nn.log_softmax_Lev: 0.6975\n","Epoch 25/80\n","891/891 [==============================] - ETA: 0s - loss: 26.0851 - dense_3_loss: 24.7297 - tf.nn.log_softmax_loss: 27.4409 - dense_3_Lev: 0.6947 - tf.nn.log_softmax_Lev: 0.6577\n","Epoch 25: val_loss did not improve from 19.47687\n","Memory usage on epoch end: 5.61 GB\n","891/891 [==============================] - 479s 536ms/step - loss: 26.0851 - dense_3_loss: 24.7297 - tf.nn.log_softmax_loss: 27.4409 - dense_3_Lev: 0.6947 - tf.nn.log_softmax_Lev: 0.6577 - val_loss: 19.4782 - val_dense_3_loss: 18.7577 - val_tf.nn.log_softmax_loss: 20.2021 - val_dense_3_Lev: 0.7230 - val_tf.nn.log_softmax_Lev: 0.7008\n","Epoch 26/80\n","891/891 [==============================] - ETA: 0s - loss: 25.7978 - dense_3_loss: 24.4206 - tf.nn.log_softmax_loss: 27.1752 - dense_3_Lev: 0.6984 - tf.nn.log_softmax_Lev: 0.6611\n","Epoch 26: val_loss improved from 19.47687 to 19.31506, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.38 GB\n","891/891 [==============================] - 473s 528ms/step - loss: 25.7978 - dense_3_loss: 24.4206 - tf.nn.log_softmax_loss: 27.1752 - dense_3_Lev: 0.6984 - tf.nn.log_softmax_Lev: 0.6611 - val_loss: 19.3151 - val_dense_3_loss: 18.5621 - val_tf.nn.log_softmax_loss: 20.0676 - val_dense_3_Lev: 0.7231 - val_tf.nn.log_softmax_Lev: 0.6978\n","Epoch 27/80\n","891/891 [==============================] - ETA: 0s - loss: 25.5117 - dense_3_loss: 24.1205 - tf.nn.log_softmax_loss: 26.9031 - dense_3_Lev: 0.7019 - tf.nn.log_softmax_Lev: 0.6646\n","Epoch 27: val_loss improved from 19.31506 to 18.93690, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.78 GB\n","891/891 [==============================] - 470s 525ms/step - loss: 25.5117 - dense_3_loss: 24.1205 - tf.nn.log_softmax_loss: 26.9031 - dense_3_Lev: 0.7019 - tf.nn.log_softmax_Lev: 0.6646 - val_loss: 18.9369 - val_dense_3_loss: 18.1316 - val_tf.nn.log_softmax_loss: 19.7429 - val_dense_3_Lev: 0.7274 - val_tf.nn.log_softmax_Lev: 0.7047\n","Epoch 28/80\n","891/891 [==============================] - ETA: 0s - loss: 25.3050 - dense_3_loss: 23.8917 - tf.nn.log_softmax_loss: 26.7188 - dense_3_Lev: 0.7044 - tf.nn.log_softmax_Lev: 0.6669\n","Epoch 28: val_loss did not improve from 18.93690\n","Memory usage on epoch end: 6.30 GB\n","891/891 [==============================] - 462s 517ms/step - loss: 25.3050 - dense_3_loss: 23.8917 - tf.nn.log_softmax_loss: 26.7188 - dense_3_Lev: 0.7044 - tf.nn.log_softmax_Lev: 0.6669 - val_loss: 19.0916 - val_dense_3_loss: 18.3401 - val_tf.nn.log_softmax_loss: 19.8429 - val_dense_3_Lev: 0.7285 - val_tf.nn.log_softmax_Lev: 0.7082\n","Epoch 29/80\n","891/891 [==============================] - ETA: 0s - loss: 25.0542 - dense_3_loss: 23.6167 - tf.nn.log_softmax_loss: 26.4918 - dense_3_Lev: 0.7076 - tf.nn.log_softmax_Lev: 0.6700\n","Epoch 29: val_loss improved from 18.93690 to 18.65665, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.76 GB\n","891/891 [==============================] - 464s 519ms/step - loss: 25.0542 - dense_3_loss: 23.6167 - tf.nn.log_softmax_loss: 26.4918 - dense_3_Lev: 0.7076 - tf.nn.log_softmax_Lev: 0.6700 - val_loss: 18.6566 - val_dense_3_loss: 17.9381 - val_tf.nn.log_softmax_loss: 19.3736 - val_dense_3_Lev: 0.7318 - val_tf.nn.log_softmax_Lev: 0.7105\n","Epoch 30/80\n","891/891 [==============================] - ETA: 0s - loss: 24.8325 - dense_3_loss: 23.3580 - tf.nn.log_softmax_loss: 26.3074 - dense_3_Lev: 0.7108 - tf.nn.log_softmax_Lev: 0.6725\n","Epoch 30: val_loss did not improve from 18.65665\n","Memory usage on epoch end: 6.32 GB\n","891/891 [==============================] - 463s 518ms/step - loss: 24.8325 - dense_3_loss: 23.3580 - tf.nn.log_softmax_loss: 26.3074 - dense_3_Lev: 0.7108 - tf.nn.log_softmax_Lev: 0.6725 - val_loss: 18.7416 - val_dense_3_loss: 18.0568 - val_tf.nn.log_softmax_loss: 19.4258 - val_dense_3_Lev: 0.7344 - val_tf.nn.log_softmax_Lev: 0.7105\n","Epoch 31/80\n","891/891 [==============================] - ETA: 0s - loss: 24.5751 - dense_3_loss: 23.0932 - tf.nn.log_softmax_loss: 26.0576 - dense_3_Lev: 0.7144 - tf.nn.log_softmax_Lev: 0.6758\n","Epoch 31: val_loss improved from 18.65665 to 18.60551, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.89 GB\n","891/891 [==============================] - 462s 517ms/step - loss: 24.5751 - dense_3_loss: 23.0932 - tf.nn.log_softmax_loss: 26.0576 - dense_3_Lev: 0.7144 - tf.nn.log_softmax_Lev: 0.6758 - val_loss: 18.6055 - val_dense_3_loss: 17.8534 - val_tf.nn.log_softmax_loss: 19.3589 - val_dense_3_Lev: 0.7353 - val_tf.nn.log_softmax_Lev: 0.7118\n","Epoch 32/80\n","891/891 [==============================] - ETA: 0s - loss: 24.4021 - dense_3_loss: 22.9053 - tf.nn.log_softmax_loss: 25.8991 - dense_3_Lev: 0.7163 - tf.nn.log_softmax_Lev: 0.6778\n","Epoch 32: val_loss improved from 18.60551 to 18.57013, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.41 GB\n","891/891 [==============================] - 461s 516ms/step - loss: 24.4021 - dense_3_loss: 22.9053 - tf.nn.log_softmax_loss: 25.8991 - dense_3_Lev: 0.7163 - tf.nn.log_softmax_Lev: 0.6778 - val_loss: 18.5701 - val_dense_3_loss: 17.8402 - val_tf.nn.log_softmax_loss: 19.3002 - val_dense_3_Lev: 0.7391 - val_tf.nn.log_softmax_Lev: 0.7158\n","Epoch 33/80\n","891/891 [==============================] - ETA: 0s - loss: 24.1470 - dense_3_loss: 22.6455 - tf.nn.log_softmax_loss: 25.6489 - dense_3_Lev: 0.7195 - tf.nn.log_softmax_Lev: 0.6812\n","Epoch 33: val_loss improved from 18.57013 to 18.44048, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.06 GB\n","891/891 [==============================] - 461s 516ms/step - loss: 24.1470 - dense_3_loss: 22.6455 - tf.nn.log_softmax_loss: 25.6489 - dense_3_Lev: 0.7195 - tf.nn.log_softmax_Lev: 0.6812 - val_loss: 18.4405 - val_dense_3_loss: 17.7159 - val_tf.nn.log_softmax_loss: 19.1633 - val_dense_3_Lev: 0.7374 - val_tf.nn.log_softmax_Lev: 0.7180\n","Epoch 34/80\n","891/891 [==============================] - ETA: 0s - loss: 23.9427 - dense_3_loss: 22.4134 - tf.nn.log_softmax_loss: 25.4722 - dense_3_Lev: 0.7220 - tf.nn.log_softmax_Lev: 0.6831\n","Epoch 34: val_loss improved from 18.44048 to 18.34044, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.40 GB\n","891/891 [==============================] - 462s 517ms/step - loss: 23.9427 - dense_3_loss: 22.4134 - tf.nn.log_softmax_loss: 25.4722 - dense_3_Lev: 0.7220 - tf.nn.log_softmax_Lev: 0.6831 - val_loss: 18.3404 - val_dense_3_loss: 17.6251 - val_tf.nn.log_softmax_loss: 19.0555 - val_dense_3_Lev: 0.7394 - val_tf.nn.log_softmax_Lev: 0.7180\n","Epoch 35/80\n","891/891 [==============================] - ETA: 0s - loss: 23.7707 - dense_3_loss: 22.2181 - tf.nn.log_softmax_loss: 25.3234 - dense_3_Lev: 0.7245 - tf.nn.log_softmax_Lev: 0.6851\n","Epoch 35: val_loss improved from 18.34044 to 18.21459, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.14 GB\n","891/891 [==============================] - 465s 520ms/step - loss: 23.7707 - dense_3_loss: 22.2181 - tf.nn.log_softmax_loss: 25.3234 - dense_3_Lev: 0.7245 - tf.nn.log_softmax_Lev: 0.6851 - val_loss: 18.2146 - val_dense_3_loss: 17.4910 - val_tf.nn.log_softmax_loss: 18.9398 - val_dense_3_Lev: 0.7421 - val_tf.nn.log_softmax_Lev: 0.7211\n","Epoch 36/80\n","891/891 [==============================] - ETA: 0s - loss: 23.5615 - dense_3_loss: 22.0069 - tf.nn.log_softmax_loss: 25.1157 - dense_3_Lev: 0.7273 - tf.nn.log_softmax_Lev: 0.6877\n","Epoch 36: val_loss improved from 18.21459 to 18.15413, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.12 GB\n","891/891 [==============================] - 472s 528ms/step - loss: 23.5615 - dense_3_loss: 22.0069 - tf.nn.log_softmax_loss: 25.1157 - dense_3_Lev: 0.7273 - tf.nn.log_softmax_Lev: 0.6877 - val_loss: 18.1541 - val_dense_3_loss: 17.4022 - val_tf.nn.log_softmax_loss: 18.9048 - val_dense_3_Lev: 0.7389 - val_tf.nn.log_softmax_Lev: 0.7179\n","Epoch 37/80\n","891/891 [==============================] - ETA: 0s - loss: 23.3858 - dense_3_loss: 21.8134 - tf.nn.log_softmax_loss: 24.9584 - dense_3_Lev: 0.7293 - tf.nn.log_softmax_Lev: 0.6897\n","Epoch 37: val_loss did not improve from 18.15413\n","Memory usage on epoch end: 5.85 GB\n","891/891 [==============================] - 475s 531ms/step - loss: 23.3858 - dense_3_loss: 21.8134 - tf.nn.log_softmax_loss: 24.9584 - dense_3_Lev: 0.7293 - tf.nn.log_softmax_Lev: 0.6897 - val_loss: 18.1591 - val_dense_3_loss: 17.4447 - val_tf.nn.log_softmax_loss: 18.8752 - val_dense_3_Lev: 0.7429 - val_tf.nn.log_softmax_Lev: 0.7234\n","Epoch 38/80\n","891/891 [==============================] - ETA: 0s - loss: 23.1804 - dense_3_loss: 21.6004 - tf.nn.log_softmax_loss: 24.7609 - dense_3_Lev: 0.7319 - tf.nn.log_softmax_Lev: 0.6920\n","Epoch 38: val_loss improved from 18.15413 to 17.82063, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.31 GB\n","891/891 [==============================] - 474s 530ms/step - loss: 23.1804 - dense_3_loss: 21.6004 - tf.nn.log_softmax_loss: 24.7609 - dense_3_Lev: 0.7319 - tf.nn.log_softmax_Lev: 0.6920 - val_loss: 17.8206 - val_dense_3_loss: 17.1113 - val_tf.nn.log_softmax_loss: 18.5302 - val_dense_3_Lev: 0.7444 - val_tf.nn.log_softmax_Lev: 0.7224\n","Epoch 39/80\n","891/891 [==============================] - ETA: 0s - loss: 23.0352 - dense_3_loss: 21.4255 - tf.nn.log_softmax_loss: 24.6447 - dense_3_Lev: 0.7342 - tf.nn.log_softmax_Lev: 0.6936\n","Epoch 39: val_loss did not improve from 17.82063\n","Memory usage on epoch end: 6.00 GB\n","891/891 [==============================] - 460s 514ms/step - loss: 23.0352 - dense_3_loss: 21.4255 - tf.nn.log_softmax_loss: 24.6447 - dense_3_Lev: 0.7342 - tf.nn.log_softmax_Lev: 0.6936 - val_loss: 17.8818 - val_dense_3_loss: 17.1857 - val_tf.nn.log_softmax_loss: 18.5805 - val_dense_3_Lev: 0.7471 - val_tf.nn.log_softmax_Lev: 0.7257\n","Epoch 40/80\n","891/891 [==============================] - ETA: 0s - loss: 22.8245 - dense_3_loss: 21.2129 - tf.nn.log_softmax_loss: 24.4360 - dense_3_Lev: 0.7363 - tf.nn.log_softmax_Lev: 0.6959\n","Epoch 40: val_loss did not improve from 17.82063\n","Memory usage on epoch end: 6.43 GB\n","891/891 [==============================] - 456s 510ms/step - loss: 22.8245 - dense_3_loss: 21.2129 - tf.nn.log_softmax_loss: 24.4360 - dense_3_Lev: 0.7363 - tf.nn.log_softmax_Lev: 0.6959 - val_loss: 17.9363 - val_dense_3_loss: 17.2913 - val_tf.nn.log_softmax_loss: 18.5794 - val_dense_3_Lev: 0.7481 - val_tf.nn.log_softmax_Lev: 0.7277\n","Epoch 41/80\n","891/891 [==============================] - ETA: 0s - loss: 22.6747 - dense_3_loss: 21.0444 - tf.nn.log_softmax_loss: 24.3044 - dense_3_Lev: 0.7389 - tf.nn.log_softmax_Lev: 0.6982\n","Epoch 41: val_loss improved from 17.82063 to 17.79162, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.20 GB\n","891/891 [==============================] - 463s 518ms/step - loss: 22.6747 - dense_3_loss: 21.0444 - tf.nn.log_softmax_loss: 24.3044 - dense_3_Lev: 0.7389 - tf.nn.log_softmax_Lev: 0.6982 - val_loss: 17.7916 - val_dense_3_loss: 17.1685 - val_tf.nn.log_softmax_loss: 18.4156 - val_dense_3_Lev: 0.7500 - val_tf.nn.log_softmax_Lev: 0.7279\n","Epoch 42/80\n","891/891 [==============================] - ETA: 0s - loss: 22.4988 - dense_3_loss: 20.8461 - tf.nn.log_softmax_loss: 24.1514 - dense_3_Lev: 0.7411 - tf.nn.log_softmax_Lev: 0.7000\n","Epoch 42: val_loss improved from 17.79162 to 17.66163, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.49 GB\n","891/891 [==============================] - 469s 525ms/step - loss: 22.4988 - dense_3_loss: 20.8461 - tf.nn.log_softmax_loss: 24.1514 - dense_3_Lev: 0.7411 - tf.nn.log_softmax_Lev: 0.7000 - val_loss: 17.6616 - val_dense_3_loss: 17.0425 - val_tf.nn.log_softmax_loss: 18.2819 - val_dense_3_Lev: 0.7500 - val_tf.nn.log_softmax_Lev: 0.7292\n","Epoch 43/80\n","891/891 [==============================] - ETA: 0s - loss: 22.2915 - dense_3_loss: 20.6402 - tf.nn.log_softmax_loss: 23.9427 - dense_3_Lev: 0.7434 - tf.nn.log_softmax_Lev: 0.7019\n","Epoch 43: val_loss improved from 17.66163 to 17.60449, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.17 GB\n","891/891 [==============================] - 470s 525ms/step - loss: 22.2915 - dense_3_loss: 20.6402 - tf.nn.log_softmax_loss: 23.9427 - dense_3_Lev: 0.7434 - tf.nn.log_softmax_Lev: 0.7019 - val_loss: 17.6045 - val_dense_3_loss: 16.8785 - val_tf.nn.log_softmax_loss: 18.3326 - val_dense_3_Lev: 0.7497 - val_tf.nn.log_softmax_Lev: 0.7302\n","Epoch 44/80\n","891/891 [==============================] - ETA: 0s - loss: 22.1749 - dense_3_loss: 20.5099 - tf.nn.log_softmax_loss: 23.8394 - dense_3_Lev: 0.7450 - tf.nn.log_softmax_Lev: 0.7035\n","Epoch 44: val_loss did not improve from 17.60449\n","Memory usage on epoch end: 6.54 GB\n","891/891 [==============================] - 471s 527ms/step - loss: 22.1749 - dense_3_loss: 20.5099 - tf.nn.log_softmax_loss: 23.8394 - dense_3_Lev: 0.7450 - tf.nn.log_softmax_Lev: 0.7035 - val_loss: 17.6263 - val_dense_3_loss: 16.9870 - val_tf.nn.log_softmax_loss: 18.2639 - val_dense_3_Lev: 0.7520 - val_tf.nn.log_softmax_Lev: 0.7298\n","Epoch 45/80\n","891/891 [==============================] - ETA: 0s - loss: 21.9746 - dense_3_loss: 20.2983 - tf.nn.log_softmax_loss: 23.6508 - dense_3_Lev: 0.7474 - tf.nn.log_softmax_Lev: 0.7057\n","Epoch 45: val_loss improved from 17.60449 to 17.47668, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.03 GB\n","891/891 [==============================] - 475s 531ms/step - loss: 21.9746 - dense_3_loss: 20.2983 - tf.nn.log_softmax_loss: 23.6508 - dense_3_Lev: 0.7474 - tf.nn.log_softmax_Lev: 0.7057 - val_loss: 17.4767 - val_dense_3_loss: 16.8221 - val_tf.nn.log_softmax_loss: 18.1290 - val_dense_3_Lev: 0.7537 - val_tf.nn.log_softmax_Lev: 0.7322\n","Epoch 46/80\n","891/891 [==============================] - ETA: 0s - loss: 21.8639 - dense_3_loss: 20.1672 - tf.nn.log_softmax_loss: 23.5602 - dense_3_Lev: 0.7491 - tf.nn.log_softmax_Lev: 0.7068\n","Epoch 46: val_loss improved from 17.47668 to 17.43326, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.32 GB\n","891/891 [==============================] - 472s 528ms/step - loss: 21.8639 - dense_3_loss: 20.1672 - tf.nn.log_softmax_loss: 23.5602 - dense_3_Lev: 0.7491 - tf.nn.log_softmax_Lev: 0.7068 - val_loss: 17.4333 - val_dense_3_loss: 16.7542 - val_tf.nn.log_softmax_loss: 18.1075 - val_dense_3_Lev: 0.7540 - val_tf.nn.log_softmax_Lev: 0.7331\n","Epoch 47/80\n","891/891 [==============================] - ETA: 0s - loss: 21.6941 - dense_3_loss: 19.9996 - tf.nn.log_softmax_loss: 23.3884 - dense_3_Lev: 0.7508 - tf.nn.log_softmax_Lev: 0.7088\n","Epoch 47: val_loss did not improve from 17.43326\n","Memory usage on epoch end: 6.09 GB\n","891/891 [==============================] - 475s 531ms/step - loss: 21.6941 - dense_3_loss: 19.9996 - tf.nn.log_softmax_loss: 23.3884 - dense_3_Lev: 0.7508 - tf.nn.log_softmax_Lev: 0.7088 - val_loss: 17.4443 - val_dense_3_loss: 16.8881 - val_tf.nn.log_softmax_loss: 18.0008 - val_dense_3_Lev: 0.7568 - val_tf.nn.log_softmax_Lev: 0.7346\n","Epoch 48/80\n","891/891 [==============================] - ETA: 0s - loss: 21.5748 - dense_3_loss: 19.8590 - tf.nn.log_softmax_loss: 23.2907 - dense_3_Lev: 0.7527 - tf.nn.log_softmax_Lev: 0.7103\n","Epoch 48: val_loss did not improve from 17.43326\n","Memory usage on epoch end: 6.69 GB\n","891/891 [==============================] - 475s 532ms/step - loss: 21.5748 - dense_3_loss: 19.8590 - tf.nn.log_softmax_loss: 23.2907 - dense_3_Lev: 0.7527 - tf.nn.log_softmax_Lev: 0.7103 - val_loss: 17.4472 - val_dense_3_loss: 16.8483 - val_tf.nn.log_softmax_loss: 18.0458 - val_dense_3_Lev: 0.7552 - val_tf.nn.log_softmax_Lev: 0.7334\n","Epoch 49/80\n","891/891 [==============================] - ETA: 0s - loss: 21.4385 - dense_3_loss: 19.7228 - tf.nn.log_softmax_loss: 23.1539 - dense_3_Lev: 0.7543 - tf.nn.log_softmax_Lev: 0.7117\n","Epoch 49: val_loss improved from 17.43326 to 17.36300, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.03 GB\n","891/891 [==============================] - 474s 530ms/step - loss: 21.4385 - dense_3_loss: 19.7228 - tf.nn.log_softmax_loss: 23.1539 - dense_3_Lev: 0.7543 - tf.nn.log_softmax_Lev: 0.7117 - val_loss: 17.3630 - val_dense_3_loss: 16.7852 - val_tf.nn.log_softmax_loss: 17.9423 - val_dense_3_Lev: 0.7573 - val_tf.nn.log_softmax_Lev: 0.7355\n","Epoch 50/80\n","891/891 [==============================] - ETA: 0s - loss: 21.2878 - dense_3_loss: 19.5494 - tf.nn.log_softmax_loss: 23.0263 - dense_3_Lev: 0.7562 - tf.nn.log_softmax_Lev: 0.7132\n","Epoch 50: val_loss improved from 17.36300 to 17.33047, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.63 GB\n","891/891 [==============================] - 467s 522ms/step - loss: 21.2878 - dense_3_loss: 19.5494 - tf.nn.log_softmax_loss: 23.0263 - dense_3_Lev: 0.7562 - tf.nn.log_softmax_Lev: 0.7132 - val_loss: 17.3305 - val_dense_3_loss: 16.7207 - val_tf.nn.log_softmax_loss: 17.9406 - val_dense_3_Lev: 0.7582 - val_tf.nn.log_softmax_Lev: 0.7366\n","Epoch 51/80\n","891/891 [==============================] - ETA: 0s - loss: 21.2002 - dense_3_loss: 19.4563 - tf.nn.log_softmax_loss: 22.9443 - dense_3_Lev: 0.7572 - tf.nn.log_softmax_Lev: 0.7143\n","Epoch 51: val_loss improved from 17.33047 to 17.28618, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.95 GB\n","891/891 [==============================] - 476s 533ms/step - loss: 21.2002 - dense_3_loss: 19.4563 - tf.nn.log_softmax_loss: 22.9443 - dense_3_Lev: 0.7572 - tf.nn.log_softmax_Lev: 0.7143 - val_loss: 17.2862 - val_dense_3_loss: 16.6805 - val_tf.nn.log_softmax_loss: 17.8913 - val_dense_3_Lev: 0.7597 - val_tf.nn.log_softmax_Lev: 0.7382\n","Epoch 52/80\n","891/891 [==============================] - ETA: 0s - loss: 21.0219 - dense_3_loss: 19.2580 - tf.nn.log_softmax_loss: 22.7854 - dense_3_Lev: 0.7595 - tf.nn.log_softmax_Lev: 0.7162\n","Epoch 52: val_loss did not improve from 17.28618\n","Memory usage on epoch end: 6.19 GB\n","891/891 [==============================] - 477s 533ms/step - loss: 21.0219 - dense_3_loss: 19.2580 - tf.nn.log_softmax_loss: 22.7854 - dense_3_Lev: 0.7595 - tf.nn.log_softmax_Lev: 0.7162 - val_loss: 17.3682 - val_dense_3_loss: 16.7683 - val_tf.nn.log_softmax_loss: 17.9674 - val_dense_3_Lev: 0.7609 - val_tf.nn.log_softmax_Lev: 0.7394\n","Epoch 53/80\n","891/891 [==============================] - ETA: 0s - loss: 20.9163 - dense_3_loss: 19.1687 - tf.nn.log_softmax_loss: 22.6643 - dense_3_Lev: 0.7608 - tf.nn.log_softmax_Lev: 0.7178\n","Epoch 53: val_loss did not improve from 17.28618\n","Memory usage on epoch end: 6.12 GB\n","891/891 [==============================] - 477s 534ms/step - loss: 20.9163 - dense_3_loss: 19.1687 - tf.nn.log_softmax_loss: 22.6643 - dense_3_Lev: 0.7608 - tf.nn.log_softmax_Lev: 0.7178 - val_loss: 17.2949 - val_dense_3_loss: 16.7169 - val_tf.nn.log_softmax_loss: 17.8736 - val_dense_3_Lev: 0.7600 - val_tf.nn.log_softmax_Lev: 0.7405\n","Epoch 54/80\n","891/891 [==============================] - ETA: 0s - loss: 20.7703 - dense_3_loss: 19.0013 - tf.nn.log_softmax_loss: 22.5400 - dense_3_Lev: 0.7625 - tf.nn.log_softmax_Lev: 0.7190\n","Epoch 54: val_loss improved from 17.28618 to 17.21226, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.19 GB\n","891/891 [==============================] - 471s 527ms/step - loss: 20.7703 - dense_3_loss: 19.0013 - tf.nn.log_softmax_loss: 22.5400 - dense_3_Lev: 0.7625 - tf.nn.log_softmax_Lev: 0.7190 - val_loss: 17.2123 - val_dense_3_loss: 16.6806 - val_tf.nn.log_softmax_loss: 17.7432 - val_dense_3_Lev: 0.7609 - val_tf.nn.log_softmax_Lev: 0.7395\n","Epoch 55/80\n","891/891 [==============================] - ETA: 0s - loss: 20.6411 - dense_3_loss: 18.8688 - tf.nn.log_softmax_loss: 22.4132 - dense_3_Lev: 0.7644 - tf.nn.log_softmax_Lev: 0.7206\n","Epoch 55: val_loss improved from 17.21226 to 17.13233, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.98 GB\n","891/891 [==============================] - 464s 519ms/step - loss: 20.6411 - dense_3_loss: 18.8688 - tf.nn.log_softmax_loss: 22.4132 - dense_3_Lev: 0.7644 - tf.nn.log_softmax_Lev: 0.7206 - val_loss: 17.1323 - val_dense_3_loss: 16.5293 - val_tf.nn.log_softmax_loss: 17.7342 - val_dense_3_Lev: 0.7626 - val_tf.nn.log_softmax_Lev: 0.7419\n","Epoch 56/80\n","891/891 [==============================] - ETA: 0s - loss: 20.5436 - dense_3_loss: 18.7603 - tf.nn.log_softmax_loss: 22.3269 - dense_3_Lev: 0.7654 - tf.nn.log_softmax_Lev: 0.7215\n","Epoch 56: val_loss improved from 17.13233 to 17.12983, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.27 GB\n","891/891 [==============================] - 471s 527ms/step - loss: 20.5436 - dense_3_loss: 18.7603 - tf.nn.log_softmax_loss: 22.3269 - dense_3_Lev: 0.7654 - tf.nn.log_softmax_Lev: 0.7215 - val_loss: 17.1298 - val_dense_3_loss: 16.5593 - val_tf.nn.log_softmax_loss: 17.7022 - val_dense_3_Lev: 0.7620 - val_tf.nn.log_softmax_Lev: 0.7404\n","Epoch 57/80\n","891/891 [==============================] - ETA: 0s - loss: 20.4536 - dense_3_loss: 18.6743 - tf.nn.log_softmax_loss: 22.2330 - dense_3_Lev: 0.7668 - tf.nn.log_softmax_Lev: 0.7226\n","Epoch 57: val_loss improved from 17.12983 to 17.08988, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.14 GB\n","891/891 [==============================] - 471s 527ms/step - loss: 20.4536 - dense_3_loss: 18.6743 - tf.nn.log_softmax_loss: 22.2330 - dense_3_Lev: 0.7668 - tf.nn.log_softmax_Lev: 0.7226 - val_loss: 17.0899 - val_dense_3_loss: 16.4957 - val_tf.nn.log_softmax_loss: 17.6824 - val_dense_3_Lev: 0.7631 - val_tf.nn.log_softmax_Lev: 0.7415\n","Epoch 58/80\n","891/891 [==============================] - ETA: 0s - loss: 20.3118 - dense_3_loss: 18.5241 - tf.nn.log_softmax_loss: 22.0997 - dense_3_Lev: 0.7685 - tf.nn.log_softmax_Lev: 0.7244\n","Epoch 58: val_loss improved from 17.08988 to 16.97875, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.12 GB\n","891/891 [==============================] - 478s 535ms/step - loss: 20.3118 - dense_3_loss: 18.5241 - tf.nn.log_softmax_loss: 22.0997 - dense_3_Lev: 0.7685 - tf.nn.log_softmax_Lev: 0.7244 - val_loss: 16.9787 - val_dense_3_loss: 16.4206 - val_tf.nn.log_softmax_loss: 17.5373 - val_dense_3_Lev: 0.7639 - val_tf.nn.log_softmax_Lev: 0.7421\n","Epoch 59/80\n","891/891 [==============================] - ETA: 0s - loss: 20.2724 - dense_3_loss: 18.4729 - tf.nn.log_softmax_loss: 22.0722 - dense_3_Lev: 0.7691 - tf.nn.log_softmax_Lev: 0.7248\n","Epoch 59: val_loss did not improve from 16.97875\n","Memory usage on epoch end: 5.59 GB\n","891/891 [==============================] - 479s 535ms/step - loss: 20.2724 - dense_3_loss: 18.4729 - tf.nn.log_softmax_loss: 22.0722 - dense_3_Lev: 0.7691 - tf.nn.log_softmax_Lev: 0.7248 - val_loss: 17.0808 - val_dense_3_loss: 16.4490 - val_tf.nn.log_softmax_loss: 17.7119 - val_dense_3_Lev: 0.7631 - val_tf.nn.log_softmax_Lev: 0.7427\n","Epoch 60/80\n","891/891 [==============================] - ETA: 0s - loss: 20.1541 - dense_3_loss: 18.3552 - tf.nn.log_softmax_loss: 21.9531 - dense_3_Lev: 0.7703 - tf.nn.log_softmax_Lev: 0.7258\n","Epoch 60: val_loss did not improve from 16.97875\n","Memory usage on epoch end: 6.05 GB\n","891/891 [==============================] - 477s 533ms/step - loss: 20.1541 - dense_3_loss: 18.3552 - tf.nn.log_softmax_loss: 21.9531 - dense_3_Lev: 0.7703 - tf.nn.log_softmax_Lev: 0.7258 - val_loss: 17.0569 - val_dense_3_loss: 16.5193 - val_tf.nn.log_softmax_loss: 17.5948 - val_dense_3_Lev: 0.7647 - val_tf.nn.log_softmax_Lev: 0.7438\n","Epoch 61/80\n","891/891 [==============================] - ETA: 0s - loss: 20.0399 - dense_3_loss: 18.2390 - tf.nn.log_softmax_loss: 21.8406 - dense_3_Lev: 0.7720 - tf.nn.log_softmax_Lev: 0.7275\n","Epoch 61: val_loss did not improve from 16.97875\n","Memory usage on epoch end: 5.59 GB\n","891/891 [==============================] - 479s 536ms/step - loss: 20.0399 - dense_3_loss: 18.2390 - tf.nn.log_softmax_loss: 21.8406 - dense_3_Lev: 0.7720 - tf.nn.log_softmax_Lev: 0.7275 - val_loss: 17.0381 - val_dense_3_loss: 16.4921 - val_tf.nn.log_softmax_loss: 17.5854 - val_dense_3_Lev: 0.7650 - val_tf.nn.log_softmax_Lev: 0.7428\n","Epoch 62/80\n","891/891 [==============================] - ETA: 0s - loss: 19.9684 - dense_3_loss: 18.1521 - tf.nn.log_softmax_loss: 21.7847 - dense_3_Lev: 0.7729 - tf.nn.log_softmax_Lev: 0.7278\n","Epoch 62: val_loss did not improve from 16.97875\n","Memory usage on epoch end: 5.88 GB\n","891/891 [==============================] - 474s 529ms/step - loss: 19.9684 - dense_3_loss: 18.1521 - tf.nn.log_softmax_loss: 21.7847 - dense_3_Lev: 0.7729 - tf.nn.log_softmax_Lev: 0.7278 - val_loss: 16.9883 - val_dense_3_loss: 16.4588 - val_tf.nn.log_softmax_loss: 17.5190 - val_dense_3_Lev: 0.7654 - val_tf.nn.log_softmax_Lev: 0.7440\n","Epoch 63/80\n","891/891 [==============================] - ETA: 0s - loss: 19.8712 - dense_3_loss: 18.0503 - tf.nn.log_softmax_loss: 21.6920 - dense_3_Lev: 0.7739 - tf.nn.log_softmax_Lev: 0.7292\n","Epoch 63: val_loss improved from 16.97875 to 16.95973, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.48 GB\n","891/891 [==============================] - 473s 529ms/step - loss: 19.8712 - dense_3_loss: 18.0503 - tf.nn.log_softmax_loss: 21.6920 - dense_3_Lev: 0.7739 - tf.nn.log_softmax_Lev: 0.7292 - val_loss: 16.9597 - val_dense_3_loss: 16.4182 - val_tf.nn.log_softmax_loss: 17.5015 - val_dense_3_Lev: 0.7667 - val_tf.nn.log_softmax_Lev: 0.7453\n","Epoch 64/80\n","891/891 [==============================] - ETA: 0s - loss: 19.7862 - dense_3_loss: 17.9674 - tf.nn.log_softmax_loss: 21.6052 - dense_3_Lev: 0.7750 - tf.nn.log_softmax_Lev: 0.7305\n","Epoch 64: val_loss did not improve from 16.95973\n","Memory usage on epoch end: 5.91 GB\n","891/891 [==============================] - 457s 511ms/step - loss: 19.7862 - dense_3_loss: 17.9674 - tf.nn.log_softmax_loss: 21.6052 - dense_3_Lev: 0.7750 - tf.nn.log_softmax_Lev: 0.7305 - val_loss: 16.9673 - val_dense_3_loss: 16.4184 - val_tf.nn.log_softmax_loss: 17.5156 - val_dense_3_Lev: 0.7668 - val_tf.nn.log_softmax_Lev: 0.7454\n","Epoch 65/80\n","891/891 [==============================] - ETA: 0s - loss: 19.7112 - dense_3_loss: 17.8882 - tf.nn.log_softmax_loss: 21.5342 - dense_3_Lev: 0.7759 - tf.nn.log_softmax_Lev: 0.7311\n","Epoch 65: val_loss did not improve from 16.95973\n","Memory usage on epoch end: 5.67 GB\n","891/891 [==============================] - 458s 512ms/step - loss: 19.7112 - dense_3_loss: 17.8882 - tf.nn.log_softmax_loss: 21.5342 - dense_3_Lev: 0.7759 - tf.nn.log_softmax_Lev: 0.7311 - val_loss: 16.9657 - val_dense_3_loss: 16.4065 - val_tf.nn.log_softmax_loss: 17.5260 - val_dense_3_Lev: 0.7671 - val_tf.nn.log_softmax_Lev: 0.7461\n","Epoch 66/80\n","891/891 [==============================] - ETA: 0s - loss: 19.6586 - dense_3_loss: 17.8197 - tf.nn.log_softmax_loss: 21.4975 - dense_3_Lev: 0.7766 - tf.nn.log_softmax_Lev: 0.7313\n","Epoch 66: val_loss improved from 16.95973 to 16.89087, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.91 GB\n","891/891 [==============================] - 476s 532ms/step - loss: 19.6586 - dense_3_loss: 17.8197 - tf.nn.log_softmax_loss: 21.4975 - dense_3_Lev: 0.7766 - tf.nn.log_softmax_Lev: 0.7313 - val_loss: 16.8909 - val_dense_3_loss: 16.3533 - val_tf.nn.log_softmax_loss: 17.4285 - val_dense_3_Lev: 0.7681 - val_tf.nn.log_softmax_Lev: 0.7465\n","Epoch 67/80\n","891/891 [==============================] - ETA: 0s - loss: 19.5788 - dense_3_loss: 17.7455 - tf.nn.log_softmax_loss: 21.4120 - dense_3_Lev: 0.7776 - tf.nn.log_softmax_Lev: 0.7326\n","Epoch 67: val_loss did not improve from 16.89087\n","Memory usage on epoch end: 5.50 GB\n","891/891 [==============================] - 477s 533ms/step - loss: 19.5788 - dense_3_loss: 17.7455 - tf.nn.log_softmax_loss: 21.4120 - dense_3_Lev: 0.7776 - tf.nn.log_softmax_Lev: 0.7326 - val_loss: 16.8961 - val_dense_3_loss: 16.3652 - val_tf.nn.log_softmax_loss: 17.4277 - val_dense_3_Lev: 0.7670 - val_tf.nn.log_softmax_Lev: 0.7464\n","Epoch 68/80\n","891/891 [==============================] - ETA: 0s - loss: 19.5351 - dense_3_loss: 17.7069 - tf.nn.log_softmax_loss: 21.3637 - dense_3_Lev: 0.7781 - tf.nn.log_softmax_Lev: 0.7333\n","Epoch 68: val_loss improved from 16.89087 to 16.88148, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.10 GB\n","891/891 [==============================] - 475s 531ms/step - loss: 19.5351 - dense_3_loss: 17.7069 - tf.nn.log_softmax_loss: 21.3637 - dense_3_Lev: 0.7781 - tf.nn.log_softmax_Lev: 0.7333 - val_loss: 16.8815 - val_dense_3_loss: 16.3572 - val_tf.nn.log_softmax_loss: 17.4058 - val_dense_3_Lev: 0.7680 - val_tf.nn.log_softmax_Lev: 0.7459\n","Epoch 69/80\n","891/891 [==============================] - ETA: 0s - loss: 19.4834 - dense_3_loss: 17.6447 - tf.nn.log_softmax_loss: 21.3223 - dense_3_Lev: 0.7787 - tf.nn.log_softmax_Lev: 0.7335\n","Epoch 69: val_loss improved from 16.88148 to 16.84909, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.68 GB\n","891/891 [==============================] - 480s 537ms/step - loss: 19.4834 - dense_3_loss: 17.6447 - tf.nn.log_softmax_loss: 21.3223 - dense_3_Lev: 0.7787 - tf.nn.log_softmax_Lev: 0.7335 - val_loss: 16.8491 - val_dense_3_loss: 16.3143 - val_tf.nn.log_softmax_loss: 17.3852 - val_dense_3_Lev: 0.7686 - val_tf.nn.log_softmax_Lev: 0.7472\n","Epoch 70/80\n","891/891 [==============================] - ETA: 0s - loss: 19.4378 - dense_3_loss: 17.6080 - tf.nn.log_softmax_loss: 21.2676 - dense_3_Lev: 0.7793 - tf.nn.log_softmax_Lev: 0.7341\n","Epoch 70: val_loss improved from 16.84909 to 16.83258, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.94 GB\n","891/891 [==============================] - 479s 535ms/step - loss: 19.4378 - dense_3_loss: 17.6080 - tf.nn.log_softmax_loss: 21.2676 - dense_3_Lev: 0.7793 - tf.nn.log_softmax_Lev: 0.7341 - val_loss: 16.8326 - val_dense_3_loss: 16.3107 - val_tf.nn.log_softmax_loss: 17.3530 - val_dense_3_Lev: 0.7687 - val_tf.nn.log_softmax_Lev: 0.7475\n","Epoch 71/80\n","891/891 [==============================] - ETA: 0s - loss: 19.3723 - dense_3_loss: 17.5307 - tf.nn.log_softmax_loss: 21.2143 - dense_3_Lev: 0.7803 - tf.nn.log_softmax_Lev: 0.7347\n","Epoch 71: val_loss did not improve from 16.83258\n","Memory usage on epoch end: 5.67 GB\n","891/891 [==============================] - 475s 531ms/step - loss: 19.3723 - dense_3_loss: 17.5307 - tf.nn.log_softmax_loss: 21.2143 - dense_3_Lev: 0.7803 - tf.nn.log_softmax_Lev: 0.7347 - val_loss: 16.8538 - val_dense_3_loss: 16.3528 - val_tf.nn.log_softmax_loss: 17.3531 - val_dense_3_Lev: 0.7687 - val_tf.nn.log_softmax_Lev: 0.7475\n","Epoch 72/80\n","891/891 [==============================] - ETA: 0s - loss: 19.3289 - dense_3_loss: 17.4985 - tf.nn.log_softmax_loss: 21.1589 - dense_3_Lev: 0.7805 - tf.nn.log_softmax_Lev: 0.7355\n","Epoch 72: val_loss did not improve from 16.83258\n","Memory usage on epoch end: 5.75 GB\n","891/891 [==============================] - 482s 539ms/step - loss: 19.3289 - dense_3_loss: 17.4985 - tf.nn.log_softmax_loss: 21.1589 - dense_3_Lev: 0.7805 - tf.nn.log_softmax_Lev: 0.7355 - val_loss: 16.8633 - val_dense_3_loss: 16.3654 - val_tf.nn.log_softmax_loss: 17.3596 - val_dense_3_Lev: 0.7691 - val_tf.nn.log_softmax_Lev: 0.7475\n","Epoch 73/80\n","891/891 [==============================] - ETA: 0s - loss: 19.3070 - dense_3_loss: 17.4607 - tf.nn.log_softmax_loss: 21.1527 - dense_3_Lev: 0.7808 - tf.nn.log_softmax_Lev: 0.7355\n","Epoch 73: val_loss improved from 16.83258 to 16.81256, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.25 GB\n","891/891 [==============================] - 485s 543ms/step - loss: 19.3070 - dense_3_loss: 17.4607 - tf.nn.log_softmax_loss: 21.1527 - dense_3_Lev: 0.7808 - tf.nn.log_softmax_Lev: 0.7355 - val_loss: 16.8126 - val_dense_3_loss: 16.2998 - val_tf.nn.log_softmax_loss: 17.3248 - val_dense_3_Lev: 0.7696 - val_tf.nn.log_softmax_Lev: 0.7487\n","Epoch 74/80\n","891/891 [==============================] - ETA: 0s - loss: 19.2979 - dense_3_loss: 17.4502 - tf.nn.log_softmax_loss: 21.1459 - dense_3_Lev: 0.7813 - tf.nn.log_softmax_Lev: 0.7358\n","Epoch 74: val_loss did not improve from 16.81256\n","Memory usage on epoch end: 5.48 GB\n","891/891 [==============================] - 484s 541ms/step - loss: 19.2979 - dense_3_loss: 17.4502 - tf.nn.log_softmax_loss: 21.1459 - dense_3_Lev: 0.7813 - tf.nn.log_softmax_Lev: 0.7358 - val_loss: 16.8424 - val_dense_3_loss: 16.3445 - val_tf.nn.log_softmax_loss: 17.3386 - val_dense_3_Lev: 0.7695 - val_tf.nn.log_softmax_Lev: 0.7479\n","Epoch 75/80\n","891/891 [==============================] - ETA: 0s - loss: 19.2224 - dense_3_loss: 17.3808 - tf.nn.log_softmax_loss: 21.0640 - dense_3_Lev: 0.7819 - tf.nn.log_softmax_Lev: 0.7365\n","Epoch 75: val_loss improved from 16.81256 to 16.79607, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.04 GB\n","891/891 [==============================] - 484s 541ms/step - loss: 19.2224 - dense_3_loss: 17.3808 - tf.nn.log_softmax_loss: 21.0640 - dense_3_Lev: 0.7819 - tf.nn.log_softmax_Lev: 0.7365 - val_loss: 16.7961 - val_dense_3_loss: 16.2871 - val_tf.nn.log_softmax_loss: 17.3052 - val_dense_3_Lev: 0.7696 - val_tf.nn.log_softmax_Lev: 0.7482\n","Epoch 76/80\n","891/891 [==============================] - ETA: 0s - loss: 19.2425 - dense_3_loss: 17.3982 - tf.nn.log_softmax_loss: 21.0871 - dense_3_Lev: 0.7816 - tf.nn.log_softmax_Lev: 0.7362\n","Epoch 76: val_loss improved from 16.79607 to 16.77779, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.24 GB\n","891/891 [==============================] - 484s 541ms/step - loss: 19.2425 - dense_3_loss: 17.3982 - tf.nn.log_softmax_loss: 21.0871 - dense_3_Lev: 0.7816 - tf.nn.log_softmax_Lev: 0.7362 - val_loss: 16.7778 - val_dense_3_loss: 16.2502 - val_tf.nn.log_softmax_loss: 17.3042 - val_dense_3_Lev: 0.7693 - val_tf.nn.log_softmax_Lev: 0.7482\n","Epoch 77/80\n","891/891 [==============================] - ETA: 0s - loss: 19.2047 - dense_3_loss: 17.3646 - tf.nn.log_softmax_loss: 21.0445 - dense_3_Lev: 0.7822 - tf.nn.log_softmax_Lev: 0.7367\n","Epoch 77: val_loss did not improve from 16.77779\n","Memory usage on epoch end: 5.51 GB\n","891/891 [==============================] - 482s 539ms/step - loss: 19.2047 - dense_3_loss: 17.3646 - tf.nn.log_softmax_loss: 21.0445 - dense_3_Lev: 0.7822 - tf.nn.log_softmax_Lev: 0.7367 - val_loss: 16.8586 - val_dense_3_loss: 16.3647 - val_tf.nn.log_softmax_loss: 17.3522 - val_dense_3_Lev: 0.7698 - val_tf.nn.log_softmax_Lev: 0.7484\n","Epoch 78/80\n","891/891 [==============================] - ETA: 0s - loss: 19.1937 - dense_3_loss: 17.3414 - tf.nn.log_softmax_loss: 21.0462 - dense_3_Lev: 0.7824 - tf.nn.log_softmax_Lev: 0.7369\n","Epoch 78: val_loss did not improve from 16.77779\n","Memory usage on epoch end: 5.28 GB\n","891/891 [==============================] - 479s 535ms/step - loss: 19.1937 - dense_3_loss: 17.3414 - tf.nn.log_softmax_loss: 21.0462 - dense_3_Lev: 0.7824 - tf.nn.log_softmax_Lev: 0.7369 - val_loss: 16.7880 - val_dense_3_loss: 16.2792 - val_tf.nn.log_softmax_loss: 17.2974 - val_dense_3_Lev: 0.7700 - val_tf.nn.log_softmax_Lev: 0.7481\n","Epoch 79/80\n","891/891 [==============================] - ETA: 0s - loss: 19.2186 - dense_3_loss: 17.3807 - tf.nn.log_softmax_loss: 21.0565 - dense_3_Lev: 0.7817 - tf.nn.log_softmax_Lev: 0.7367\n","Epoch 79: val_loss did not improve from 16.77779\n","Memory usage on epoch end: 5.66 GB\n","891/891 [==============================] - 481s 538ms/step - loss: 19.2186 - dense_3_loss: 17.3807 - tf.nn.log_softmax_loss: 21.0565 - dense_3_Lev: 0.7817 - tf.nn.log_softmax_Lev: 0.7367 - val_loss: 16.7897 - val_dense_3_loss: 16.2746 - val_tf.nn.log_softmax_loss: 17.3023 - val_dense_3_Lev: 0.7700 - val_tf.nn.log_softmax_Lev: 0.7491\n","Epoch 80/80\n","891/891 [==============================] - ETA: 0s - loss: 19.1295 - dense_3_loss: 17.2768 - tf.nn.log_softmax_loss: 20.9817 - dense_3_Lev: 0.7831 - tf.nn.log_softmax_Lev: 0.7377\n","Epoch 80: val_loss did not improve from 16.77779\n","Memory usage on epoch end: 5.51 GB\n","891/891 [==============================] - 480s 537ms/step - loss: 19.1295 - dense_3_loss: 17.2768 - tf.nn.log_softmax_loss: 20.9817 - dense_3_Lev: 0.7831 - tf.nn.log_softmax_Lev: 0.7377 - val_loss: 16.7919 - val_dense_3_loss: 16.2891 - val_tf.nn.log_softmax_loss: 17.2929 - val_dense_3_Lev: 0.7701 - val_tf.nn.log_softmax_Lev: 0.7487\n","     47/Unknown - 15s 318ms/step - loss: 16.7778 - dense_3_loss: 16.2502 - tf.nn.log_softmax_loss: 17.3042 - dense_3_Lev: 0.7693 - tf.nn.log_softmax_Lev: 0.7482"]},{"name":"stderr","output_type":"stream","text":["2023-07-19 20:53:15.605748: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8981036001774190784\n","2023-07-19 20:53:15.605943: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9286916121095794810\n","2023-07-19 20:53:15.605951: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4650622395172680226\n","2023-07-19 20:53:15.605955: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 1244231037246136320\n","2023-07-19 20:53:15.605958: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 6200868172222727184\n","2023-07-19 20:53:15.605963: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6074612256099051604\n","2023-07-19 20:53:15.605967: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2576685624732534958\n","2023-07-19 20:53:15.605970: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 7197138884580612840\n","2023-07-19 20:53:15.605974: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 16251098445298526400\n","2023-07-19 20:53:15.605978: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5219059941683793304\n","2023-07-19 20:53:15.605983: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 9534246946975120792\n","2023-07-19 20:53:15.611805: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4273887271130063115\n","2023-07-19 20:53:15.611821: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 445244564130325033\n","2023-07-19 20:53:15.611828: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7172322797806871439\n","2023-07-19 20:53:15.611832: I tensorflow/core/framework/local_rendezvous.cc:425] Local rendezvous send item cancelled. Key hash: 191340078222750473\n","2023-07-19 20:53:15.611837: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14457791103803209183\n","2023-07-19 20:53:15.611842: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3465288978825197219\n","2023-07-19 20:53:15.611846: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16907662211580556971\n","2023-07-19 20:53:15.611851: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9623511319840123901\n","2023-07-19 20:53:15.611859: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13831977509210365623\n","2023-07-19 20:53:15.611865: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16747720353396584357\n"]},{"name":"stdout","output_type":"stream","text":["47/47 [==============================] - 16s 340ms/step - loss: 16.7778 - dense_3_loss: 16.2502 - tf.nn.log_softmax_loss: 17.3042 - dense_3_Lev: 0.7693 - tf.nn.log_softmax_Lev: 0.7482\n"]}],"source":["train(use_supplemental=True)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.507582Z","iopub.status.busy":"2023-07-18T09:04:44.507205Z","iopub.status.idle":"2023-07-18T09:04:44.521779Z","shell.execute_reply":"2023-07-18T09:04:44.520195Z","shell.execute_reply.started":"2023-07-18T09:04:44.507536Z"},"trusted":true},"outputs":[],"source":["# Inference "]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.523717Z","iopub.status.busy":"2023-07-18T09:04:44.523335Z","iopub.status.idle":"2023-07-18T09:04:44.53874Z","shell.execute_reply":"2023-07-18T09:04:44.536985Z","shell.execute_reply.started":"2023-07-18T09:04:44.523687Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.541198Z","iopub.status.busy":"2023-07-18T09:04:44.540385Z","iopub.status.idle":"2023-07-18T09:04:44.556953Z","shell.execute_reply":"2023-07-18T09:04:44.555702Z","shell.execute_reply.started":"2023-07-18T09:04:44.541158Z"},"trusted":true},"outputs":[],"source":["class InferModel(tf.Module):\n","    def __init__(self, model,config=CFG):\n","        super().__init__()\n","\n","        self.model = model\n","        self.max_len=config.max_len\n","\n","    @tf.function(\n","        input_signature=[tf.TensorSpec(shape=(None,Constants.NUM_INPUT_FEATURES), dtype=tf.float32, name=\"inputs\")]\n","    )\n","    def __call__(self, inputs):\n","        \"\"\"\n","        Applies the feature generation model and main model to the input tensor.\n","\n","        Args:\n","            inputs: Input tensor with shape (T, F).\n","\n","        Returns:\n","            A dictionary with a single key 'outputs' and corresponding output tensor.\n","        \"\"\"\n","        x=tf.cast(inputs,tf.float32)\n","        x = x[None] # trick to deal with empty frames\n","        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, Constants.NUM_INPUT_FEATURES)), lambda: tf.identity(x))\n","        x = x[0]\n","        x = preprocess(x,max_len=self.max_len)\n","      \n","        x = self.model(x[None],training=False)[0]\n","                    \n","        x=decode_phrase(x)        \n","        x = tf.cond(tf.shape(x)[0] == 0, lambda: tf.zeros(1, tf.int64), lambda: tf.identity(x))                   \n","                    \n","        outputs=tf.one_hot(x,depth=59,dtype=tf.float32)\n","        return {\"outputs\": outputs}\n"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.558943Z","iopub.status.busy":"2023-07-18T09:04:44.558502Z","iopub.status.idle":"2023-07-18T09:04:48.116622Z","shell.execute_reply":"2023-07-18T09:04:48.11492Z","shell.execute_reply.started":"2023-07-18T09:04:44.55891Z"},"trusted":true},"outputs":[{"ename":"ValueError","evalue":"Layer count mismatch when loading weights from file. Model expected 68 layers, found 64 saved layers.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m saved_based_model \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/kaggle/input/weights/\u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39mcomment\u001b[39m}\u001b[39;00m\u001b[39m-exp\u001b[39m\u001b[39m{\u001b[39;00mexperiment_id\u001b[39m}\u001b[39;00m\u001b[39m-best.h5\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[39m#saved_based_model = f\"/kaggle/input/weights-from-12h-run/{config.comment}-exp{experiment_id}-best.h5\"\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m model\u001b[39m.\u001b[39;49mload_weights(saved_based_model)\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel with weights \u001b[39m\u001b[39m{\u001b[39;00msaved_based_model\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/code/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m~/code/.venv/lib/python3.10/site-packages/keras/src/saving/legacy/hdf5_format.py:819\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, model)\u001b[0m\n\u001b[1;32m    817\u001b[0m layer_names \u001b[39m=\u001b[39m filtered_layer_names\n\u001b[1;32m    818\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(layer_names) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(filtered_layers):\n\u001b[0;32m--> 819\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    820\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLayer count mismatch when loading weights from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    821\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel expected \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(filtered_layers)\u001b[39m}\u001b[39;00m\u001b[39m layers, found \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    822\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(layer_names)\u001b[39m}\u001b[39;00m\u001b[39m saved layers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    823\u001b[0m     )\n\u001b[1;32m    825\u001b[0m \u001b[39m# We batch weight value assignments in a single backend call\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[39m# which provides a speedup in TensorFlow.\u001b[39;00m\n\u001b[1;32m    827\u001b[0m weight_value_tuples \u001b[39m=\u001b[39m []\n","\u001b[0;31mValueError\u001b[0m: Layer count mismatch when loading weights from file. Model expected 68 layers, found 64 saved layers."]}],"source":["\n","config=CFG\n","\n","model = get_model(\n","    max_len=config.max_len,\n","    output_dim=config.output_dim,\n","    dim=config.dim,\n","    input_pad=Constants.INPUT_PAD,\n",")\n","experiment_id=0\n","\n","saved_based_model = f\"/kaggle/input/weights/{config.comment}-exp{experiment_id}-best.h5\"\n","#saved_based_model = f\"/kaggle/input/weights-from-12h-run/{config.comment}-exp{experiment_id}-best.h5\"\n","model.load_weights(saved_based_model)\n","print(f\"model with weights {saved_based_model}\")"]},{"cell_type":"markdown","metadata":{},"source":["#Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:48.118699Z","iopub.status.busy":"2023-07-18T09:04:48.1183Z","iopub.status.idle":"2023-07-18T09:04:55.78829Z","shell.execute_reply":"2023-07-18T09:04:55.784098Z","shell.execute_reply.started":"2023-07-18T09:04:48.118667Z"},"trusted":true},"outputs":[],"source":["# Sanity Check\n","import json\n","with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n","    character_map = json.load(f)\n","rev_character_map = {j:i for i,j in character_map.items()}\n","\n","infer_keras_model=InferModel(model)\n","\n","main_dir = '/kaggle/input/asl-fingerspelling/'\n","path = f'{main_dir}train_landmarks/5414471.parquet'\n","cols=selected_columns(path)\n","df = pd.read_parquet(path, engine = 'auto', columns = cols)\n","seq_id=1816796431\n","seq=df.loc[seq_id]\n","data = seq[cols].to_numpy()\n","print(f'input shape: {data.shape}, dtype: {data.dtype}')\n","output = infer_keras_model(data)[\"outputs\"]\n","prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output, axis=1)])\n","\n","print(prediction_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:55.79275Z","iopub.status.busy":"2023-07-18T09:04:55.792139Z","iopub.status.idle":"2023-07-18T09:05:47.089729Z","shell.execute_reply":"2023-07-18T09:05:47.088743Z","shell.execute_reply.started":"2023-07-18T09:04:55.792699Z"},"trusted":true},"outputs":[],"source":["SAVED_MODEL_PATH=\"/kaggle/working/infer_model\"\n","\n","tf.saved_model.save(infer_keras_model,SAVED_MODEL_PATH)\n","keras_model_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_PATH)\n","keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","#keras_model_converter.target_spec.supported_types = [tf.float16]\n","#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","#converter.allow_custom_ops=True\n","tflite_model = keras_model_converter.convert()\n","TFLITE_FILE_PATH=\"/kaggle/working/model.tflite\"\n","with open(TFLITE_FILE_PATH, \"wb\") as f:\n","    f.write(tflite_model)\n","\n","with open('/kaggle/working/inference_args.json', 'w') as f:\n","     json.dump({ 'selected_columns': cols }, f)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:05:47.092039Z","iopub.status.busy":"2023-07-18T09:05:47.091681Z","iopub.status.idle":"2023-07-18T09:05:47.257675Z","shell.execute_reply":"2023-07-18T09:05:47.255994Z","shell.execute_reply.started":"2023-07-18T09:05:47.092009Z"},"trusted":true},"outputs":[],"source":["interpreter = tf.lite.Interpreter(TFLITE_FILE_PATH)\n","REQUIRED_SIGNATURE = \"serving_default\"\n","REQUIRED_OUTPUT = \"outputs\"\n","found_signatures = list(interpreter.get_signature_list().keys())\n","if REQUIRED_SIGNATURE not in found_signatures:\n","    print(\"Required input signature not found.\")\n","\n","prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n","output = prediction_fn(inputs=data)\n","prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n","print(prediction_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:05:47.26006Z","iopub.status.busy":"2023-07-18T09:05:47.25933Z","iopub.status.idle":"2023-07-18T09:05:48.830171Z","shell.execute_reply":"2023-07-18T09:05:48.828963Z","shell.execute_reply.started":"2023-07-18T09:05:47.260006Z"},"trusted":true},"outputs":[],"source":["!zip submission.zip \"/kaggle/working/model.tflite\" \"/kaggle/working/inference_args.json\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:05:48.832628Z","iopub.status.busy":"2023-07-18T09:05:48.832209Z","iopub.status.idle":"2023-07-18T09:05:48.840337Z","shell.execute_reply":"2023-07-18T09:05:48.838608Z","shell.execute_reply.started":"2023-07-18T09:05:48.832593Z"},"trusted":true},"outputs":[],"source":["#!pip install /kaggle/input/tflite-wheels-2140/tflite_runtime_nightly-2.14.0.dev20230508-cp310-cp310-manylinux2014_x86_64.whl"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:05:48.84292Z","iopub.status.busy":"2023-07-18T09:05:48.842282Z","iopub.status.idle":"2023-07-18T09:05:48.857104Z","shell.execute_reply":"2023-07-18T09:05:48.856148Z","shell.execute_reply.started":"2023-07-18T09:05:48.842886Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","import json\n","import pandas as pd\n","import tflite_runtime.interpreter as tflite\n","import numpy as np\n","import time\n","from tqdm import tqdm\n","import Levenshtein as Lev\n","import glob\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:05:48.859108Z","iopub.status.busy":"2023-07-18T09:05:48.858802Z","iopub.status.idle":"2023-07-18T09:05:48.878044Z","shell.execute_reply":"2023-07-18T09:05:48.876595Z","shell.execute_reply.started":"2023-07-18T09:05:48.859082Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","SEL_FEATURES = json.load(open('/kaggle/working/inference_args.json'))['selected_columns']\n","\n","def load_relevant_data_subset(pq_path):\n","        return pd.read_parquet(pq_path, columns=SEL_FEATURES) #selected_columns)\n","\n","with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n","    character_map = json.load(f)\n","rev_character_map = {j:i for i,j in character_map.items()}\n","\n","\n","df = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\n","\n","idx = 0\n","sample = df.loc[idx]\n","loaded = load_relevant_data_subset('/kaggle/input/asl-fingerspelling/' + sample['path'])\n","loaded = loaded[loaded.index==sample['sequence_id']].values\n","print(loaded.shape)\n","frames = loaded\n","\n","def wer__(s1, s2):\n","    w1 = len(s1.split())\n","    lvd = Lev.distance(s1, s2)\n","    return lvd / w1\n","\n","interpreter = tflite.Interpreter('model.tflite')\n","found_signatures = list(interpreter.get_signature_list().keys())\n","\n","REQUIRED_SIGNATURE = 'serving_default'\n","REQUIRED_OUTPUT = 'outputs'\n","if REQUIRED_SIGNATURE not in found_signatures:\n","    raise KernelEvalException('Required input signature not found.')\n","\n","prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n","output_lite = prediction_fn(inputs=frames)\n","prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output_lite[REQUIRED_OUTPUT], axis=1)])\n","print(prediction_str)\n","\n","\n","st = time.time()\n","count=0\n","model_time = 0\n","\n","levs = []\n","\n","files=glob.glob('/kaggle/input/asl-fingerspelling/train_landmarks/*.parquet')\n","for f in files:\n","    df = load_relevant_data_subset(f)\n","    seq=df.index.drop_duplicates()\n","    for ind in tqdm(seq):\n","        loaded = df.loc[ind].values\n","        count+=1\n","        md_st = time.time()\n","        output_ = prediction_fn(inputs=loaded)\n","        out= output_[REQUIRED_OUTPUT]\n","        assert out.ndim==2\n","        assert out.shape[1]==59\n","        assert out.dtype==np.float32\n","        assert np.all(np.isfinite(out))\n","        \n","        prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output_[REQUIRED_OUTPUT], axis=1)])\n","        model_time += time.time() - md_st\n","    \n","        #cur_lev = wer__(sample['phrase'], prediction_str) \n","        #print(sample['phrase'], '|', prediction_str, '|', cur_lev)\n","        #print()\n","\n","        #levs.append(cur_lev)\n","\n","#print(f'WER: {np.mean(levs):.5f}')\n","print(f'Mean time: {(time.time() - st)/count:.2f}')\n","print(f'Mean time only infer: {model_time/count:.2f}')\n","\n","out=prediction_fn(inputs=np.empty(0,dtype=np.float32))[\"outputs\"]\n","print(out.shape,output_.dtype)\n","\"\"\" "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":4}
