{"cells":[{"cell_type":"markdown","metadata":{"id":"nHNr0zBdcj-Z"},"source":["# Pips"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":82150,"status":"ok","timestamp":1690422334858,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"M4lTWchocj-b","outputId":"591bb5d2-800a-48b4-e531-c4f004d4f9ae","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n","Collecting pip\n","  Downloading pip-23.2.1-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 23.1.2\n","    Uninstalling pip-23.1.2:\n","      Successfully uninstalled pip-23.1.2\n","Successfully installed pip-23.2.1\n","Collecting cached_property\n","  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n","Installing collected packages: cached_property\n","Successfully installed cached_property-1.5.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting tensorflow-addons\n","  Obtaining dependency information for tensorflow-addons from https://files.pythonhosted.org/packages/12/34/221b939127e629c1ebbfb8a0281f67b084515ed53e5042582238817bcc54/tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n","  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n","Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: typeguard, tensorflow-addons\n","Successfully installed tensorflow-addons-0.21.0 typeguard-2.13.3\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting Levenshtein\n","  Obtaining dependency information for Levenshtein from https://files.pythonhosted.org/packages/e6/02/0a4ed6a9e2b78f6b57f25a87fc194d7d10c2bbe95d985f36390e86285232/Levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n","  Downloading Levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n","Collecting rapidfuzz<4.0.0,>=2.3.0 (from Levenshtein)\n","  Obtaining dependency information for rapidfuzz<4.0.0,>=2.3.0 from https://files.pythonhosted.org/packages/c7/3f/074006ee7946bef71aba8a1226a637ed29b0617690ea379787834ccc418c/rapidfuzz-3.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n","  Downloading rapidfuzz-3.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Downloading Levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (172 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rapidfuzz-3.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n","Successfully installed Levenshtein-0.21.1 rapidfuzz-3.1.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0minstall done\n"]}],"source":["!pip install -U pip\n","#!pip install tensorflow==2.13.0\n","!pip install cached_property\n","!pip install -U tensorflow-addons\n","!pip install Levenshtein\n","print(\"install done\")"]},{"cell_type":"markdown","metadata":{"id":"ENYLFLzurs3S"},"source":["# For Colab"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59919,"status":"ok","timestamp":1690422394773,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"agm4BN62kwwE","outputId":"b1ebdf13-06a2-456d-cde4-7d5dc6b229bc","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Colab: True\n","Mounted at /content/drive\n"]}],"source":["import os\n","IN_COLAB = 'COLAB_GPU' in os.environ\n","print(\"Colab:\",IN_COLAB)\n","if IN_COLAB:\n","  from google.colab import auth,drive\n","  auth.authenticate_user()\n","  drive.mount('/content/drive')\n","  !mkdir -p /kaggle/working"]},{"cell_type":"markdown","metadata":{"id":"trRlKZV_cj-e"},"source":["# Imports"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9264,"status":"ok","timestamp":1690422404028,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"eoljCzzvYi1z","outputId":"4dfef903-4158-4f72-9ba5-ebfabe849e4d","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["tensorflow version 2.12.0\n"]}],"source":["\n","import os\n","import numpy as np\n","import pandas as pd\n","\n","import random\n","import psutil\n","import gc\n","import math\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","\n","gpus = tf.config.list_physical_devices(\"GPU\")\n","for gpu in gpus:\n","    tf.config.experimental.set_memory_growth(gpu, True)\n","\n","print(\"tensorflow version\",tf.__version__)\n"]},{"cell_type":"markdown","metadata":{"id":"OJoFPLB2cj-e"},"source":["# Strategy"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8414,"status":"ok","timestamp":1690422412433,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"Asq1MGhs41ZX","outputId":"199a8360-8121-4c11-8e87-87ac37e6aba5","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on Colab TPU  ['10.86.70.18:8470']\n","<tensorflow.python.distribute.tpu_strategy.TPUStrategyV2 object at 0x7b3c46b71660>\n","get strategy replicas: 8\n"]}],"source":["\n","IS_TPU=False\n","STRATEGY=None\n","if IN_COLAB:\n","    try:\n","      tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","      tf.config.experimental_connect_to_cluster(tpu)\n","      tf.tpu.experimental.initialize_tpu_system(tpu)\n","      STRATEGY = tf.distribute.TPUStrategy(tpu)\n","      print('Running on Colab TPU ', tpu.cluster_spec().as_dict()['worker'])\n","      IS_TPU=True\n","\n","    except ValueError:\n","        print(\"Not using Colab TPU\")\n","\n","else: #check KAGGLE\n","    try:\n","        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=\"local\") # \"local\" for 1VM TPU\n","        STRATEGY = tf.distribute.TPUStrategy(tpu)\n","        print(\"on Kaggle TPU\")\n","        IS_TPU=True\n","    except:\n","        print(\"Not using Kaggle TPU\")\n","\n","\n","if not IS_TPU:# check GPU\n","\n","    logical_devices = tf.config.list_logical_devices()\n","    # Check if GPU is available\n","    gpu_available = any(\"GPU\" in device.name for device in logical_devices)\n","    if gpu_available:\n","        ngpu = len(gpus)\n","        print(\"Num GPUs Available: \", ngpu)\n","        if ngpu > 1:\n","            STRATEGY = tf.distribute.MirroredStrategy()\n","        else:\n","            STRATEGY = tf.distribute.get_strategy()\n","\n","    else:\n","        print(\"Runing on CPU\")\n","        STRATEGY = tf.distribute.get_strategy()\n","\n","REPLICAS = STRATEGY.num_replicas_in_sync\n","\n","print(STRATEGY)\n","print(f\"get strategy replicas: {REPLICAS}\")\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3wCTyTzWcj-f"},"source":["# CTC Loss"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":246,"status":"ok","timestamp":1690422412639,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"570cYNljcj-f","trusted":true},"outputs":[],"source":["from __future__ import annotations\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Tue Jul 18 20:29:39 2023\n","\"\"\"\n","\n","# Copyright 2021 Alexey Tochin\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# ==============================================================================\n","\n","from typing import Union, Callable, List, Optional, Type\n","import tensorflow as tf\n","import numpy as np\n","\n","from abc import ABC, abstractmethod\n","from cached_property import cached_property\n","\n","\n","inf = tf.constant(np.inf)\n","\n","\n","def logit_to_logproba(logit: tf.Tensor, axis: int) -> tf.Tensor:\n","    \"\"\"Converts logits to logarithmic probabilities:\n","        logit_to_logproba(x) = x - log (sum along axis (exp(x))\n","\n","    Args:\n","        logit:  tf.Tensor, dtype = tf.float32\n","        axis: integer, like for tf.reduce_logsumexp\n","\n","    Returns:    tf.Tensor, of the same shape and size as input logit\n","    \"\"\"\n","    log_probas = logit - tf.reduce_logsumexp(input_tensor=logit, axis=axis, keepdims=True)\n","    return log_probas\n","\n","\n","def apply_logarithmic_mask(tensor: tf.Tensor, mask: tf.Tensor) -> tf.Tensor:\n","    \"\"\"Masks a logarithmic representation of a tensor, namely\n","    1. Keeps the value of tensor unchanged for True values of mask\n","    2. Replace the value of tensor by -tf.inf for False values of mask\n","\n","    Args:\n","        tensor: tf.Tensor, dtype = tf.float32 of the same shape as mask or broadcastable\n","        mask:   tf.Tensor, dbool = tf.float32 of the same shape as tensor or broadcastable\n","\n","    Returns:    tf.Tensor, dtype = tf.float32 of the same shape as tensor\n","    \"\"\"\n","    return tensor + tf.math.log(tf.cast(mask, dtype=tf.float32))\n","\n","\n","def logsumexp(x: tf.Tensor, y: tf.Tensor) -> tf.Tensor:\n","    \"\"\"A numerically stable version of elementwise function\n","        logsumexp(x, y) = log (e ** x + e ** y)\n","\n","    Args:\n","        x:      tf.Tensor of the shape and size as y or broadcastable\n","        y:      tf.Tensor of the shape and size as x or broadcastable\n","\n","    Returns:    tf.Tensor of the shape and size as x and y\n","    \"\"\"\n","    return tf.where(\n","        condition=x < y,\n","        x=y + tf.math.softplus(x - y),\n","        y=tf.where(\n","            condition=x > y,\n","            x=x + tf.math.softplus(y - x),\n","            y=x + np.log(2.)\n","        ),\n","    )\n","\n","\n","def subexp(x: tf.Tensor, y: tf.Tensor) -> tf.Tensor:\n","    \"\"\"A numerically stable version of elementwise function\n","        subexp(x,y) := exp x - exp y\n","\n","    Args:\n","        x:      tf.Tensor, shape broadcastable to y\n","        y:      tf.Tensor, shape broadcastable to x\n","\n","    Returns:    tf.Tensor, shape, the same as x and y\n","    \"\"\"\n","    return tf.where(\n","        condition=x > y,\n","        x=-tf.exp(x) * tf.math.expm1(y - x),\n","        y=tf.where(\n","            condition=x < y,\n","            x=tf.exp(y) * tf.math.expm1(x - y),\n","            y=tf.zeros_like(x),\n","        ),\n","    )\n","\n","\n","def unsorted_segment_logsumexp(data: tf.Tensor, segment_ids: tf.Tensor, num_segments: Union[int, tf.Tensor])\\\n","        -> tf.Tensor:\n","    \"\"\"Computes the logarithmic sum of exponents along segments of a tensor\n","    like other operators from tf.math.unsorted_segment_* family.\n","\n","    Args:\n","        data:           tf.Tensor,  shape = [...] + data_dims,\n","        segment_ids:    tf.Tensor,  shape = [...], dtype = tf.int32\n","        num_segments:   tf.Tensor,  shape = [], dtype = tf.int32\n","\n","    Returns:            tf.Tensor,  shape = [num_segments] + data_dims, for the same type as data\n","    \"\"\"\n","    data_max = tf.math.unsorted_segment_max(data=data, segment_ids=segment_ids, num_segments=num_segments)\n","    data_normed = data - tf.gather(params=data_max, indices=segment_ids)\n","    output = data_max + tf.math.log(tf.math.unsorted_segment_sum(\n","        data=tf.exp(data_normed),\n","        segment_ids=segment_ids,\n","        num_segments=num_segments,\n","    ))\n","    return output\n","\n","\n","def pad_until(\n","        tensor: tf.Tensor,\n","        desired_size: Union[tf.Tensor, int],\n","        axis: int,\n","        pad_value: Union[tf.Tensor, int, float, bool] = 0\n",") -> tf.Tensor:\n","    \"\"\"Pads tensor until desired dimension from right,\n","\n","    Args:\n","        tensor:         tf.Tensor, of any shape and type\n","        desired_size:   tf.Tensor or pythonic static integer\n","        axis:           pythonic static integer for pad axes\n","        pad_value:      tf.Tensor or pythonic numerical for padding\n","\n","    Returns:            tf.Tensor, the same shape as tensor except axis that equals to desired_size.\n","    \"\"\"\n","    rank = len(tensor.shape)\n","    if axis >= rank:\n","        raise ValueError()\n","\n","    current_size = tf.shape(tensor)[axis]\n","    paddings = [[0, 0]] * axis + [[0, desired_size - current_size]] + [[0, 0]] * (rank - axis - 1)\n","    return tf.pad(tensor=tensor, paddings=paddings, constant_values=pad_value)\n","\n","\n","def insert_zeros(tensor: tf.Tensor, mask: tf.Tensor) -> tf.Tensor:\n","    \"\"\"Inserts zeros into tensor before each masked element.\n","    For example:\n","    ```python\n","        output = insert_zeros(\n","            tensor =  tf.constant([[1, 2, 3, 4, 5], [10, 20, 30, 40, 50]], dtype = tf.int32),\n","            mask = tf.constant([[False, True, False, False, True], [False, True,  True, True,  False]]),\n","        )\n","        # -> [[1, 0, 2, 3, 4, 0, 5, 0], [10, 0, 20, 0, 30, 0, 40, 50]]\n","        # We insert 0s 2, 5, 20, 30, and 40 because their positions in input tensor corresponds to True value\n","        in mask.\n","    ```\n","\n","    Args:\n","        tensor: tf.Tensor, shape = [batch, length], any type and the same shape as mask\n","        mask:   tf.Tensor, shape = [batch, length], dtype = tf.bool and the same shape as tensor\n","\n","    Returns:    tf.Tensor, shape = [batch, length + max_num_insertions],\n","                where max_num_insertions is the maximal number of True values along the 0 batch dimension of mask.\n","                dtype = same as input tensor\n","    \"\"\"\n","    batch_size = tf.shape(tensor)[0]\n","    length = tf.shape(mask)[1]\n","\n","    delta = tf.cumsum(tf.cast(mask, dtype=tf.int32), exclusive=False, axis=1)\n","    max_num_insertions = tf.reduce_max(delta[:, -1])\n","\n","    y, x = tf.meshgrid(tf.range(length), tf.range(batch_size))\n","    y = y + delta\n","    indices = tf.reshape(tf.stack([x, y], 2), [-1, 2])\n","\n","    output = tf.scatter_nd(\n","        indices=indices,\n","        updates=tf.reshape(tensor, shape=[-1]),\n","        shape=tf.stack([batch_size, length + max_num_insertions])\n","    )\n","\n","    return output\n","\n","\n","def unfold(\n","        init_tensor: tf.Tensor,\n","        iterfunc: Callable[[tf.Tensor, tf.Tensor], tf.Tensor],\n","        num_iters: Union[int, tf.Tensor],\n","        d_i: int,\n","        element_shape: tf.TensorShape,\n","        swap_memory: bool = False,\n","        name: str = \"unfold\",\n",") -> tf.Tensor:\n","    \"\"\"Calculates a tensor by iterations over i that is the concatenation\n","        for d_i = +1:\n","            init_tensor\n","            iterfunc(init_tensor, 0)\n","            iterfunc(iterfunc(init_tensor, 0), 1)\n","            ...\n","            ..., num_iters - 1)\n","            ..., num_iters - 1), num_iters)\n","        for d_i = -1:\n","            ..., 2), 1), 0)\n","            ..., 2), 1)\n","            ...\n","            iterfunc(iterfunc(init_tensor, num_iters - 1), num_iters - 2)\n","            iterfunc(init_tensor, num_iters - 1)\n","            init_tensor\n","    For example:\n","    ```python\n","        unfold(\n","            init_tensor=tf.constant(0),\n","            iterfunc=lambda x, i: x + i,\n","            num_iters=5,\n","            d_i=1,\n","            element_shape=tf.TensorShape([]),\n","        )\n","        # -> [0, 0, 1, 3, 6, 10]\n","    ```\n","\n","    Args:\n","        init_tensor:    tf.Tensor, of any shape that is the initial value of the iterations.\n","        iterfunc:       tf.Tensor, tf.Tensor -> tf.Tensor, that is the iteration function\n","                            from and onto the same shape as init_tensor\n","        num_iters:      tf.Tensor or static integer that is the number of iterations\n","        d_i:            either +1 or -1, where\n","                            +1 corresponds for the iterations from 0 to num_iters inclusive\n","                            -1 corresponds for the iterations from num_iters to 0 inclusive\n","        element_shape:  tf.TensorShape([]) that is the shape of init_tensor\n","        swap_memory:    the same as for tf.while_loop, argument\n","        name:           str, local tensor names scope\n","\n","    Returns:            tf.Tensor, shape = [num_iters + 1] + init_tensor.shape\n","                        dtype the same as init_tensor\n","    \"\"\"\n","    assert d_i in {-1, 1}\n","    positive_direction = d_i == 1\n","\n","    with tf.name_scope(name):\n","        num_iters = tf.convert_to_tensor(num_iters)\n","\n","        tensor_array = tf.TensorArray(\n","            dtype=init_tensor.dtype,\n","            size=num_iters + 1,\n","            element_shape=element_shape,\n","            clear_after_read=False,\n","            infer_shape=True,\n","            dynamic_size=False,\n","        )\n","        tensor_array = tensor_array.write(0 if positive_direction else num_iters, init_tensor)\n","\n","        def body(i, tensor_slice):\n","            last_value = tensor_slice.read(i if positive_direction else i + 1)\n","            new_value = iterfunc(last_value, i)\n","            tensor_slice = tensor_slice.write(i + 1 if positive_direction else i, new_value)\n","            return i + d_i, tensor_slice\n","\n","        n = tf.constant(0, dtype=tf.int32) if positive_direction else num_iters - 1\n","        _, array_out = tf.while_loop(\n","            cond=lambda i, _: tf.constant(True),\n","            body=body,\n","            loop_vars=(n, tensor_array),\n","            maximum_iterations=num_iters,\n","            swap_memory=swap_memory,\n","            name=f\"unfold_while_loop\",\n","        )\n","        return array_out.stack()\n","\n","\n","def reduce_max_with_default(input_tensor: tf.Tensor, default: tf.Tensor) -> tf.Tensor:\n","    \"\"\"A version of tf.reduce_max function that supports default values for zero size input.\n","    Support axis=None case only that corresponds to scalar output\n","\n","    Args:\n","        input_tensor:   tf.Tensor, of any shape and numerical type\n","        default:        tf.Tensor, shape = [], dtype the same as input_tensor\n","\n","    Returns:            tf.Tensor, shape = [], dtype the same as input_tensor\n","    \"\"\"\n","    total_size = tf.shape(tf.reshape(input_tensor, [-1]))[0]\n","    return tf.where(\n","        condition=total_size > 0,\n","        x=tf.reduce_max(input_tensor),\n","        y=default\n","    )\n","\n","\n","def expand_many_dims(input: tf.Tensor, axes: List[int]) -> tf.Tensor:\n","    \"\"\"Analogous of tf.expand_dims for multiple new dimensions.\n","    Like for tf.expand_dims no new memory allocated for the output tensor.\n","\n","    For example:\n","        expand_many_dims(tf.zeros(shape=[5, 1, 3]), axes=[0, 4, 5]).shape\n","        # -> [1, 5, 1, 3, 1, 1]\n","\n","    Args:\n","        input:  tf.Tensor of any rank shape and type\n","        axes:   list of integer that are supposed to be the indexes of new dimensions.\n","\n","    Returns:    tf.Tensor of the same type an input and rank = rank(input) + len(axes)\n","    \"\"\"\n","    tensor = input\n","    for axis in axes:\n","        tensor = tf.expand_dims(input=tensor, axis=axis)\n","\n","    return tensor\n","\n","\n","def smart_transpose(a: tf.Tensor, perm=List[int]) -> tf.Tensor:\n","    \"\"\"Extension of tf.transpose.\n","    Parameter perm may be shorter list than rank on input tensor a.\n","    This case all dimensions that are beyond the list perm remain unchanged.\n","\n","    For example:\n","        smart_transpose(tf.zeros(shape=[2, 3, 4, 5, 6]), [2, 1, 0]).shape\n","        # -> [4, 3, 2, 5, 6]\n","\n","    Args:\n","        a:      tf.Tensor of any rank shape and type\n","        perm:   list of integers like for tf.transpose but in may be shorter than the shape of a.\n","\n","    Returns:    tf.Tensor of the same type and rank as th input tensor a.\n","    \"\"\"\n","    if len(perm) > len(a.shape):\n","        raise ValueError(f\"Tensor with shape '{a.shape}' cannot be reshaped to '{perm}'\")\n","    else:\n","        perm_rest = list(range(len(perm), len(a.shape)))\n","\n","    return tf.transpose(a=a, perm=perm + perm_rest)\n","\n","\n","def smart_reshape(tensor: tf.Tensor, shape: List[Optional[Union[int, tf.Tensor]]]) -> tf.Tensor:\n","    \"\"\"A version of tf.reshape.\n","    1. The ouput tensor is always of the same rank as input tensor.\n","    2. The parameter shape is supposed to be a list that is smaller or equal\n","    than the tensor shape.\n","    3. The list shape may contain None, that means \"keep this dimension unchanged\".\n","    4. The list shape is appended with None value to be of the same length as the input tensor shape.\n","    5. Like for tf.reshape output tensor does not requre new memory for allocation.\n","\n","    For example:\n","    ```python\n","        smart_reshape(\n","            tensor=tf.zeros(shape=[2, 3, 4, 5]),\n","            shape=[8, None, 1]\n","        )\n","        # -> tf.Tensor([8, 3, 1, 5])\n","    ```\n","\n","    Args:\n","        tensor: tf.Tensor of any shape and type\n","        shape:  list of optional static of dynamic integrates\n","\n","    Returns:    tf.Tensor of the same typey and rank as the input tensor\n","    \"\"\"\n","    if len(shape) > len(tensor.shape):\n","        raise ValueError(f\"Tensor with shape {tensor.shape} cannot be reshaped to {shape}.\")\n","    else:\n","        shape = shape + [None] * (len(tensor.shape) - len(shape))\n","\n","    original_shape = tf.shape(tensor)\n","    new_shape = []\n","    for index, dim in enumerate(shape):\n","        if dim is None:\n","            new_shape.append(original_shape[index])\n","        else:\n","            new_shape.append(dim)\n","\n","    return tf.reshape(tensor=tensor, shape=new_shape)\n","\n","\n","\n","def ctc_loss(\n","        labels: tf.Tensor,\n","        logits: tf.Tensor,\n","        label_length: tf.Tensor,\n","        logit_length: tf.Tensor,\n","        blank_index: Union[int, tf.Tensor],\n","        ctc_loss_data_cls: Type[BaseCtcLossData],\n",") -> tf.Tensor:\n","    \"\"\"Computes a version of CTC loss from\n","    http://www.cs.toronto.edu/~graves/icml_2006.pdf.\n","\n","    Args:\n","        labels:             tf.Tensor, shape = [batch, max_label_length],       dtype = tf.int32\n","        logits:             tf.Tensor, shape = [batch, max_length, mum_tokens], dtype = tf.float32\n","        label_length:       tf.Tensor, shape = [batch],                         dtype = tf.int32\n","        logit_length:       tf.Tensor, shape = [batch],                         dtype = tf.int32\n","        blank_index:        static integer >= 0\n","        ctc_loss_data_cls:  BaseCtcLossData class\n","\n","    Returns:                tf.Tensor, shape = [batch, max_length, mum_tokens], dtype = tf.float32\n","    \"\"\"\n","    log_probas = logit_to_logproba(logit=logits, axis=2)\n","    loss = ctc_loss_from_logproba(\n","        labels=labels,\n","        logprobas=log_probas,\n","        label_length=label_length,\n","        logit_length=logit_length,\n","        blank_index=blank_index,\n","        ctc_loss_data_cls=ctc_loss_data_cls,\n","    )\n","    return loss\n","\n","\n","def ctc_loss_from_logproba(\n","        labels: tf.Tensor,\n","        logprobas: tf.Tensor,\n","        label_length: tf.Tensor,\n","        logit_length: tf.Tensor,\n","        blank_index: Union[int, tf.Tensor],\n","        ctc_loss_data_cls: Type[BaseCtcLossData],\n",") -> tf.Tensor:\n","    \"\"\"Computes a version of CTC loss from logarothmic probabilities considered as independent parameters.\n","\n","    Args:\n","        labels:             tf.Tensor, shape = [batch, max_label_length],       dtype = tf.int32\n","        logprobas:          tf.Tensor, shape = [batch, max_length, mum_tokens], dtype = tf.float32\n","        label_length:       tf.Tensor, shape = [batch],                         dtype = tf.int32\n","        logit_length:       tf.Tensor, shape = [batch],                         dtype = tf.int32\n","        blank_index:        static integer >= 0\n","        ctc_loss_data_cls:  BaseCtcLossData class\n","\n","    Returns:                tf.Tensor, shape = [batch, max_length, mum_tokens], dtype = tf.float32\n","    \"\"\"\n","    loss_data = ctc_loss_data_cls(\n","        labels=labels,\n","        logprobas=tf.stop_gradient(logprobas),\n","        label_length=label_length,\n","        logit_length=logit_length,\n","        blank_index=blank_index,\n","    )\n","\n","    return loss_data.forward_fn(logprobas)\n","\n","\n","class BaseCtcLossData(ABC):\n","    \"\"\" Base class for CTC loss data. \"\"\"\n","    def __init__(\n","            self,\n","            labels: tf.Tensor,\n","            logprobas: tf.Tensor,\n","            label_length: tf.Tensor,\n","            logit_length: tf.Tensor,\n","            blank_index: Union[int, tf.Tensor],\n","            swap_memory: bool = False,\n","            **kwargs\n","    ):\n","        super().__init__(**kwargs)\n","        self._logprobas = logprobas\n","        self._original_label = labels\n","        self._logit_length = logit_length\n","        self._original_label_length = label_length\n","        self.max_label_length_plus_one = tf.shape(labels)[1]\n","        self._verify_inputs()\n","\n","        if isinstance(blank_index, (tf.Tensor, tf.Variable)):\n","            self._blank_index = blank_index\n","        else:\n","            self._blank_index = tf.constant(blank_index, dtype=tf.int32)\n","\n","        self._swap_memory = swap_memory\n","\n","    def _verify_inputs(self) -> None:\n","        assert len(self._logprobas.shape) == 3\n","        assert self._logprobas.dtype == tf.float32\n","        assert len(self._original_label.shape) == 2\n","        assert len(self._logit_length.shape) == 1\n","        assert len(self._original_label_length.shape) == 1\n","\n","        assert self._logprobas.shape[0] == self._original_label.shape[0]\n","        assert self._logprobas.shape[0] == self._logit_length.shape[0]\n","        assert self._logprobas.shape[0] == self._original_label_length.shape[0]\n","\n","    @tf.custom_gradient\n","    def forward_fn(self, unused_logprobas: tf.Tensor) -> tf.Tensor:\n","        def backprop(d_loss):\n","            return expand_many_dims(d_loss, axes=[1, 2]) * self.gradient_fn(unused_logprobas)\n","\n","        return self.loss, backprop\n","\n","    @tf.custom_gradient\n","    def gradient_fn(self, unused_logprobas: tf.Tensor) -> tf.Tensor:\n","        def backprop(d_gradient):\n","            output = tf.reduce_sum(\n","                input_tensor=expand_many_dims(d_gradient, axes=[1, 2]) * self.hessian_fn(unused_logprobas),\n","                axis=[3, 4]\n","            )\n","            return output\n","\n","        return self.gradient, backprop\n","\n","    @tf.custom_gradient\n","    def hessian_fn(self, unused_logprobas: tf.Tensor) -> tf.Tensor:\n","        def backprop(d_hessian):\n","            raise NotImplementedError(f\"Third order derivative over the ctc loss function is not implemented.\")\n","\n","        return self.hessian, backprop\n","\n","    @cached_property\n","    def hessian(self) -> tf.Tensor:\n","        \"\"\"Calculates Hessian of loss w.r.t. input logits.\n","\n","        Returns: tf.Tensor, shape = [batch_size, max_logit_length, num_tokens, max_logit_length, num_tokens]\n","        \"\"\"\n","        alpha_gamma_term = self.combine_transition_probabilities(a=self.alpha[:, :-1], b=self.gamma[:, 1:])\n","        # shape = [batch_size, max_logit_length, num_tokens, max_logit_length + 1, max_label_length + 1]\n","        alpha_gamma_beta_term = \\\n","            self.combine_transition_probabilities(a=alpha_gamma_term[:, :, :, :-1], b=self.beta[:, 1:])\n","        # shape = [batch_size, max_logit_length, num_tokens, max_logit_length, num_tokens]\n","        alpha_gamma_beta_loss_term = expand_many_dims(self.loss, axes=[1, 2, 3, 4]) + alpha_gamma_beta_term\n","        # shape = [batch_size, max_logit_length, num_tokens]\n","        logit_length_x_num_tokens = self.max_logit_length * self.num_tokens\n","        first_term = tf.reshape(\n","            tf.linalg.set_diag(\n","                input=tf.reshape(\n","                    tensor=alpha_gamma_beta_loss_term,\n","                    shape=[self.batch_size, logit_length_x_num_tokens, logit_length_x_num_tokens]\n","                ),\n","                diagonal=tf.reshape(\n","                    tensor=self.logarithmic_logproba_gradient,\n","                    shape=[self.batch_size, logit_length_x_num_tokens]\n","                )\n","            ),\n","            shape=tf.shape(alpha_gamma_beta_term),\n","        )\n","\n","        mask = expand_many_dims(\n","            input=tf.linalg.band_part(tf.ones(shape=[self.max_logit_length] * 2, dtype=tf.bool), 0, -1),\n","            axes=[0, 2, 4]\n","        )\n","        symmetrized_first_term = tf.where(\n","            condition=mask,\n","            x=first_term,\n","            y=tf.transpose(first_term, [0, 3, 4, 1, 2]),\n","        )\n","        # shape = [batch_size, max_logit_length, num_tokens, max_logit_length, num_tokens]\n","        hessian = \\\n","            -tf.exp(symmetrized_first_term) \\\n","            + expand_many_dims(self.gradient, [3, 4]) * expand_many_dims(self.gradient, [1, 2])\n","        # shape = [batch_size, max_logit_length, num_tokens, max_logit_length, num_tokens]\n","\n","        # Filter out samples with infinite loss\n","        hessian = tf.where(\n","            condition=expand_many_dims(self.loss == inf, [1, 2, 3, 4]),\n","            x=tf.zeros(shape=[1, 1, 1, 1, 1]),\n","            y=hessian,\n","        )\n","        # shape = [batch_size, max_logit_length, num_tokens, max_logit_length, num_tokens]\n","\n","        # Filter out logits that beyond logits length\n","        hessian = tf.where(\n","            condition=expand_many_dims(self.logit_length_mask, axes=[2, 3, 4]),\n","            x=hessian,\n","            y=0.\n","        )\n","        hessian = tf.where(\n","            condition=expand_many_dims(self.logit_length_mask, axes=[1, 2, 4]),\n","            x=hessian,\n","            y=0.\n","        )\n","\n","        return hessian\n","\n","    @cached_property\n","    def gradient(self) -> tf.Tensor:\n","        # shape = [batch_size, max_logit_length, num_tokens]\n","        return -tf.exp(self.logarithmic_logproba_gradient)\n","\n","    @cached_property\n","    def logarithmic_logproba_gradient(self) -> tf.Tensor:\n","        \"\"\"Calculates logarithmic gradient of log loss w.r.t. input logarithmic probabilities.\n","\n","        Returns: tf.Tensor, shape = [batch_size, max_logit_length, num_tokens]\n","        \"\"\"\n","        logarithmic_logproba_gradient = \\\n","            tf.reshape(self.loss, [-1, 1, 1]) \\\n","            + self.combine_transition_probabilities(a=self.alpha[:, :-1], b=self.beta[:, 1:])\n","        # shape = [batch_size, max_logit_length, num_tokens]\n","\n","        # Filter out samples infinite loss\n","        logarithmic_logproba_gradient = tf.where(\n","            condition=expand_many_dims(self.loss == inf, [1, 2]),\n","            x=-inf,\n","            y=logarithmic_logproba_gradient,\n","        )\n","        # shape = [batch_size, max_logit_length, num_tokens]\n","\n","        # Filter out logits that beyond logits length\n","        logarithmic_logproba_gradient = apply_logarithmic_mask(\n","            tensor=logarithmic_logproba_gradient,\n","            mask=tf.expand_dims(self.logit_length_mask, axis=2),\n","        )\n","        # shape = [batch_size, max_logit_length, num_tokens]\n","\n","        return logarithmic_logproba_gradient\n","\n","    @property\n","    @abstractmethod\n","    def alpha(self) -> tf.Tensor:\n","        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, ...]\n","        raise NotImplementedError()\n","\n","    @property\n","    @abstractmethod\n","    def beta(self) -> tf.Tensor:\n","        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, ...]\n","        raise NotImplementedError()\n","\n","    @property\n","    @abstractmethod\n","    def gamma(self) -> tf.Tensor:\n","        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, ...,\n","        #   max_logit_length + 1, max_label_length + 1, ...]\n","        raise NotImplementedError()\n","\n","    @cached_property\n","    def expected_token_logproba(self) -> tf.Tensor:\n","        \"\"\"Logarithmic probability to predict label token.\n","\n","        Returns:shape = [batch_size, max_logit_length, max_label_length + 1]\n","        \"\"\"\n","        label_logproba = tf.gather(\n","            params=self.logproba,\n","            indices=self.label,\n","            axis=2,\n","            batch_dims=1,\n","        )\n","        expected_token_logproba = \\\n","            apply_logarithmic_mask(label_logproba, tf.expand_dims(self.label_length_mask, axis=1))\n","        # shape = [batch_size, max_logit_length, max_label_length + 1]\n","        return expected_token_logproba\n","\n","    @property\n","    @abstractmethod\n","    def loss(self) -> tf.Tensor:\n","        \"\"\"Samplewise loss function value that is minus logarithmic probability to predict label sequence.\n","\n","        Returns:    tf.Tensor, shape = [batch_size]\n","        \"\"\"\n","        raise NotImplementedError()\n","\n","    @cached_property\n","    def label_token_logproba(self) -> tf.Tensor:\n","        \"\"\" shape = [batch_size, max_logit_length, max_label_length + 1] \"\"\"\n","        return tf.gather(\n","            params=self.logproba,\n","            indices=self.label,\n","            axis=2,\n","            batch_dims=1,\n","        )\n","\n","    @cached_property\n","    def blank_logproba(self):\n","        \"\"\"Calculates logarithmic probability to predict blank token for given logit.\n","\n","        Returns:    tf.Tensor, shape = [batch_size, max_logit_length]\n","        \"\"\"\n","        return self.logproba[:, :, self.blank_token_index]\n","\n","    @cached_property\n","    def input_proba(self) -> tf.Tensor:\n","        \"\"\" shape = [batch_size, input_logit_tensor_length, num_tokens], dtype = tf.float32 \"\"\"\n","        return tf.exp(self.logproba)\n","\n","    @cached_property\n","    def logproba(self) -> tf.Tensor:\n","        mask = tf.expand_dims(tf.sequence_mask(lengths=self._logit_length, maxlen=self.max_logit_length), 2)\n","        blank_logprobas = tf.reshape(tf.math.log(tf.one_hot(self.blank_token_index, self.num_tokens)), shape=[1, 1, -1])\n","        logprobas = tf.where(\n","            condition=mask,\n","            x=self._logprobas,\n","            y=blank_logprobas,\n","        )\n","        return logprobas\n","\n","    '''\n","    def cleaned_label(self) -> tf.Tensor:\n","        \"\"\" shape = [batch, max_label_length + 1] \"\"\"\n","        _ = self.max_label_length_plus_one\n","    '''\n","    @cached_property\n","    def cleaned_label(self):\n","        # Repair padding- apparently, TPU/ GPU jit cannot handle the padding here; I'm not sure why. Anyway, it does not seem necessary in our case.\n","        labels = self._original_label[:, :self.max_label_length_plus_one]\n","        '''\n","        labels = tf.cond(\n","            pred=tf.shape(self._original_label)[1] > self.max_label_length,\n","            true_fn=lambda: self._original_label[:, :self.max_label_length_plus_one],\n","            false_fn=lambda: pad_until(\n","                tensor=self._original_label,\n","                desired_size=self.max_label_length_plus_one,\n","                pad_value=self.pad_token_index,\n","                axis=1\n","            )\n","        )\n","        '''\n","        mask = tf.sequence_mask(lengths=self._original_label_length, maxlen=tf.shape(labels)[1])\n","        blank_label = tf.ones_like(labels) * self.pad_token_index\n","        cleaned_label = tf.where(\n","            condition=mask,\n","            x=labels,\n","            y=blank_label,\n","        )\n","        return cleaned_label\n","\n","    def select_from_act(self, act: tf.Tensor, label: tf.Tensor) -> tf.Tensor:\n","        \"\"\"Takes tensor of acts act_{b, a, t, u, ...} and labels label_{b,u},\n","        where b is the batch index, t is the logit index, and u is the label index,\n","        and returns for each token index k the tensor\n","\n","            output_{b,a,t,k,...} = logsumexp_u act_{b,a,t,u_k,...} * kroneker_delta(u_k = label_{b,u})\n","\n","        that is logarithmic sum of exponents of acts for all u_k = label_{b,u}, given b, t and k.\n","\n","        Args:\n","            act:    tf.Tensor, shape = [batch_size, dim_a, max_logit_length, max_label_length + 1, ...]\n","            label:  tf.Tensor, shape = [batch_size, max_label_length + 1]\n","\n","        Returns:    tf.Tensor, shape = [batch_size, max_label_length + 1, num_tokens, ...]\n","        \"\"\"\n","        data = smart_transpose(a=act, perm=[0, 3, 2, 1])\n","        # shape = [batch_size, max_label_length + 1, max_logit_length, dim_a, ...]\n","        data = tf.squeeze(\n","            input=smart_reshape(\n","                tensor=data,\n","                shape=[1, self.batch_size * self.max_label_length_plus_one, self.max_logit_length]\n","            ),\n","            axis=0\n","        )\n","        # shape = [batch_size * (max_label_length + 1), max_logit_length, dim_a, ...]\n","\n","        segment_ids = tf.reshape(label + tf.expand_dims(tf.range(self.batch_size), 1) * self.num_tokens, shape=[-1])\n","        # shape = [batch_size * (max_label_length + 1)]\n","        num_segments = self.batch_size * self.num_tokens\n","\n","        output = unsorted_segment_logsumexp(data=data, segment_ids=segment_ids, num_segments=num_segments)\n","        # shape = [batch_size * num_tokens, max_logit_length, dim_a, ...]\n","        output = smart_reshape(tf.expand_dims(output, 0), [self.batch_size, self.num_tokens, self.max_logit_length])\n","        # shape = [batch_size, num_tokens, max_logit_length, dim_a, ...]\n","        output = smart_transpose(output, [0, 3, 2, 1])\n","        # shape = [batch_size, dim_a, max_logit_length, num_tokens, ...]\n","        return output\n","\n","    @cached_property\n","    def max_logit_length_plus_one(self) -> tf.Tensor:\n","        return self.max_logit_length + tf.constant(1, dtype=tf.int32)\n","\n","    @cached_property\n","    def max_logit_length(self) -> tf.Tensor:\n","        return tf.shape(self._logprobas)[1]\n","\n","    @cached_property\n","    def max_label_length_plus_one(self) -> tf.Tensor:\n","        return self.max_label_length + tf.constant(1, dtype=tf.int32)\n","\n","    @cached_property\n","    def max_label_length(self) -> tf.Tensor:\n","        return reduce_max_with_default(self._original_label_length, default=tf.constant(0, dtype=tf.int32))\n","\n","    @cached_property\n","    def pad_token_index(self) -> tf.Tensor:\n","        return self.blank_token_index\n","\n","    @cached_property\n","    def num_tokens(self) -> tf.Tensor:\n","        return tf.shape(self._logprobas)[2]\n","\n","    @cached_property\n","    def blank_token_index(self) -> tf.Tensor:\n","        return self._blank_index\n","\n","    @cached_property\n","    def logit_length_mask(self) -> tf.Tensor:\n","        \"\"\" shape = [batch_size, max_logit_length] \"\"\"\n","        return tf.sequence_mask(\n","            lengths=self._logit_length,\n","            maxlen=self.max_logit_length,\n","        )\n","\n","    @cached_property\n","    def label_length_mask(self) -> tf.Tensor:\n","        \"\"\" shape = [batch_size, max_label_length + 1], dtype = tf.bool \"\"\"\n","        return tf.sequence_mask(lengths=self.label_length, maxlen=self.max_label_length_plus_one)\n","\n","    @property\n","    def label_length(self) -> tf.Tensor:\n","        return self._original_label_length\n","\n","    @cached_property\n","    def preceded_label(self) -> tf.Tensor:\n","        \"\"\"Preceded label. For example, for label \"abc_\" the sequence \"_abc\" is returned.\n","\n","        Returns:    tf.Tensor, shape = [batch_size, max_label_length + 1]\n","        \"\"\"\n","        return tf.roll(self.label, shift=1, axis=1)\n","\n","    @cached_property\n","    def label(self) -> tf.Tensor:\n","        \"\"\" shape = [batch, max_label_length + 1] \"\"\"\n","        return self.cleaned_label\n","\n","    @cached_property\n","    def batch_size(self) -> tf.Tensor:\n","        return tf.shape(self._logprobas)[0]\n","\n","    @abstractmethod\n","    def combine_transition_probabilities(self, a: tf.Tensor, b: tf.Tensor) -> tf.Tensor:\n","        \"\"\"Given logarithmic probabilities a and b are merges like\n","            a, b -> log( exp a exp p exp b )\n","        \"\"\"\n","        raise NotImplementedError()\n","\n","\n","def classic_ctc_loss(\n","        labels: tf.Tensor,\n","        logits: tf.Tensor,\n","        label_length: tf.Tensor,\n","        logit_length: tf.Tensor,\n","        blank_index: Union[int, tf.Tensor] = 0,\n",") -> tf.Tensor:\n","    \"\"\"Computes CTC (Connectionist Temporal Classification) loss from\n","    http://www.cs.toronto.edu/~graves/icml_2006.pdf.\n","\n","    Repeated non-blank labels will be merged.\n","    For example, predicted sequence\n","        a_bb_ccc_cc\n","    corresponds to label\n","        abcc\n","    where \"_\" is the blank token.\n","\n","    If label length is longer then the logit length the output loss for the corresponding sample in the batch\n","    is +tf.inf and the gradient is 0. For example, for label \"abb\" at least 4 tokens are needed.\n","    It is because the output sequence must be at least \"ab_b\" in order to handle the repeated token.\n","\n","    Args:\n","        labels:         tf.Tensor, shape = [batch, max_label_length],       dtype = tf.int32\n","        logits:         tf.Tensor, shape = [batch, max_length, mum_tokens], dtype = tf.float32\n","        label_length:   tf.Tensor, shape = [batch],                         dtype = tf.int32\n","        logit_length:   tf.Tensor, shape = [batch],                         dtype = tf.int32\n","        blank_index:    tf.Tensor or pythonic static integer between 0 <= blank_index < mum_tokens\n","\n","    Returns:            tf.Tensor, shape = [batch, max_length, mum_tokens], dtype = tf.float32\n","    \"\"\"\n","    return ctc_loss(\n","        labels=labels,\n","        logits=logits,\n","        label_length=label_length,\n","        logit_length=logit_length,\n","        blank_index=blank_index,\n","        ctc_loss_data_cls=ClassicCtcLossData\n","    )\n","\n","\n","class ClassicCtcLossData(BaseCtcLossData):\n","    \"\"\"Calculate loss data for CTC (Connectionist Temporal Classification) loss from\n","    http://www.cs.toronto.edu/~graves/icml_2006.pdf.\n","\n","    This loss is actually the logarithmic likelihood for the classification task with multiple expected class.\n","    All predicated sequences consist of tokens (denoted like \"a\", \"b\", ... below) and the blank \"_\".\n","    The classic CTC decoding merges all repeated non-blank labels and removes the blank.\n","    For example, predicted sequence\n","        a_bb_ccc_c is decoded as \"abcc\".\n","    All predicated sequences that coincided with the label after the decoding are the expected classes\n","    in the logarithmic likelihood loss function.\n","\n","    Implementation:\n","\n","    We calculate alpha_{b,t,l,s} and beta_{b,t,l,s} that are the logarithmic probabilities similar to\n","    this the ones from the sited paper and defined precisely below.\n","    Here, b corresponds to batch, t to logit position, l to label index, and s=0,1 to state (see below for details).\n","\n","    During the decoding procedure, after handling of a part of the logit sequence,\n","    we predict only a part of the target label tokens. We call this subsequence the in the target space as \"state\".\n","    For example, two decode label \"abc\" we have to decode \"a\" first then add \"b\" and move tot the state \"ab\" and\n","    then to the state \"abc\".\n","\n","    In order to handle the token duplication swap in the classic CTC loss we extend the set of all possible labels.\n","    For each token sequence we define two sequences called \"closed\" and \"open\".\n","    For example, for label \"abc\" we consider its two states denoted \"abc>\" (closed) and \"abc<\" (open).\n","    The difference between them is in their behaviour with respect to the token appending. The rules are:\n","        \"...a>\" + \"_\" -> \"...a>\",\n","        \"...a<\" + \"_\" -> \"...a>\",\n","        \"...a>\" + \"a\" -> \"...aa<\",\n","        \"...a<\" + \"a\" -> \"...a<\",\n","        \"...a>\" + \"b\" -> \"...ab<\",\n","        \"...a<\" + \"b\" -> \"...ab<\",\n","    for any different tokens \"a\" and \"b\" and any token sequence denoted by \"...\".\n","    Namely, appending a token the is equal to the last one to an open state does not change this state.\n","    Appending a blank to a state always males this state closed.\n","\n","    This is why alpha_{b,t,l,s} and beta_{b,t,l,s} in the code below are equipped with an additional index s=0,1.\n","    Closed states corresponds s=0 and open ones to s=1.\n","\n","    In particular, the flowing identity is satisfied\n","        sum_s sum_l exp alpha_{b,t,l,s} * exp beta_{b,t,l,s} = loss_{b}, for any b and t\n","    \"\"\"\n","    @cached_property\n","    def diagonal_non_blank_grad_term(self) -> tf.Tensor:\n","        \"\"\" shape = [batch_size, max_logit_length, num_tokens] \"\"\"\n","        input_tensor = \\\n","            self.alpha[:, :-1] \\\n","            + self.any_to_open_diagonal_step_log_proba \\\n","            + tf.roll(self.beta[:, 1:, :, 1:], shift=-1, axis=2)\n","        # shape = [batch_size, max_logit_length, max_label_length + 1, states]\n","        act = tf.reduce_logsumexp(\n","            input_tensor=input_tensor,\n","            axis=3,\n","        )\n","        # shape = [batch_size, max_logit_length, max_label_length + 1]\n","        diagonal_non_blank_grad_term = self.select_from_act(act=act, label=self.label)\n","        # shape = [batch_size, max_logit_length, num_tokens]\n","        return diagonal_non_blank_grad_term\n","\n","    @cached_property\n","    def horizontal_non_blank_grad_term(self) -> tf.Tensor:\n","        \"\"\"Horizontal steps from repeated token: open alpha state to open beta state.\n","\n","        Returns: shape = [batch_size, max_logit_length, num_tokens]\n","        \"\"\"\n","        act = self.alpha[:, :-1, :, 1] + self.previous_label_token_log_proba + self.beta[:, 1:, :, 1]\n","        # shape = [batch_size, max_logit_length, max_label_length + 1]\n","        horizontal_non_blank_grad_term = self.select_from_act(act, self.preceded_label)\n","        return horizontal_non_blank_grad_term\n","\n","    @cached_property\n","    def loss(self) -> tf.Tensor:\n","        \"\"\" shape = [batch_size] \"\"\"\n","        params = tf.reduce_logsumexp(self.alpha[:, -1], -1)\n","        # shape = [batch_size, max_label_length + 1]\n","        loss = -tf.gather(\n","            params=params,                # shape = [batch_size, max_label_length + 1]\n","            indices=self.label_length,    # shape = [batch_size]\n","            batch_dims=1,\n","        )\n","        return loss\n","\n","    @cached_property\n","    def gamma(self) -> tf.Tensor:\n","        \"\"\" shape = [\n","                batch_size,\n","                max_logit_length + 1,\n","                max_label_length + 1,\n","                state,\n","                max_logit_length + 1,\n","                max_label_length + 1,\n","                state,\n","            ],\n","        \"\"\"\n","        # This is to avoid InaccessibleTensorError in graph mode\n","        _, _, _ = self.horizontal_step_log_proba, self.any_to_open_diagonal_step_log_proba, self.diagonal_gamma\n","\n","        gamma_forward_transposed = unfold(\n","            init_tensor=self.diagonal_gamma,\n","            # init_tensor=tf.tile(self.diagonal_gamma, [self.batch_size, self.max_logit_length_plus_one, 1, 1, 1, 1]),\n","            iterfunc=self.gamma_step,\n","            d_i=1,\n","            num_iters=self.max_logit_length,\n","            element_shape=tf.TensorShape([None, None, None, None, None, None]),\n","            name=\"gamma_1\",\n","        )\n","        # shape = [max_logit_length + 1, batch_size, max_logit_length + 1, max_label_length + 1, state,\n","        #   max_label_length + 1, state]\n","\n","        gamma_forward = tf.transpose(gamma_forward_transposed, [1, 2, 3, 4, 0, 5, 6])\n","        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, state,\n","        #   max_logit_length + 1, max_label_length + 1, state]\n","\n","        mask = expand_many_dims(\n","            input=tf.linalg.band_part(tf.ones(shape=[self.max_logit_length_plus_one] * 2, dtype=tf.bool), 0, -1),\n","            axes=[0, 2, 3, 5, 6]\n","        )\n","        # shape = [1, max_logit_length + 1, 1, 1, max_logit_length + 1, 1, 1]\n","        gamma = apply_logarithmic_mask(gamma_forward, mask)\n","        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, state,\n","        #   max_logit_length + 1, max_label_length + 1, state]\n","\n","        return gamma\n","\n","    def gamma_step(\n","        self,\n","        previous_slice: tf.Tensor,\n","        i: tf.Tensor,\n","    ) -> tf.Tensor:\n","        \"\"\"Args:\n","            previous_slice: tf.Tensor,\n","                            shape = [batch_size, max_logit_length + 1, max_label_length + 1, state,\n","                                max_label_length + 1, state]\n","            i:              tf.Tensor,\n","                            shape = [], 0 <= i < max_logit_length + 1\n","\n","        Returns:            tf.Tensor,\n","                            shape = [batch_size, max_logit_length + 1, max_label_length + 1, state,\n","                                max_label_length + 1, state]\n","        \"\"\"\n","        horizontal_step_states = \\\n","            expand_many_dims(self.horizontal_step_log_proba[:, i], axes=[1, 2, 3]) \\\n","            + tf.expand_dims(previous_slice, 5)\n","        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, state,\n","        #          max_label_length + 1, next_state, previous_state]\n","        horizontal_step = tf.reduce_logsumexp(horizontal_step_states, axis=6)\n","        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, state, max_label_length + 1, state]\n","\n","        diagonal_step_log_proba = tf.reduce_logsumexp(\n","            expand_many_dims(self.any_to_open_diagonal_step_log_proba[:, i], axes=[1, 2, 3]) + previous_slice,\n","            axis=5\n","        )\n","        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, state, max_label_length + 1]\n","\n","        # We move by one token because it is a diagonal step\n","        moved_diagonal_step_log_proba = tf.roll(diagonal_step_log_proba, shift=1, axis=4)\n","        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, state, max_label_length + 1]\n","\n","        # Out state is always open:\n","        diagonal_step = tf.pad(\n","            tensor=tf.expand_dims(moved_diagonal_step_log_proba, 5),\n","            paddings=[[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0]],\n","            constant_values=-np.inf\n","        )\n","        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, state, max_label_length + 1, state]\n","        new_gamma_slice = logsumexp(\n","            x=horizontal_step,\n","            y=diagonal_step,\n","        )\n","        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, state, max_label_length + 1, state]\n","\n","        condition = tf.reshape(tf.range(self.max_logit_length_plus_one) <= i, shape=[1, -1, 1, 1, 1, 1])\n","        # shape = [1, max_logit_length + 1, 1, 1, 1, 1, 1]\n","        output_slice = tf.where(\n","            condition=condition,\n","            x=new_gamma_slice,\n","            y=self.diagonal_gamma,\n","        )\n","        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, state, max_label_length + 1, state]\n","\n","        return output_slice\n","\n","    @cached_property\n","    def diagonal_gamma(self) -> tf.Tensor:\n","        \"\"\" shape = [batch_size, max_logit_length_plus_one, max_label_length + 1, state,\n","                     max_label_length + 1, state]\n","        \"\"\"\n","        diagonal_gamma = tf.math.log(\n","            tf.reshape(\n","                tensor=tf.eye(self.max_label_length_plus_one * 2, dtype=tf.float32),\n","                shape=[1, 1, self.max_label_length_plus_one, 2, self.max_label_length_plus_one, 2]\n","            )\n","        )\n","        diagonal_gamma = tf.tile(diagonal_gamma, [self.batch_size, self.max_logit_length_plus_one, 1, 1, 1, 1])\n","        return diagonal_gamma\n","\n","    @cached_property\n","    def beta(self) -> tf.Tensor:\n","        \"\"\"Calculates the beta_{b,t,l,s} that is logarithmic probability of sample 0 <= b < batch_size - 1 in the batch\n","        with logit subsequence from\n","            t, t + 1, ... max_logit_length - 2, max_logit_length - 1,\n","        for t < max_logit_length\n","        to predict the sequence of tokens\n","            w_max_label_length, w_{max_label_length + 1}, ... w_{max_label_length - 2}, w_{max_label_length - 1}\n","        for l < max_label_length\n","        that is either closed s=0 or open s=1.\n","        from label_b = [w_0, w_1, ... w_{max_label_length - 2}, w_{max_label_length - 1}].\n","\n","        This logarithmic probability is calculated by iterations\n","            exp beta_{t-1,l} = p_horizontal_step_{t-1,l} * exp beta_{t,l} + p_diagonal_step_{t-1,l} * exp beta_{t,l+1},\n","        for 0 <= t < max_logit_length,\n","        where p_diagonal_step_{t,l} is the probability to predict label token w_l with logit l\n","        and p_horizontal_step_{t,l} is the probability to skip token w_l prediction with logit l, for example, with\n","        the blank prediction.\n","\n","        Returns:    tf.Tensor,  shape = [batch_size, max_logit_length + 1, max_label_length + 1, state],\n","                    dtype = tf.float32\n","        \"\"\"\n","        # This is to avoid InaccessibleTensorError in graph mode\n","        _, _ = self.horizontal_step_log_proba, self.any_to_open_diagonal_step_log_proba\n","\n","        beta = unfold(\n","            init_tensor=self.last_beta_slice,\n","            iterfunc=self.beta_step,\n","            d_i=-1,\n","            num_iters=self.max_logit_length,\n","            element_shape=tf.TensorShape([None, None, 2]),\n","            name=\"beta\",\n","        )\n","        # shape = [logit_length + 1, batch, label_length + 1, state]\n","        return tf.transpose(beta, [1, 0, 2, 3])\n","\n","    def beta_step(self, previous_slice: tf.Tensor, i: tf.Tensor) -> tf.Tensor:\n","        \"\"\" shape = [batch_size, max_label_length + 1, state] \"\"\"\n","        horizontal_step = \\\n","            tf.reduce_logsumexp(self.horizontal_step_log_proba[:, i] + tf.expand_dims(previous_slice, 3), 2)\n","        # shape = [batch_size, max_label_length + 1, state]\n","        diagonal_step = \\\n","            self.any_to_open_diagonal_step_log_proba[:, i] + tf.roll(previous_slice[:, :, 1:], shift=-1, axis=1)\n","        # shape = [batch_size, max_label_length + 1, state]\n","        new_beta_slice = logsumexp(\n","            x=horizontal_step,  # shape = [batch_size, max_label_length + 1, state]\n","            y=diagonal_step,    # shape = [batch_size, max_label_length + 1, state]\n","        )\n","        # shape = [batch_size, max_label_length + 1, state]\n","        return new_beta_slice\n","\n","    @cached_property\n","    def last_beta_slice(self) -> tf.Tensor:\n","        \"\"\" shape = [batch_size, max_label_length + 1, state] \"\"\"\n","        beta_last = tf.math.log(tf.one_hot(indices=self.label_length, depth=self.max_label_length_plus_one))\n","        beta_last = tf.tile(input=tf.expand_dims(beta_last, axis=2), multiples=[1, 1, 2])\n","        return beta_last\n","\n","    @cached_property\n","    def alpha(self) -> tf.Tensor:\n","        \"\"\"Calculates the alpha_{b,t,l,s} that is\n","        the logarithmic probability of sample 0 <= b < batch_size - 1 in the batch\n","        with logits subsequence from 0, 1, 2, ... t - 2, t - 1, for t < max_logit_length\n","        to predict the sequence of tokens w_0, w_1, w_2, ... w_{l-2}, w_{l-1} for l < max_label_length + 1\n","        that is either closed s=0 or open s=1.\n","        from label_b = [w_0, w_1, ... w_{max_label_length - 2}, w_{max_label_length - 1}].\n","\n","        This logarithmic probability is calculated by iterations\n","            exp alpha_{t + 1,l} = p_horizontal_step_{t,l} * exp alpha_{t,l} + p_diagonal_step_{t,l} * exp alpha_{t,l-1},\n","        for 0 <= t < max_logit_length,\n","        where p_diagonal_step_{t,l} is the probability to predict label token w_l with logit l\n","        and p_horizontal_step_{t,l} is the probability to skip token w_l prediction with logit l, for example, with\n","        the blank prediction.\n","\n","        Returns:    tf.Tensor,  shape = [batch_size, max_logit_length + 1, max_label_length + 1, state],\n","                    dtype = tf.float32\n","        \"\"\"\n","        # This is to avoid InaccessibleTensorError in graph mode\n","        _, _ = self.horizontal_step_log_proba, self.any_to_open_diagonal_step_log_proba\n","\n","        alpha = unfold(\n","            init_tensor=self.first_alpha_slice,\n","            iterfunc=self.alpha_step,\n","            d_i=1,\n","            num_iters=self.max_logit_length,\n","            element_shape=tf.TensorShape([None, None, 2]),\n","            name=\"alpha\",\n","        )\n","        # shape = [logit_length + 1, batch_size, label_length + 1, state]\n","        return tf.transpose(alpha, [1, 0, 2, 3])\n","\n","    def alpha_step(self, previous_slice: tf.Tensor, i: tf.Tensor) -> tf.Tensor:\n","        \"\"\"Args:\n","            previous_slice: shape = [batch_size, max_label_length + 1, state]\n","            i:\n","\n","        Returns:            shape = [batch_size, max_label_length + 1, state]\n","        \"\"\"\n","        temp = self.horizontal_step_log_proba[:, i] + tf.expand_dims(previous_slice, 2)\n","        # shape = [batch_size, max_label_length + 1, next_state, previous_state]\n","        horizontal_step = tf.reduce_logsumexp(temp, 3)\n","        # shape = [batch_size, max_label_length + 1, state]\n","        diagonal_step_log_proba = \\\n","            tf.reduce_logsumexp(self.any_to_open_diagonal_step_log_proba[:, i] + previous_slice, 2)\n","        # shape = [batch_size, max_label_length + 1]\n","\n","        # We move by one token because it is a diagonal step\n","        moved_diagonal_step_log_proba = tf.roll(diagonal_step_log_proba, shift=1, axis=1)\n","        # shape = [batch_size, max_label_length + 1]\n","\n","        # Out state is always open:\n","        diagonal_step = tf.pad(\n","            tensor=tf.expand_dims(moved_diagonal_step_log_proba, 2),\n","            paddings=[[0, 0], [0, 0], [1, 0]],\n","            constant_values=-np.inf\n","        )\n","        # shape = [batch_size, max_label_length + 1, state]\n","        new_alpha_slice = logsumexp(\n","            x=horizontal_step,\n","            y=diagonal_step,\n","        )\n","        # shape = [batch_size, max_label_length + 1, state]\n","        return new_alpha_slice\n","\n","    @cached_property\n","    def first_alpha_slice(self) -> tf.Tensor:\n","        \"\"\" shape = [batch_size, max_label_length + 1, state] \"\"\"\n","        alpha_0 = tf.math.log(tf.one_hot(indices=0, depth=self.max_label_length_plus_one * 2))\n","        alpha_0 = tf.tile(input=tf.reshape(alpha_0, [1, -1, 2]), multiples=[self.batch_size, 1, 1])\n","        return alpha_0\n","\n","    @cached_property\n","    def any_to_open_diagonal_step_log_proba(self) -> tf.Tensor:\n","        \"\"\"Logarithmic probability to make a diagonal step from given state to an open state\n","\n","        Returns:shape = [batch_size, max_logit_length, max_label_length + 1, state]\n","        \"\"\"\n","        return tf.stack(\n","            values=[self.closed_to_open_diagonal_step_log_proba, self.open_to_open_diagonal_step_log_proba],\n","            axis=3\n","        )\n","\n","    @cached_property\n","    def open_to_open_diagonal_step_log_proba(self) -> tf.Tensor:\n","        \"\"\"Logarithmic probability to make a diagonal step from an open state to an open state\n","        with expected token prediction that is different from the previous one.\n","\n","        Returns:shape = [batch_size, max_logit_length, max_label_length + 1]\n","        \"\"\"\n","        # We check that the predicting token does not equal to previous one\n","        token_repetition_mask = self.label != tf.roll(self.label, shift=1, axis=1)\n","        # shape = [batch_size, max_label_length + 1]\n","        open_diagonal_step_log_proba = \\\n","            apply_logarithmic_mask(\n","                self.closed_to_open_diagonal_step_log_proba,\n","                tf.expand_dims(token_repetition_mask, axis=1)\n","            )\n","        return open_diagonal_step_log_proba\n","\n","    @cached_property\n","    def closed_to_open_diagonal_step_log_proba(self) -> tf.Tensor:\n","        \"\"\"Logarithmic probability to make a diagonal step from a closed state to an open state\n","        with expected token prediction.\n","\n","        Returns:shape = [batch_size, max_logit_length, max_label_length + 1]\n","        \"\"\"\n","        return self.expected_token_logproba\n","\n","    @cached_property\n","    def horizontal_step_log_proba(self) -> tf.Tensor:\n","        \"\"\"Calculates logarithmic probability of the horizontal step for given logit x label position.\n","\n","        This is possible in two alternative cases:\n","        1. Blank\n","        2. Not blank token from previous label position.\n","\n","        Returns: tf.Tensor, shape = [batch_size, max_logit_length, max_label_length + 1, next_state, previous_state]\n","        \"\"\"\n","        # We map closed and open states to closed states\n","        blank_term = tf.tile(\n","            input=tf.expand_dims(tf.expand_dims(self.blank_logproba, 2), 3),\n","            multiples=[1, 1, self.max_label_length_plus_one, 2]\n","        )\n","        # shape = [batch_size, max_logit_length, max_label_length + 1, 2]\n","        non_blank_term = tf.pad(\n","            tf.expand_dims(self.not_blank_horizontal_step_log_proba, 3),\n","            paddings=[[0, 0], [0, 0], [0, 0], [1, 0]],\n","            constant_values=tf.constant(-np.inf),\n","        )\n","        # shape = [batch_size, max_logit_length, max_label_length + 1, 2]\n","        horizontal_step_log_proba = tf.stack([blank_term, non_blank_term], axis=3)\n","        return horizontal_step_log_proba\n","\n","    @cached_property\n","    def not_blank_horizontal_step_log_proba(self) -> tf.Tensor:\n","        \"\"\" shape = [batch_size, max_logit_length, max_label_length + 1] \"\"\"\n","        mask = tf.reshape(1 - tf.one_hot(self.blank_token_index, depth=self.num_tokens), shape=[1, 1, -1])\n","        not_blank_log_proba = apply_logarithmic_mask(self.logproba, mask)\n","        not_blank_horizontal_step_log_proba = tf.gather(\n","            params=not_blank_log_proba,\n","            indices=tf.roll(self.label, shift=1, axis=1),\n","            axis=2,\n","            batch_dims=1,\n","        )\n","        # shape = [batch_size, max_logit_length, max_label_length + 1]\n","        return not_blank_horizontal_step_log_proba\n","\n","    @cached_property\n","    def previous_label_token_log_proba(self) -> tf.Tensor:\n","        \"\"\"Calculates the probability to predict token that preceded to label token.\n","\n","        Returns:    tf.Tensor,  shape = [batch_size, max_logit_length, max_label_length + 1]\n","        \"\"\"\n","        previous_label_token_log_proba = tf.gather(\n","            params=self.logproba,\n","            indices=self.preceded_label,\n","            axis=2,\n","            batch_dims=1,\n","        )\n","        # shape = [batch_size, max_logit_length, max_label_length + 1]\n","        return previous_label_token_log_proba\n","\n","    @cached_property\n","    def blank_logproba(self) -> tf.Tensor:\n","        \"\"\" shape = [batch_size, max_logit_length] \"\"\"\n","        return self.logproba[:, :, self.blank_token_index]\n","\n","    def combine_transition_probabilities(self, a: tf.Tensor, b: tf.Tensor) -> tf.Tensor:\n","        \"\"\"Transforms logarithmic transition probabilities a and b.\n","\n","        Args:\n","            a:      shape = [batch, DIMS_A, max_logit_length, max_label_length + 1, state]\n","            b:      shape = [batch, max_logit_length, max_label_length + 1, state, DIMS_B]\n","\n","        Returns:    shape = [batch, DIMS_A, max_logit_length, num_tokens, DIMS_B]\n","        \"\"\"\n","        assert len(a.shape) >= 4\n","        assert len(b.shape) >= 4\n","        assert a.shape[-1] == 2\n","        assert b.shape[3] == 2\n","\n","        dims_a = tf.shape(a)[1:-3]\n","        dims_b = tf.shape(b)[4:]\n","        a = tf.reshape(a, shape=[self.batch_size, -1, self.max_logit_length, self.max_label_length_plus_one, 2, 1])\n","        # shape = [batch_size, dims_a, max_logit_length, max_label_length + 1, state, 1]\n","        b = tf.reshape(b, shape=[self.batch_size, 1, self.max_logit_length, self.max_label_length_plus_one, 2, -1])\n","        # shape = [batch_size, 1, max_logit_length, max_label_length + 1, state, dims_b]\n","\n","        # Either open or closed state from alpha and only closed state from beta\n","        ab_term = tf.reduce_logsumexp(a, 4) + b[:, :, :, :, 0]\n","        # shape = [batch_size, dims_a, max_logit_length, max_label_length + 1, dims_b]\n","\n","        horizontal_blank_grad_term = \\\n","            expand_many_dims(self.blank_logproba, axes=[1, 3]) + tf.reduce_logsumexp(ab_term, axis=3)\n","        # shape = [batch_size, dims_a, max_logit_length, dims_b]\n","\n","        act = a[:, :, :, :, 1] + expand_many_dims(self.previous_label_token_log_proba, axes=[1, 4]) + b[:, :, :, :, 1]\n","        # shape = [batch_size, dim_a, max_logit_length, max_label_length + 1, dim_b]\n","\n","        horizontal_non_blank_grad_term = self.select_from_act(act, self.preceded_label)\n","        # shape = [batch_size, dim_a, max_logit_length, num_tokens, dim_b]\n","\n","        input_tensor = a + expand_many_dims(self.any_to_open_diagonal_step_log_proba, axes=[1, 5]) + \\\n","            tf.roll(b[:, :, :, :, 1:], shift=-1, axis=3)\n","        # shape = [batch_size, dim_a, max_logit_length, max_label_length + 1, states, dim_b]\n","\n","        act = tf.reduce_logsumexp(input_tensor=input_tensor, axis=4)\n","        # shape = [batch_size, dim_a, max_logit_length, max_label_length + 1, dim_b]\n","\n","        diagonal_non_blank_grad_term = self.select_from_act(act=act, label=self.label)\n","        # shape = [batch_size, dim_a, max_logit_length, num_tokens, dim_b]\n","\n","        non_blank_grad_term = logsumexp(horizontal_non_blank_grad_term, diagonal_non_blank_grad_term)\n","        # shape = [batch_size, dim_a, max_logit_length, num_tokens, dim_b]\n","\n","        blank_mask = self.blank_token_index == tf.range(self.num_tokens)\n","        # shape = [num_tokens]\n","\n","        output = tf.where(\n","            condition=expand_many_dims(blank_mask, axes=[0, 1, 2, 4]),\n","            x=tf.expand_dims(horizontal_blank_grad_term, 3),\n","            y=non_blank_grad_term,\n","        )\n","        # shape = [batch, dim_a, max_logit_length, num_tokens, dim_b]\n","        output_shape = tf.concat(\n","            [\n","                tf.expand_dims(self.batch_size, axis=0),\n","                dims_a,\n","                tf.expand_dims(self.max_logit_length, axis=0),\n","                tf.expand_dims(self.num_tokens, axis=0),\n","                dims_b\n","            ],\n","            axis=0\n","        )\n","        output_reshaped = tf.reshape(output, shape=output_shape)\n","        # shape = [batch, DIMS_A, max_logit_length, num_tokens, DIMS_B]\n","\n","        return output_reshaped"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1690422412641,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"0gUj7mAgYi10","trusted":true},"outputs":[],"source":["class MemoryUsageCallbackExtended(tf.keras.callbacks.Callback):\n","    \"\"\"Monitor memory usage on epoch begin and end, collect garbage\"\"\"\n","\n","    # def on_epoch_begin(self, epoch, logs=None):\n","    #    print(\"**Epoch {}**\".format(epoch))\n","    #    print(\n","    #        f\"Memory usage on epoch begin: {int(psutil.Process(os.getpid()).memory_info().rss)/1e9:.2f GB}\"\n","    #    )\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        print(\n","            f\"Memory usage on epoch end: {int(psutil.Process(os.getpid()).memory_info().rss)/1e9:.2f} GB\"\n","        )\n","        gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"eErQkrlpYi11"},"source":["# Scheduler"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":102,"status":"ok","timestamp":1690422412737,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"umV9OdZCYi11","trusted":true},"outputs":[],"source":["\n","class CosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    \"\"\"A LearningRateSchedule that uses a cosine decay with optional warmup.\n","\n","    See [Loshchilov & Hutter, ICLR2016](https://arxiv.org/abs/1608.03983),\n","    SGDR: Stochastic Gradient Descent with Warm Restarts.\n","\n","    For the idea of a linear warmup of our learning rate,\n","    see [Goyal et al.](https://arxiv.org/pdf/1706.02677.pdf).\n","\n","    When we begin training a model, we often want an initial increase in our\n","    learning rate followed by a decay. If `warmup_target` is an int, this\n","    schedule applies a linear increase per optimizer step to our learning rate\n","    from `initial_learning_rate` to `warmup_target` for a duration of\n","    `warmup_steps`. Afterwards, it applies a cosine decay function taking our\n","    learning rate from `warmup_target` to `alpha` for a duration of\n","    `decay_steps`. If `warmup_target` is None we skip warmup and our decay\n","    will take our learning rate from `initial_learning_rate` to `alpha`.\n","    It requires a `step` value to  compute the learning rate. You can\n","    just pass a TensorFlow variable that you increment at each training step.\n","\n","    The schedule is a 1-arg callable that produces a warmup followed by a\n","    decayed learning rate when passed the current optimizer step. This can be\n","    useful for changing the learning rate value across different invocations of\n","    optimizer functions.\n","\n","    Our warmup is computed as:\n","\n","    ```python\n","    def warmup_learning_rate(step):\n","        completed_fraction = step / warmup_steps\n","        total_delta = target_warmup - initial_learning_rate\n","        return completed_fraction * total_delta\n","    ```\n","\n","    And our decay is computed as:\n","\n","    ```python\n","    if warmup_target is None:\n","        initial_decay_lr = initial_learning_rate\n","    else:\n","        initial_decay_lr = warmup_target\n","\n","    def decayed_learning_rate(step):\n","        step = min(step, decay_steps)\n","        cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))\n","        decayed = (1 - alpha) * cosine_decay + alpha\n","        return initial_decay_lr * decayed\n","    ```\n","\n","    Example usage without warmup:\n","\n","    ```python\n","    decay_steps = 1000\n","    initial_learning_rate = 0.1\n","    lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n","        initial_learning_rate, decay_steps)\n","    ```\n","\n","    Example usage with warmup:\n","\n","    ```python\n","    decay_steps = 1000\n","    initial_learning_rate = 0\n","    warmup_steps = 1000\n","    target_learning_rate = 0.1\n","    lr_warmup_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n","        initial_learning_rate, decay_steps, warmup_target=target_learning_rate,\n","        warmup_steps=warmup_steps\n","    )\n","    ```\n","\n","    You can pass this schedule directly into a `tf.keras.optimizers.Optimizer`\n","    as the learning rate. The learning rate schedule is also serializable and\n","    deserializable using `tf.keras.optimizers.schedules.serialize` and\n","    `tf.keras.optimizers.schedules.deserialize`.\n","\n","    Returns:\n","      A 1-arg callable learning rate schedule that takes the current optimizer\n","      step and outputs the decayed learning rate, a scalar `Tensor` of the same\n","      type as `initial_learning_rate`.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        initial_learning_rate,\n","        decay_steps,\n","        alpha=0.0,\n","        name=None,\n","        warmup_target=None,\n","        warmup_steps=0,\n","    ):\n","        \"\"\"Applies cosine decay to the learning rate.\n","\n","        Args:\n","          initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a\n","            Python int. The initial learning rate.\n","          decay_steps: A scalar `int32` or `int32` `Tensor` or a Python int.\n","            Number of steps to decay over.\n","          alpha: A scalar `float32` or `float64` `Tensor` or a Python int.\n","            Minimum learning rate value for decay as a fraction of\n","            `initial_learning_rate`.\n","          name: String. Optional name of the operation.  Defaults to\n","            'CosineDecay'.\n","          warmup_target: None or a scalar `float32` or `float64` `Tensor` or a\n","            Python int. The target learning rate for our warmup phase. Will cast\n","            to the `initial_learning_rate` datatype. Setting to None will skip\n","            warmup and begins decay phase from `initial_learning_rate`.\n","            Otherwise scheduler will warmup from `initial_learning_rate` to\n","            `warmup_target`.\n","          warmup_steps: A scalar `int32` or `int32` `Tensor` or a Python int.\n","            Number of steps to warmup over.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.initial_learning_rate = initial_learning_rate\n","        self.decay_steps = decay_steps\n","        self.alpha = alpha\n","        self.name = name\n","        self.warmup_steps = warmup_steps\n","        self.warmup_target = warmup_target\n","\n","    def _decay_function(self, step, decay_steps, decay_from_lr, dtype):\n","        with tf.name_scope(self.name or \"CosineDecay\"):\n","            completed_fraction = step / decay_steps\n","            tf_pi = tf.constant(math.pi, dtype=dtype)\n","            cosine_decayed = 0.5 * (1.0 + tf.cos(tf_pi * completed_fraction))\n","            decayed = (1 - self.alpha) * cosine_decayed + self.alpha\n","            return tf.multiply(decay_from_lr, decayed)\n","\n","    def _warmup_function(self, step, warmup_steps, warmup_target, initial_learning_rate):\n","        with tf.name_scope(self.name or \"CosineDecay\"):\n","            completed_fraction = step / warmup_steps\n","            total_step_delta = warmup_target - initial_learning_rate\n","            return total_step_delta * completed_fraction + initial_learning_rate\n","\n","    def __call__(self, step):\n","        with tf.name_scope(self.name or \"CosineDecay\"):\n","            initial_learning_rate = tf.convert_to_tensor(\n","                self.initial_learning_rate, name=\"initial_learning_rate\"\n","            )\n","            dtype = initial_learning_rate.dtype\n","            decay_steps = tf.cast(self.decay_steps, dtype)\n","            global_step_recomp = tf.cast(step, dtype)\n","\n","            if self.warmup_target is None:\n","                global_step_recomp = tf.minimum(global_step_recomp, decay_steps)\n","                return self._decay_function(\n","                    global_step_recomp,\n","                    decay_steps,\n","                    initial_learning_rate,\n","                    dtype,\n","                )\n","\n","            warmup_target = tf.cast(self.warmup_target, dtype)\n","            warmup_steps = tf.cast(self.warmup_steps, dtype)\n","\n","            global_step_recomp = tf.minimum(global_step_recomp, decay_steps + warmup_steps)\n","\n","            return tf.cond(\n","                global_step_recomp < warmup_steps,\n","                lambda: self._warmup_function(\n","                    global_step_recomp,\n","                    warmup_steps,\n","                    warmup_target,\n","                    initial_learning_rate,\n","                ),\n","                lambda: self._decay_function(\n","                    global_step_recomp - warmup_steps,\n","                    decay_steps,\n","                    warmup_target,\n","                    dtype,\n","                ),\n","            )\n","\n","    def get_config(self):\n","        return {\n","            \"initial_learning_rate\": self.initial_learning_rate,\n","            \"decay_steps\": self.decay_steps,\n","            \"alpha\": self.alpha,\n","            \"name\": self.name,\n","            \"warmup_target\": self.warmup_target,\n","            \"warmup_steps\": self.warmup_steps,\n","        }\n"]},{"cell_type":"markdown","metadata":{"id":"Vh1J_bDCYi13"},"source":["# Constants"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1690422412737,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"5icpG0BeYi13","trusted":true},"outputs":[],"source":["def get_char_dict():\n","    char_dict = {\n","        \" \": 0,\n","        \"!\": 1,\n","        \"#\": 2,\n","        \"$\": 3,\n","        \"%\": 4,\n","        \"&\": 5,\n","        \"'\": 6,\n","        \"(\": 7,\n","        \")\": 8,\n","        \"*\": 9,\n","        \"+\": 10,\n","        \",\": 11,\n","        \"-\": 12,\n","        \".\": 13,\n","        \"/\": 14,\n","        \"0\": 15,\n","        \"1\": 16,\n","        \"2\": 17,\n","        \"3\": 18,\n","        \"4\": 19,\n","        \"5\": 20,\n","        \"6\": 21,\n","        \"7\": 22,\n","        \"8\": 23,\n","        \"9\": 24,\n","        \":\": 25,\n","        \";\": 26,\n","        \"=\": 27,\n","        \"?\": 28,\n","        \"@\": 29,\n","        \"[\": 30,\n","        \"_\": 31,\n","        \"a\": 32,\n","        \"b\": 33,\n","        \"c\": 34,\n","        \"d\": 35,\n","        \"e\": 36,\n","        \"f\": 37,\n","        \"g\": 38,\n","        \"h\": 39,\n","        \"i\": 40,\n","        \"j\": 41,\n","        \"k\": 42,\n","        \"l\": 43,\n","        \"m\": 44,\n","        \"n\": 45,\n","        \"o\": 46,\n","        \"p\": 47,\n","        \"q\": 48,\n","        \"r\": 49,\n","        \"s\": 50,\n","        \"t\": 51,\n","        \"u\": 52,\n","        \"v\": 53,\n","        \"w\": 54,\n","        \"x\": 55,\n","        \"y\": 56,\n","        \"z\": 57,\n","        \"~\": 58,\n","    }\n","    char_dict[\"P\"] = 59\n","    #char_dict[\"SOS\"] = 60\n","    #char_dict[\"EOS\"] = 61\n","    return char_dict\n","\n","\n","class Constants:\n","    ROWS_PER_FRAME = 543\n","    MAX_STRING_LEN = 50\n","    INPUT_PAD = -100.0\n","    char_dict = get_char_dict()\n","    LABEL_PAD = char_dict[\"P\"]\n","    inv_dict = {v: k for k, v in char_dict.items()}\n","    NOSE = [1, 2, 98, 327]\n","\n","    REYE = [33, 7, 163, 144, 145, 153, 154, 155, 133, 246, 161, 160, 159, 158, 157, 173]\n","    LEYE = [263, 249, 390, 373, 374, 380, 381, 382, 362, 466, 388, 387, 386, 385, 384, 398]\n","\n","    LHAND = list(range(468, 489))\n","    RHAND = list(range(522, 543))\n","\n","    LNOSE = [98]\n","    RNOSE = [327]\n","\n","    LLIP = [84, 181, 91, 146, 61, 185, 40, 39, 37, 87, 178, 88, 95, 78, 191, 80, 81, 82]\n","    RLIP = [\n","        314,\n","        405,\n","        321,\n","        375,\n","        291,\n","        409,\n","        270,\n","        269,\n","        267,\n","        317,\n","        402,\n","        318,\n","        324,\n","        308,\n","        415,\n","        310,\n","        311,\n","        312,\n","    ]\n","    POSE = [500, 502, 504, 501, 503, 505, 512, 513]\n","    LPOSE = [513, 505, 503, 501]\n","    RPOSE = [512, 504, 502, 500]\n","\n","    POINT_LANDMARKS_PARTS = [LHAND, RHAND, LLIP, RLIP, LPOSE, RPOSE, NOSE, REYE, LEYE]\n","    # POINT_LANDMARKS_PARTS = [LHAND, RHAND, NOSE]\n","    POINT_LANDMARKS = [item for sublist in POINT_LANDMARKS_PARTS for item in sublist]\n","    parts = {\n","        \"LLIP\": LLIP,\n","        \"RLIP\": RLIP,\n","        \"LHAND\": LHAND,\n","        \"RHAND\": RHAND,\n","        \"LPOSE\": LPOSE,\n","        \"RPOSE\": RPOSE,\n","        \"LNOSE\": LNOSE,\n","        \"RNOSE\": RNOSE,\n","        \"REYE\": REYE,\n","        \"LEYE\": LEYE,\n","    }\n","\n","    LANDMARK_INDICES = {}  # type: ignore\n","    for part in parts:\n","        LANDMARK_INDICES[part] = []\n","        for landmark in parts[part]:\n","            if landmark in POINT_LANDMARKS:\n","                LANDMARK_INDICES[part].append(POINT_LANDMARKS.index(landmark))\n","\n","    CENTER_LANDMARKS = LNOSE + RNOSE\n","    CENTER_INDICES = LANDMARK_INDICES[\"LNOSE\"] + LANDMARK_INDICES[\"RNOSE\"]\n","\n","    NUM_NODES = len(POINT_LANDMARKS)\n","    NUM_INPUT_FEATURES = 2 * NUM_NODES # (x,y)\n","    CHANNELS = 6 * NUM_NODES #(x,y,dx,dy,dx2,dy2)\n"]},{"cell_type":"markdown","metadata":{"id":"MMEA9LflYi14"},"source":["# Utils"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":107,"status":"ok","timestamp":1690422412841,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"P-kprZ1_Yi14","trusted":true},"outputs":[],"source":["\n","# Seed all random number generators\n","def seed_everything(seed=42):\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    tf.random.set_seed(seed)\n","\n","\n","def selected_columns(file_example):\n","    df = pd.read_parquet(file_example)\n","    selected_x = df.columns[[x + 1 for x in Constants.POINT_LANDMARKS]].tolist()\n","    selected_y = [c.replace(\"x\", \"y\") for c in selected_x]\n","    selected = []\n","    for i in range(Constants.NUM_NODES):\n","        selected.append(selected_x[i])\n","        selected.append(selected_y[i])\n","    return selected  # x1,y1,x2,y2,...\n","\n","\n","\n","def num_to_char_fn(y):\n","    return [Constants.inv_dict.get(x, \"\") for x in y]\n","\n","\n","# A callback class to output a few transcriptions during training\n","class CallbackEval(tf.keras.callbacks.Callback):\n","    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n","\n","    def __init__(self, model, dataset):\n","        super().__init__()\n","        self.dataset = dataset\n","        self.model = model\n","\n","    def on_epoch_end(self, epoch: int, logs=None):\n","        predictions = []\n","        targets = []\n","        for batch in self.dataset:\n","            X, y = batch\n","            batch_predictions = self.model(X)\n","            batch_predictions = decode_batch_predictions(batch_predictions)\n","            predictions.extend(batch_predictions)\n","            for label in y:\n","                label = \"\".join(num_to_char_fn(label.numpy()))\n","                targets.append(label)\n","        print(\"-\" * 100)\n","        # for i in np.random.randint(0, len(predictions), 2):\n","        for i in range(10):\n","            print(f\"Target    : {targets[i]}\")\n","            print(f\"Prediction: {predictions[i]}, len: {len(predictions[i])}\")\n","            print(\"-\" * 100)\n","\n","\n","def decode_phrase(pred):\n","    # decode cts prediction by prunning\n","    # (T,CHAR_NUMS)\n","    x = tf.argmax(pred, axis=1) # (T,)\n","    paddings = tf.constant(\n","        [\n","            [0, 1],\n","        ]\n","    )\n","    x = tf.pad(x, paddings)\n","    diff = tf.not_equal(x[:-1], x[1:])\n","    adjacent_indices = tf.where(diff)[:, 0]\n","    x = tf.gather(x, adjacent_indices)\n","    mask = x != Constants.LABEL_PAD\n","    x = tf.boolean_mask(x, mask, axis=0)\n","    return x\n","\n","\n","# A utility function to decode the output of the network\n","def decode_batch_predictions(pred):\n","    output_text = []\n","    for result in pred:\n","        result = \"\".join(num_to_char_fn(decode_phrase(result).numpy()))\n","        output_text.append(result)\n","    return output_text\n","\n","\n","\n","\n","def code_to_label(label_code):\n","    label = [Constants.inv_dict[x] for x in label_code if Constants.inv_dict[x] != \"P\"]\n","    label = \"\".join(label)\n","    return label\n","\n","\n","def convert_to_strings(batch_label_code):\n","    output = []\n","    for label_code in batch_label_code:\n","        output.append(code_to_label(label_code))\n","    return output\n","\n","\n","def global_metric(val_ds, model):\n","    global_N, global_D = 0, 0\n","    count = 0\n","    metric = LevDistanceMetric()\n","    for batch in val_ds:\n","        count += 1\n","        print(count)\n","        feature, label = batch\n","        logits = model(feature)\n","        _, _, D = batch_edit_distance(label, logits)\n","        metric.update_state(label, logits)\n","\n","    result = metric.result().numpy()\n","\n","    return result\n","\n","\n","def sparse_from_dense_ignore_value(dense_tensor):\n","    mask = tf.not_equal(dense_tensor, Constants.LABEL_PAD)\n","    indices = tf.where(mask)\n","    values = tf.boolean_mask(dense_tensor, mask)\n","\n","    return tf.SparseTensor(indices, values, tf.shape(dense_tensor, out_type=tf.int64))\n","\n","\n","def batch_edit_distance(y_true, y_logits):\n","    blank = Constants.LABEL_PAD\n","    #y_true=tf.ensure_shape(y_true,(128,Constants.MAX_STRING_LEN))\n","    #y_logits=tf.ensure_shape(y_logits,(128,128,60))\n","    #tf.print(\"edit distance true shape\",tf.shape(y_true))\n","    #tf.print(\"edit distance logits shape\",tf.shape(y_logits))\n","\n","    B = tf.shape(y_logits)[0]\n","    seq_length = tf.shape(y_logits)[1]\n","    to_decode = tf.transpose(y_logits, perm=[1, 0, 2])\n","    sequence_length = tf.fill(dims=[B], value=seq_length)\n","    hypothesis = tf.nn.ctc_greedy_decoder(\n","        tf.cast(to_decode, tf.float32), sequence_length, blank_index=blank\n","    )[0][\n","        0\n","    ]  # full is [B,...]\n","\n","    truth = sparse_from_dense_ignore_value(y_true)  # full is [B,...]\n","    truth = tf.cast(truth, hypothesis.dtype)\n","    edit_dist = tf.edit_distance(hypothesis, truth, normalize=False)\n","\n","    non_ignore_mask = tf.not_equal(y_true, blank)\n","    N = tf.reduce_sum(tf.cast(non_ignore_mask, tf.float32))\n","    D = tf.reduce_sum(edit_dist)\n","    result = (N - D) / N\n","    result = tf.clip_by_value(result, 0.0, 1.0)\n","    return result, N, D\n","\n","\n","class LevDistanceMetric(tf.keras.metrics.Metric):\n","    def __init__(self, name=\"Lev\", **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        self.distance = self.add_weight(name=\"dist\", initializer=\"zeros\")\n","        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n","\n","    def update_state(self, y_true, y_logits, sample_weight=None):\n","        # if using with keras compile, make sure the model outputs logits, not softmax probabilities\n","        _, N, D = batch_edit_distance(y_true, y_logits)\n","        self.distance.assign_add(D)\n","        self.count.assign_add(N)\n","\n","    def result(self):\n","        result = (self.count - self.distance) / self.count\n","        result = tf.clip_by_value(result, 0.0, 1.0)\n","        return result\n","\n","    def reset_state(self):\n","        self.count.assign(0.0)\n","        self.distance.assign(0.0)\n","\n","\n","\n","class SWA(tf.keras.callbacks.Callback):\n","    # Stochastic Weight Averaging\n","    def __init__(\n","        self,\n","        save_name,\n","        swa_epochs=[],\n","        strategy=None,\n","        train_ds=None,\n","        valid_ds=None,\n","        train_steps=1000,\n","    ):\n","        super().__init__()\n","        self.swa_epochs = swa_epochs\n","        self.swa_weights = None\n","        self.save_name = save_name\n","        self.train_ds = train_ds\n","        self.valid_ds = valid_ds\n","        self.train_steps = train_steps\n","        self.strategy = strategy\n","\n","    def train_step(self, iterator):\n","        \"\"\"The step function for one training step.\"\"\"\n","\n","        def step_fn(inputs):\n","            \"\"\"The computation to run on each device.\"\"\"\n","            x, y = inputs\n","            _ = self.model(x, training=True)\n","\n","        for x in iterator:\n","            self.strategy.run(step_fn, args=(x,))\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        if epoch in self.swa_epochs:\n","            if self.swa_weights is None:\n","                self.swa_weights = self.model.get_weights()\n","            else:\n","                w = self.model.get_weights()\n","                for i in range(len(self.swa_weights)):\n","                    self.swa_weights[i] += w[i]\n","\n","    def on_train_end(self, logs=None):\n","        if len(self.swa_epochs):\n","            print(\"applying SWA...\")\n","            for i in range(len(self.swa_weights)):\n","                self.swa_weights[i] = self.swa_weights[i] / len(self.swa_epochs)\n","            self.model.set_weights(self.swa_weights)\n","            if self.train_ds is not None:  # for the re-calculation of running mean and var\n","                self.train_step(self.train_ds.take(self.train_steps))\n","            print(f\"save SWA weights to {self.save_name}-SWA.h5\")\n","            self.model.save_weights(f\"{self.save_name}-SWA.h5\")\n","            if self.valid_ds is not None:\n","                self.model.evaluate(self.valid_ds)\n","\n","\n","class AWP(tf.keras.Model):\n","    # Adversarial Weight Perturbation\n","    def __init__(self, *args, delta=0.1, eps=1e-4, start_step=0, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.delta = delta\n","        self.eps = eps\n","        self.start_step = start_step\n","\n","    def train_step_awp(self, data):\n","        # Unpack the data. Its structure depends on your model and\n","        # on what you pass to `fit()`.\n","        x, y = data\n","\n","        with tf.GradientTape() as tape:\n","            y_pred = self(x, training=True)\n","            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n","        params = self.trainable_variables\n","        params_gradients = tape.gradient(loss, self.trainable_variables)\n","        for i in range(len(params_gradients)):\n","            grad = tf.zeros_like(params[i]) + params_gradients[i]\n","            delta = tf.math.divide_no_nan(\n","                self.delta * grad, tf.math.sqrt(tf.reduce_sum(grad**2)) + self.eps\n","            )\n","            self.trainable_variables[i].assign_add(delta)\n","        with tf.GradientTape() as tape2:\n","            y_pred = self(x, training=True)\n","            new_loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n","            if hasattr(self.optimizer, \"get_scaled_loss\"):\n","                new_loss = self.optimizer.get_scaled_loss(new_loss)\n","\n","        gradients = tape2.gradient(new_loss, self.trainable_variables)\n","        if hasattr(self.optimizer, \"get_unscaled_gradients\"):\n","            gradients = self.optimizer.get_unscaled_gradients(gradients)\n","        for i in range(len(params_gradients)):\n","            grad = tf.zeros_like(params[i]) + params_gradients[i]\n","            delta = tf.math.divide_no_nan(\n","                self.delta * grad, tf.math.sqrt(tf.reduce_sum(grad**2)) + self.eps\n","            )\n","            self.trainable_variables[i].assign_sub(delta)\n","        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","        # self_loss.update_state(loss)\n","        self.compiled_metrics.update_state(y, y_pred)\n","        return {m.name: m.result() for m in self.metrics}\n","\n","    def train_step(self, data):\n","        return tf.cond(\n","            self._train_counter < self.start_step,\n","            lambda: super(AWP, self).train_step(data),\n","            lambda: self.train_step_awp(data),\n","        )\n"]},{"cell_type":"markdown","metadata":{"id":"VZZRnooBcj-n"},"source":["# Lev Callback"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":331,"status":"ok","timestamp":1690422413170,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"cj7O-Tcncj-n","trusted":true},"outputs":[],"source":["import Levenshtein as lev\n","import json\n","\n","rev_character_map=Constants.inv_dict\n","class val_lev_callback(tf.keras.callbacks.Callback):\n","    def __init__(self,val_set):\n","        super().__init__()\n","        self.val_set=val_set\n","    def on_epoch_end(self, epoch: int, logs=None):\n","        calculate_val_lev(self.model,self.val_set)\n","\n","def calculate_val_lev(model,val_set):\n","    preds = []\n","    targets = []\n","    for batch_idx in range(len(val_set)):\n","        preds_batch = model.predict(val_set[batch_idx][0],verbose=0)[0]\n","        targets_batch = val_set[batch_idx][1]\n","        for pred_idx in range(len(preds_batch)):\n","            preds.append(\"\".join([rev_character_map.get(s, \"\") for s in decode_phrase(preds_batch[pred_idx]).numpy()]))\n","            targets.append(\"\".join([rev_character_map.get(s, \"\") for s in targets_batch[pred_idx].numpy()]))\n","\n","    N = [len(phrase) for phrase in targets]\n","    lev_dist = [lev.distance(preds[i], targets[i]) for i in range(len(targets))]\n","    N=np.sum(N)\n","    D=np.sum(lev_dist)\n","    lev_d=(N-D)/N\n","    print(\"\")\n","    print('Lev distance: ',lev_d)"]},{"cell_type":"markdown","metadata":{"id":"YQAPz56nYi15"},"source":["# Model"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690422413170,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"wnoZyLAcYi15","trusted":true},"outputs":[],"source":["\n","class CTCLossWrap(tf.keras.losses.Loss):\n","    def __init__(self, pad_token_idx,batch_size,max_string_len,output_dim,output_steps,replicas):\n","        self.pad_token_idx = pad_token_idx\n","        self.batch_size=batch_size\n","        self.max_string_len=max_string_len\n","        self.output_steps=output_steps\n","        self.output_dim=output_dim\n","        self.replicas=replicas\n","        super().__init__()\n","\n","    def call(self, labels, logits):\n","\n","        #logits=tf.ensure_shape(logits,(self.batch_size//self.replicas,self.output_steps,self.output_dim))\n","        #labels=tf.ensure_shape(labels,(self.batch_size//self.replicas,self.max_string_len))\n","        label_length = tf.reduce_sum(tf.cast(labels != self.pad_token_idx, tf.int32), axis=-1)\n","        logit_length = tf.ones(tf.shape(logits)[0], dtype=tf.int32) * tf.shape(logits)[1]\n","\n","        #ctc_loss_fn = tf.nn.ctc_loss(\n","        ctc_loss_fn=classic_ctc_loss(\n","            labels=labels,\n","            logits=logits,\n","            label_length=label_length,\n","            logit_length=logit_length,\n","            blank_index=self.pad_token_idx,\n","            #logits_time_major=False\n","        )\n","\n","        return ctc_loss_fn\n","\n","\n","class ECA(tf.keras.layers.Layer):\n","    # Efficient Channel Attention\n","    def __init__(self, kernel_size=5, **kwargs):\n","        super().__init__(**kwargs)\n","        self.supports_masking = True\n","        self.kernel_size = kernel_size\n","        self.conv = tf.keras.layers.Conv1D(\n","            1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False\n","        )\n","\n","    def call(self, inputs, mask=None):\n","        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n","        nn = tf.expand_dims(nn, -1)\n","        nn = self.conv(nn)\n","        nn = tf.squeeze(nn, -1)\n","        nn = tf.nn.sigmoid(nn)\n","        nn = nn[:, None, :]\n","        return inputs * nn\n","\n","\n","class LateDropout(tf.keras.layers.Layer):\n","    def __init__(self, rate, noise_shape=None, start_step=0, **kwargs):\n","        super().__init__(**kwargs)\n","        self.supports_masking = True\n","        self.rate = rate\n","        self.start_step = start_step\n","        self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n","\n","    def build(self, input_shape):\n","        super().build(input_shape)\n","        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n","        self._train_counter = tf.Variable(0, dtype=\"int32\", aggregation=agg, trainable=False)\n","\n","    def call(self, inputs, training=False):\n","        x = tf.cond(\n","            self._train_counter < self.start_step,\n","            lambda: inputs,\n","            lambda: self.dropout(inputs, training=training),\n","        )\n","        if training:\n","            self._train_counter.assign_add(1)\n","        return x\n","\n","\n","class CausalDWConv1D(tf.keras.layers.Layer):\n","    # Causal Depth Wise Convolution\n","    def __init__(\n","        self,\n","        kernel_size=17,\n","        dilation_rate=1,\n","        use_bias=False,\n","        depthwise_initializer=\"glorot_uniform\",\n","        name=\"\",\n","        **kwargs,\n","    ):\n","        super().__init__(name=name, **kwargs)\n","        self.causal_pad = tf.keras.layers.ZeroPadding1D(\n","            (dilation_rate * (kernel_size - 1), 0), name=name + \"_pad\"\n","        )\n","        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n","            kernel_size,\n","            strides=1,\n","            dilation_rate=dilation_rate,\n","            padding=\"valid\",\n","            use_bias=use_bias,\n","            depthwise_initializer=depthwise_initializer,\n","            name=name + \"_dwconv\",\n","        )\n","        self.supports_masking = True\n","\n","    def call(self, inputs):\n","        x = self.causal_pad(inputs)\n","        x = self.dw_conv(x)\n","        return x\n","\n","\n","\n","def Conv1DBlock(\n","    channel_size,\n","    kernel_size,\n","    dilation_rate=1,\n","    drop_rate=0.0,\n","    expand_ratio=2,\n","    # se_ratio=0.25,\n","    activation=\"swish\",\n","    name=None,\n","):\n","    \"\"\"\n","    efficient conv1d block, @hoyso48\n","    \"\"\"\n","    if name is None:\n","        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n","\n","    # Expansion phase\n","    def apply(inputs):\n","        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n","        channels_expand = channels_in * expand_ratio\n","\n","        skip = inputs\n","\n","        x = tf.keras.layers.Dense(\n","            channels_expand, use_bias=True, activation=activation, name=name + \"_expand_conv\"\n","        )(inputs)\n","\n","        # Depthwise Convolution\n","        x = CausalDWConv1D(\n","            kernel_size, dilation_rate=dilation_rate, use_bias=False, name=name + \"_dwconv\"\n","        )(x)\n","\n","        #x = tf.keras.layers.LayerNormalization(name=name + \"_bn\")(x)\n","        x = tf.keras.layers.BatchNormalization(name=name + \"_bn\")(x)\n","\n","        x = ECA()(x)  # efficient channel attention\n","\n","        x = tf.keras.layers.Dense(channel_size, use_bias=True, name=name + \"_project_conv\")(x)\n","\n","        if drop_rate > 0:\n","            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + \"_drop\")(x)\n","\n","        if channels_in == channel_size:\n","            x = tf.keras.layers.add([x, skip], name=name + \"_add\")\n","        return x\n","\n","    return apply\n","\n","\n","class MultiHeadSelfAttention(tf.keras.layers.Layer):\n","    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n","        super().__init__(**kwargs)\n","        self.dim = dim\n","        self.scale = self.dim**-0.5\n","        self.num_heads = num_heads\n","        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n","        self.drop1 = tf.keras.layers.Dropout(dropout)\n","        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        qkv = self.qkv(inputs)\n","        qkv = tf.keras.layers.Permute((2, 1, 3))(\n","            tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv)\n","        )\n","        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n","\n","        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n","\n","        if mask is not None:\n","            mask = mask[:, None, None, :]\n","\n","        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n","        attn = self.drop1(attn)\n","\n","        x = attn @ v\n","        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n","        x = self.proj(x)\n","        return x\n","\n","\n","def TransformerBlock(\n","    dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation=\"swish\"\n","):\n","    def apply(inputs):\n","        x = inputs\n","        x = tf.keras.layers.LayerNormalization()(x)\n","        x = MultiHeadSelfAttention(dim=dim, num_heads=num_heads, dropout=attn_dropout)(x)\n","        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1))(x)\n","        x = tf.keras.layers.Add()([inputs, x])\n","        attn_out = x\n","\n","        x = tf.keras.layers.LayerNormalization()(x)\n","        x = tf.keras.layers.Dense(dim * expand, use_bias=False, activation=activation)(x)\n","        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n","        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1))(x)\n","        x = tf.keras.layers.Add()([attn_out, x])\n","        return x\n","\n","    return apply\n","\n","def build_model1(\n","    output_dim,\n","    max_len=64,\n","    dropout_step=0,\n","    dim=192,\n","    input_pad=-100,\n","    with_transformer=False,\n","    drop_rate=0.2,\n","):\n","    inp = tf.keras.Input(shape=(max_len, Constants.CHANNELS), dtype=tf.float32, name=\"inputs\")\n","    x = tf.keras.layers.Masking(mask_value=input_pad, input_shape=(max_len, Constants.CHANNELS))(\n","        inp\n","    )\n","    ksize = 17\n","    x = tf.keras.layers.Dense(dim, use_bias=False, name=\"stem_conv\")(x)\n","    #x = tf.keras.layers.LayerNormalization(name=\"stem_bn\")(x)\n","    x = tf.keras.layers.BatchNormalization(name=\"stem_bn\")(x)\n","\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    if with_transformer:\n","        x = TransformerBlock(dim, expand=2)(x)\n","\n","    #x = tf.keras.layers.AvgPool1D(2, 2)(x)\n","\n","\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    if with_transformer:\n","        x = TransformerBlock(dim, expand=2)(x)\n","\n","    x = tf.keras.layers.AvgPool1D(2, 2)(x) # [B,T,dim]\n","\n","    #lstm1 = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(units=dim//2,dtype=\"float32\"), return_sequences=True,dtype=\"float32\")\n","    lstm1=tf.keras.layers.LSTM(units=dim//2,return_sequences=True,dtype=\"float32\")\n","    x2 = tf.keras.layers.Bidirectional(lstm1)(x) #[B,T,dim]\n","\n","    x2=tf.keras.layers.BatchNormalization()(x2)\n","    x2=tf.keras.layers.Dense(output_dim)(x2)\n","    soft=tf.keras.layers.Activation('softmax', dtype='float32')(x2)\n","    logsoft=tf.keras.layers.Activation('log_softmax',dtype='float32',name=\"internal\")(x2)\n","\n","    x=tf.keras.layers.Dense(dim)(soft)+x\n","    x=tf.keras.layers.BatchNormalization()(x)\n","    if dim == 384:  # for the 4x sized model\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        if with_transformer:\n","            x = TransformerBlock(dim, expand=2)(x)\n","\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        if with_transformer:\n","            x = TransformerBlock(dim, expand=2)(x)\n","\n","\n","    #lstm2 = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(units=dim//2,dtype=\"float32\"), return_sequences=True,dtype=\"float32\")\n","    lstm2=tf.keras.layers.LSTM(units=dim//2,return_sequences=True,dtype=\"float32\")\n","    x = tf.keras.layers.Bidirectional(lstm2)(x)\n","\n","    x = LateDropout(0.6, start_step=dropout_step)(x)\n","\n","    x=tf.keras.layers.Dense(output_dim)(x)\n","    output = tf.keras.layers.Activation(\"log_softmax\",name=\"final\",dtype=\"float32\")(x)  # logits\n","\n","    model = tf.keras.Model(inp, outputs=[output,logsoft])\n","    return model\n","\n","\n","def get_model(output_dim, max_len, dim, input_pad,dropout_step=0,drop_rate=0.):\n","\n","    model = build_model1(output_dim, max_len=max_len, input_pad=input_pad, dim=dim,  dropout_step=dropout_step,drop_rate=drop_rate)\n","\n","    return model\n"]},{"cell_type":"markdown","metadata":{"id":"eMpk8n7Ccj-o"},"source":["# Configuration"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690422413171,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"DMrwH-y7cj-o","trusted":true},"outputs":[],"source":["from functools import lru_cache\n","\n","@lru_cache(maxsize=None)\n","def get_strategy():\n","    strategy=STRATEGY\n","    replicas=REPLICAS\n","    is_tpu=IS_TPU\n","\n","    return strategy, replicas, is_tpu\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":145,"status":"ok","timestamp":1690422413313,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"7e1LGyFWYi16","trusted":true},"outputs":[],"source":["\n","class CFG:\n","    # These 3 variables are update dynamically later by calling update_config_with_strategy.\n","    strategy = None  # type: ignore\n","    replicas = 1\n","    is_tpu = False\n","\n","    save_output = True\n","    records_path=\"/kaggle/input/sign-tfrecords\"\n","    input_path = \"/kaggle/input/asl-fingerspelling\"\n","    output_path = \"/kaggle/working\"\n","\n","    seed = 42\n","    verbose = 1  # 0) silent 1) progress bar 2) one line per epoch\n","\n","    # max number of frames\n","    max_len = 256\n","    replicas = 1\n","\n","    lr = 3e-4   # 5e-4\n","    weight_decay = 1e-4  # 4e-4\n","    #epochs = 300 without chicago\n","    epochs = 300 # with chicago\n","\n","    batch_size=128\n","\n","    snapshot_epochs = []  # type: ignore\n","    swa_epochs = list(range(3*(epochs//4),epochs+1))\n","\n","\n","    fp16=False\n","\n","    awp = True\n","    awp_lambda = 0.1\n","    awp_start_epoch = 15\n","    dropout_start_epoch = 15\n","    resume = 0\n","\n","    dim = 384\n","\n","    comment = f\"model-{dim}-seed{seed}\"\n","    output_dim = 60\n","    num_eval = 6\n","\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1690422413314,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"HCKh-AxTYi16","trusted":true},"outputs":[],"source":["\n","def update_config_with_strategy(config):\n","    strategy=STRATEGY\n","    replicas=REPLICAS\n","    is_tpu=IS_TPU\n","    if is_tpu:\n","      gs_bucket=\"gs://kds-ad2d88388ea5863d5107c67abb42057c7041171af54562e4721cf0e6\"\n","      config.records_path=gs_bucket\n","      config.input_path=\"/content/drive/MyDrive/kaggle-asl/asl-fingerspelling\"\n","      config.output_path = \"/content/drive/MyDrive/kaggle/working\"\n","\n","    config.strategy = strategy\n","    config.replicas = replicas\n","    config.is_tpu = is_tpu\n","    config.lr = config.lr * replicas\n","    config.batch_size = config.batch_size * replicas\n","    return config"]},{"cell_type":"markdown","metadata":{"id":"5msRCmt0Yi16"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"nrUlKKykrs3c"},"source":[]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1690422413315,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"j_tHWSs8Yi16","trusted":true},"outputs":[],"source":["\n","def count_data_items(dataset):\n","    dataset_size = 0\n","    for _ in dataset:\n","        dataset_size += 1\n","    return dataset_size\n","\n","\n","def interp1d_(x, target_len):\n","    target_len = tf.maximum(1, target_len)\n","    x = tf.image.resize(x, (target_len, tf.shape(x)[1]))\n","    return x\n","\n","\n","def tf_nan_mean(x, axis=0, keepdims=False):\n","    return tf.reduce_sum(\n","        tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims\n","    ) / tf.reduce_sum(\n","        tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims\n","    )\n","\n","\n","def tf_nan_std(x, center=None, axis=0, keepdims=False):\n","    if center is None:\n","        center = tf_nan_mean(x, axis=axis, keepdims=True)\n","    d = x - center\n","    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n","\n","\n","def flip_lr(x):\n","    if x.shape[1] == Constants.ROWS_PER_FRAME:\n","        LHAND = Constants.LHAND\n","        RHAND = Constants.RHAND\n","        LLIP = Constants.LLIP\n","        RLIP = Constants.RLIP\n","        LEYE = Constants.LEYE\n","        REYE = Constants.REYE\n","        LNOSE = Constants.LNOSE\n","        RNOSE = Constants.RNOSE\n","        LPOSE = Constants.LPOSE\n","        RPOSE = Constants.RPOSE\n","    else:\n","        LHAND = Constants.LANDMARK_INDICES[\"LHAND\"]\n","        RHAND = Constants.LANDMARK_INDICES[\"RHAND\"]\n","        LLIP = Constants.LANDMARK_INDICES[\"LLIP\"]\n","        RLIP = Constants.LANDMARK_INDICES[\"RLIP\"]\n","        LEYE = Constants.LANDMARK_INDICES[\"LEYE\"]\n","        REYE = Constants.LANDMARK_INDICES[\"REYE\"]\n","        LNOSE = Constants.LANDMARK_INDICES[\"LNOSE\"]\n","        RNOSE = Constants.LANDMARK_INDICES[\"RNOSE\"]\n","        LPOSE = Constants.LANDMARK_INDICES[\"LPOSE\"]\n","        RPOSE = Constants.LANDMARK_INDICES[\"RPOSE\"]\n","\n","    x, y = tf.unstack(x, axis=-1)\n","    x = 1 - x\n","    new_x = tf.stack([x, y], -1)\n","    new_x = tf.transpose(new_x, [1, 0, 2])\n","    lhand = tf.gather(new_x, LHAND, axis=0)\n","    rhand = tf.gather(new_x, RHAND, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LHAND)[..., None], rhand)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RHAND)[..., None], lhand)\n","    llip = tf.gather(new_x, LLIP, axis=0)\n","    rlip = tf.gather(new_x, RLIP, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LLIP)[..., None], rlip)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RLIP)[..., None], llip)\n","    lpose = tf.gather(new_x, LPOSE, axis=0)\n","    rpose = tf.gather(new_x, RPOSE, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LPOSE)[..., None], rpose)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RPOSE)[..., None], lpose)\n","    leye = tf.gather(new_x, LEYE, axis=0)\n","    reye = tf.gather(new_x, REYE, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LEYE)[..., None], reye)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(REYE)[..., None], leye)\n","    lnose = tf.gather(new_x, LNOSE, axis=0)\n","    rnose = tf.gather(new_x, RNOSE, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LNOSE)[..., None], rnose)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RNOSE)[..., None], lnose)\n","    new_x = tf.transpose(new_x, [1, 0, 2])\n","    return new_x\n","\n","\n","def resample(x, rate=(0.8, 1.2)):\n","    rate = tf.random.uniform((), rate[0], rate[1])\n","    length = tf.shape(x)[0]\n","    new_size = tf.cast(rate * tf.cast(length, tf.float32), tf.int32)\n","    new_x = interp1d_(x, new_size)\n","    return new_x\n","\n","\n","def spatial_random_affine(\n","    xyz,\n","    scale=(0.8, 1.2),\n","    shear=(-0.1, 0.1),\n","    shift=(-0.1, 0.1),\n","    degree=(-20, 20),\n","):\n","    center = tf.constant([0.5, 0.5])\n","    if degree is not None:\n","        xy = xyz[..., :2]\n","        z = xyz[..., 2:]\n","        xy -= center\n","        degree = tf.random.uniform((), *degree)\n","        radian = degree / 180 * np.pi\n","        c = tf.math.cos(radian)\n","        s = tf.math.sin(radian)\n","        rotate_mat = tf.identity(\n","            [\n","                [c, s],\n","                [-s, c],\n","            ]\n","        )\n","        xy = xy @ rotate_mat\n","        xy = xy + center\n","        xyz = tf.concat([xy, z], axis=-1)\n","\n","    if scale is not None:\n","        scale = tf.random.uniform((), *scale)\n","        xyz = scale * xyz\n","\n","    if shear is not None:\n","        xy = xyz[..., :2]\n","        z = xyz[..., 2:]\n","        shear_x = shear_y = tf.random.uniform((), *shear)\n","        if tf.random.uniform(()) < 0.5:\n","            shear_x = 0.0\n","        else:\n","            shear_y = 0.0\n","        shear_mat = tf.identity([[1.0, shear_x], [shear_y, 1.0]])\n","        xy = xy @ shear_mat\n","        xyz = tf.concat([xy, z], axis=-1)\n","\n","    if shift is not None:\n","        shift = tf.random.uniform((), *shift)\n","        xyz = xyz + shift\n","\n","    return xyz\n","\n","\n","def temporal_mask(x, size=[1, 15], mask_value=float(\"nan\")):\n","    l0 = tf.shape(x)[0]\n","    if size[1] > l0 // 8:\n","        size[1] = l0 // 8\n","        if size[1] <= 1:\n","            size[1] = 2\n","    mask_size = tf.random.uniform((), *size, dtype=tf.int32)\n","    mask_offset = tf.random.uniform((), 0, tf.clip_by_value(l0 - mask_size, 1, l0), dtype=tf.int32)\n","    x = tf.tensor_scatter_nd_update(\n","        x,\n","        tf.range(mask_offset, mask_offset + mask_size)[..., None],\n","        tf.fill([mask_size, tf.shape(x)[1], 2], mask_value),\n","    )\n","    return x\n","\n","\n","def spatial_mask(x, size=(0.05, 0.2), mask_value=float(\"nan\")):\n","    mask_offset_y = tf.random.uniform(())\n","    mask_offset_x = tf.random.uniform(())\n","    mask_size = tf.random.uniform((), *size)\n","    mask_x = (mask_offset_x < x[..., 0]) & (x[..., 0] < mask_offset_x + mask_size)\n","    mask_y = (mask_offset_y < x[..., 1]) & (x[..., 1] < mask_offset_y + mask_size)\n","    mask = mask_x & mask_y\n","    x = tf.where(mask[..., None], mask_value, x)\n","    return x\n","\n","\n","\n","def augment_fn(x):\n","    # shape (T,F)\n","    x = tf.reshape(x, (tf.shape(x)[0], -1, 2))\n","    if tf.random.uniform(()) < 0.6:\n","        x = resample(x, (0.5, 1.5))\n","    if tf.random.uniform(()) < 0.6:\n","        x = flip_lr(x)\n","    if tf.random.uniform(()) < 0.6:\n","        x = spatial_random_affine(x)\n","    if tf.random.uniform(()) < 0.4:\n","        x = temporal_mask(x)\n","    if tf.random.uniform(()) < 0.4:\n","        x = spatial_mask(x)\n","    x = tf.reshape(x, (tf.shape(x)[0], -1))\n","    return x\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":205,"status":"ok","timestamp":1690422413513,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"jYUGdwEDYi17","trusted":true},"outputs":[],"source":["\n","class Preprocess(tf.keras.layers.Layer):\n","    def __init__(self, max_len, normalize=False, **kwargs):\n","        super().__init__(**kwargs)\n","        self.max_len = max_len\n","        self.center = Constants.CENTER_INDICES\n","        self.normalize = normalize\n","\n","    # preprocess a batch of data\n","    def call(self, x):\n","        # rank is 3: [B,T,F]\n","        # if your input is just [T,F], extend its dimesnion before calling.\n","\n","        x = tf.reshape(x, (tf.shape(x)[0], tf.shape(x)[1], Constants.NUM_NODES, 2))\n","        # dimensions now are [B,T,F//2,2]\n","\n","        x_selected = x\n","        if self.normalize:\n","            mean = tf_nan_mean(tf.gather(x, self.center, axis=2), axis=[1, 2], keepdims=True)\n","            mean = tf.where(tf.math.is_nan(mean), tf.constant(0.5, x.dtype), mean)\n","            std = tf_nan_std(x_selected, center=mean, axis=[1, 2], keepdims=True)\n","            x = (x_selected - mean) / std\n","        else:\n","            x = x_selected\n","\n","        dx = tf.cond(\n","            tf.shape(x)[1] > 1,\n","            lambda: tf.pad(x[:, 1:] - x[:, :-1], [[0, 0], [0, 1], [0, 0], [0, 0]]),\n","            lambda: tf.zeros_like(x),\n","        )\n","\n","        dx2 = tf.cond(\n","            tf.shape(x)[1] > 2,\n","            lambda: tf.pad(x[:, 2:] - x[:, :-2], [[0, 0], [0, 2], [0, 0], [0, 0]]),\n","            lambda: tf.zeros_like(x),\n","        )\n","        length = tf.shape(x)[1]\n","\n","        x = tf.concat(\n","            [\n","                tf.reshape(x, (-1, length, 2 * Constants.NUM_NODES)),  # x1,y1,x2,y2,...\n","                tf.reshape(dx, (-1, length, 2 * Constants.NUM_NODES)),\n","                tf.reshape(dx2, (-1, length, 2 * Constants.NUM_NODES)),\n","            ],\n","            axis=-1,\n","        )\n","\n","        # x1,y1,x2,y2,...dx1,dy1,dx2,dy2,...\n","        x = tf.where(tf.math.is_nan(x), tf.constant(0.0, x.dtype), x)\n","        return x\n","\n","\n","def pad_if_short(x, max_len):\n","    # shape (T,F)\n","    pad_len = max_len - tf.shape(x)[0]\n","    padding = tf.ones((pad_len, tf.shape(x)[1]), dtype=x.dtype) * Constants.INPUT_PAD\n","    x = tf.concat([x, padding], axis=0)\n","    return x\n","\n","\n","def shrink_if_long(x, max_len):\n","    # shape is [T,F]\n","    if tf.shape(x)[0] > max_len:\n","        # we need to extend the dimension to [T,F,channels]  for tf.image.resize\n","        x = tf.image.resize(x[..., None], (max_len, tf.shape(x)[1]))\n","        x = tf.squeeze(x, axis=2)\n","\n","    return x\n","\n","def preprocess(x, max_len, do_pad=True):\n","    # shape (T,F)\n","    x = shrink_if_long(x, max_len=max_len)\n","    # Preprocess expects a batch, so we extend the dimension to (None,T,F), then reduce the output back to (T,F).\n","    x = tf.cast(Preprocess(max_len=max_len)(x[None, ...])[0], tf.float32)\n","\n","    if do_pad:  # we can avoid this step if there is batch padding\n","        x = pad_if_short(x, max_len=max_len)\n","        #x=tf.ensure_shape(x,(max_len,Constants.CHANNELS))\n","    else:\n","        #x=tf.ensure_shape(x,(None,Constants.CHANNELS))\n","        pass\n","    return x"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1690422413514,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"FdtpVVSbYi17","trusted":true},"outputs":[],"source":["\n","def decode_tfrec(record_bytes):\n","    features = tf.io.parse_single_example(\n","        record_bytes,\n","        {\n","            \"coordinates\": tf.io.VarLenFeature(tf.float32),\n","            \"label\": tf.io.VarLenFeature(tf.int64),\n","        },\n","    )\n","    coords = tf.sparse.to_dense(features[\"coordinates\"])\n","    coords = tf.reshape(coords, (-1, Constants.NUM_INPUT_FEATURES))\n","    label = tf.sparse.to_dense(features[\"label\"])\n","    label=tf.cast(label,dtype=tf.int32)\n","\n","    #coords=tf.ensure_shape(coords,(None,Constants.NUM_INPUT_FEATURES))\n","    #label=tf.ensure_shape(label,(None,))\n","\n","\n","    return (coords, label)\n","\n","def ensure_shapes(x,y,batch_size,max_len):\n","  x=tf.ensure_shape(x,(batch_size,max_len,Constants.CHANNELS))\n","  y=tf.ensure_shape(y,(batch_size,Constants.MAX_STRING_LEN))\n","  tf.print(\"ensure\",tf.shape(x),tf.shape(y))\n","  return x,y"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690422413515,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"ImAgehPoYi18","trusted":true},"outputs":[],"source":["\n","def get_dataset(\n","    filenames,\n","    input_path,\n","    max_len,\n","    batch_size=64,\n","    drop_remainder=False,\n","    augment=False,\n","    shuffle_buffer=None,\n","    repeat=False,\n","    use_tfrecords=True,\n","):\n","    ignore_order = tf.data.Options()\n","    ignore_order.experimental_deterministic = False\n","\n","\n","    ds = tf.data.TFRecordDataset(\n","        filenames, num_parallel_reads=tf.data.AUTOTUNE, compression_type=\"GZIP\"\n","    )\n","    ds.with_options(ignore_order)\n","    ds = ds.map(decode_tfrec, tf.data.AUTOTUNE)\n","\n","    if augment:\n","        ds = ds.map(lambda x, y: (augment_fn(x), y), tf.data.AUTOTUNE)\n","\n","    ds = ds.map(lambda x, y: (preprocess(x, max_len=max_len, do_pad=False), y), tf.data.AUTOTUNE)\n","    #if repeat:\n","    #    ds = ds.repeat()\n","\n","    if shuffle_buffer is not None:\n","        ds = ds.shuffle(shuffle_buffer)\n","\n","    ds = ds.padded_batch(\n","        batch_size,\n","        padding_values=(\n","            tf.constant(Constants.INPUT_PAD, dtype=tf.float32),\n","            tf.constant(Constants.LABEL_PAD, dtype=tf.int32),\n","        ),\n","        padded_shapes=([max_len, Constants.CHANNELS], [Constants.MAX_STRING_LEN]),\n","        drop_remainder=drop_remainder,\n","    )\n","\n","    #tf.data.experimental.assert_cardinality(len(labels) // BATCH_SIZE)\n","    ds.map(lambda x,y: ensure_shapes(x,y,batch_size,max_len),tf.data.AUTOTUNE)\n","    ds = ds.prefetch(tf.data.AUTOTUNE)\n","\n","    return ds\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":99,"status":"ok","timestamp":1690422413608,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"4HDYkwwgYi18","trusted":true},"outputs":[],"source":["\n","def train_run(train_files, valid_files, config, num_train, num_valid,experiment_id=0, use_tfrecords=True,summary=False,evaluate_only=False):\n","    #gc.collect()\n","    #tf.keras.backend.clear_session()\n","\n","\n","    if config.fp16:\n","        if config.is_tpu:\n","            policy = \"mixed_bfloat16\"\n","        else:\n","            policy = \"mixed_float16\"\n","    else:\n","        policy = \"float32\"\n","\n","\n","    tf.keras.mixed_precision.set_global_policy(policy)\n","    print(f\"\\n... TWO IMPORTANT ASPECTS OF THE GLOBAL MIXED PRECISION POLICY:\")\n","    print(f'\\t--> COMPUTE DTYPE  : {tf.keras.mixed_precision.global_policy().compute_dtype}')\n","    print(f'\\t--> VARIABLE DTYPE : {tf.keras.mixed_precision.global_policy().variable_dtype}')\n","    augment_train= True\n","    repeat_train = True\n","    if config.is_tpu:\n","      shuffle_buffer = 16384 #4096\n","    else:\n","      shuffle_buffer=4096\n","    print(\"shuffle_buffer\",shuffle_buffer)\n","    train_ds = get_dataset(\n","        train_files,\n","        input_path=config.input_path,\n","        max_len=config.max_len,\n","        batch_size=config.batch_size,\n","        drop_remainder=True,\n","        augment=augment_train,\n","        repeat=repeat_train,\n","        shuffle_buffer=shuffle_buffer,\n","        use_tfrecords=True,\n","    )\n","    if valid_files is not None:\n","        valid_ds = get_dataset(\n","            valid_files,\n","            input_path=config.input_path,\n","            max_len=config.max_len,\n","            batch_size=config.batch_size,\n","            use_tfrecords=True,\n","            drop_remainder=True\n","        )\n","    else:\n","        valid_ds = None\n","        valid_files = []\n","\n","    #valid_set_memory=[x for x in valid_ds]\n","\n","    #num_train = count_data_items(train_ds)\n","    #num_valid = count_data_items(valid_ds)\n","    #print(\"num_train batches\",num_train, \"num_valid batches\",num_valid,\"batch_size\", config.batch_size)\n","    #assert False\n","\n","    steps_per_epoch = num_train // config.batch_size\n","    dropout_step = config.dropout_start_epoch * steps_per_epoch\n","    strategy = config.strategy\n","    with strategy.scope():\n","        model = get_model(\n","            max_len=config.max_len,\n","            output_dim=config.output_dim,\n","            input_pad=Constants.INPUT_PAD,\n","            dim=config.dim,\n","            dropout_step=dropout_step,\n","            drop_rate=0.2\n","        )\n","\n","        base_lr = config.lr\n","        lr_schedule = CosineDecay(\n","            initial_learning_rate=base_lr/10,\n","            decay_steps=int(0.95 * steps_per_epoch * config.epochs),\n","            alpha=0.005,\n","            name=None,\n","            warmup_target=base_lr,\n","            warmup_steps=int(0.05 * steps_per_epoch * config.epochs),\n","        )\n","        decay_schedule = CosineDecay(\n","            initial_learning_rate=config.weight_decay*base_lr,\n","            decay_steps=int(0.95 * steps_per_epoch * config.epochs),\n","            alpha=0.005,\n","            name=None,\n","            warmup_target=base_lr,\n","            warmup_steps=int(0.05 * steps_per_epoch * config.epochs),\n","        )\n","\n","\n","        #radam=tfa.optimizers.RectifiedAdam(learning_rate=lr_schedule,weight_decay=decay_schedule)\n","        radam=tfa.optimizers.RectifiedAdam(learning_rate=lr_schedule,weight_decay=config.weight_decay)\n","        ranger = tfa.optimizers.Lookahead(radam, sync_period=6,slow_step_size=0.5)\n","        opt=ranger\n","        awp_step = config.awp_start_epoch * steps_per_epoch\n","        if config.awp:\n","            model = AWP(model.input, model.output, delta=config.awp_lambda, eps=0., start_step=awp_step)\n","            print(\"Using AWP\")\n","        else:\n","            print(\"Warning: AWP Off\")\n","\n","        ctc_loss1 = CTCLossWrap(pad_token_idx=Constants.LABEL_PAD,batch_size=config.batch_size,\n","                           max_string_len=Constants.MAX_STRING_LEN,\n","                           output_dim=config.output_dim,\n","                           output_steps=config.max_len//2,replicas=config.replicas)\n","        ctc_loss2 = CTCLossWrap(pad_token_idx=Constants.LABEL_PAD,batch_size=config.batch_size,\n","                           max_string_len=Constants.MAX_STRING_LEN,\n","                           output_dim=config.output_dim,\n","                           output_steps=config.max_len//2,replicas=config.replicas)\n","\n","        if not config.is_tpu:\n","          metrics=metrics= [LevDistanceMetric(),]\n","        else:\n","          metrics=None\n","        model.compile(\n","          optimizer=opt,\n","          loss=[ctc_loss1,ctc_loss2],\n","          loss_weights=[0.5,0.5],\n","          metrics= metrics,\n","          #steps_per_execution=16\n","        )\n","\n","\n","\n","    if summary:\n","        print()\n","        model.summary()\n","        print()\n","        print(train_ds, valid_ds)\n","        print()\n","    print(f\"---------experiment {experiment_id}---------\")\n","    print(f\"train:{num_train} \")\n","    print()\n","\n","    if evaluate_only:\n","        model.load_weights(f\"{config.output_path}/{config.comment}-exp{experiment_id}-best.h5\")\n","        cv=model.evaluate(valid_ds,verbose=config.verbose)\n","        return model,cv,None\n","\n","    if config.resume:\n","        print(f\"resume from epoch{config.resume}\")\n","        model.load_weights(f\"{config.output_path}/{config.comment}-exp{experiment_id}-last.h5\")\n","        if train_ds is not None:\n","            model.evaluate(train_ds.take(steps_per_epoch))\n","        if valid_ds is not None:\n","            model.evaluate(valid_ds)\n","\n","    tb_logger = tf.keras.callbacks.TensorBoard(\n","        log_dir=config.output_path,\n","    )\n","    sv_loss = tf.keras.callbacks.ModelCheckpoint(\n","        f\"{config.output_path}/{config.comment}-exp{experiment_id}-best.h5\",\n","        monitor=\"val_final_loss\",\n","        verbose=1,\n","        save_best_only=True,\n","        save_weights_only=True,\n","        mode=\"min\",\n","        save_freq=\"epoch\",\n","    )\n","\n","    # Callback function to check transcription on the val set.\n","    # validation_callback = CallbackEval(model, valid_ds)\n","    memory_usage = MemoryUsageCallbackExtended()\n","    swa = SWA(\n","        f\"{config.output_path}/{config.comment}-exp{experiment_id}\",\n","        config.swa_epochs,\n","        strategy=strategy,\n","        train_ds=train_ds,\n","        valid_ds=valid_ds,\n","    )\n","    #val_lev=val_lev_callback(valid_set_memory)\n","    callbacks = []\n","    if config.save_output:\n","        #callbacks.append(tb_logger)\n","        #callbacks.append(val_lev)\n","        #callbacks.append(swa)\n","        callbacks.append(sv_loss)\n","    #callbacks.append(memory_usage)\n","        callbacks.append(tf.keras.callbacks.TerminateOnNaN())\n","    # callbacks.append(validation_callback)\n","\n","    history = model.fit(\n","        train_ds,\n","        epochs=config.epochs - config.resume,\n","        #steps_per_epoch=steps_per_epoch,\n","        #validation_steps=num_valid // config.batch_size,\n","        callbacks=callbacks,\n","        validation_data=valid_ds,\n","        verbose=config.verbose,\n","    )\n","\n","    if config.save_output:  # reload the saved best weights checkpoint\n","        saved_based_model = f\"{config.output_path}/{config.comment}-exp{experiment_id}-best.h5\"\n","        if os.path.exists(saved_based_model):\n","            model.load_weights(saved_based_model)\n","        else:\n","            print(f\"Warning: could not find {saved_based_model}\")\n","    if valid_ds is not None:\n","        cv = model.evaluate(valid_ds, verbose=config.verbose)\n","    else:\n","        cv = None\n","    return model, cv, history\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690422413609,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"YpxT9BTFYi18","trusted":true},"outputs":[],"source":["\n","def train(config, experiment_id=0, use_supplemental=False,use_chicago=False,evaluate_only=False):\n","    #tf.keras.backend.clear_session()\n","    if config.strategy is None:\n","      update_config_with_strategy(config)\n","    print(f\"using {config.replicas} replicas\")\n","    print(f\"batch size {config.batch_size}\")\n","    print(f\"learning rate {config.lr}\")\n","    print(f\"fp16={config.fp16}\")\n","    seed_everything(config.seed)\n","\n","\n","    all_filenames = sorted(tf.io.gfile.glob(config.records_path+\"/*.tfrecord\"))\n","\n","    regular = [x for x in all_filenames if \"train\" in x]\n","    supp = [x for x in all_filenames if \"supp\" in x]\n","    chicago =[x for x in all_filenames if \"chicago\" in x]\n","\n","    data_filenames = regular\n","    if use_supplemental:\n","        data_filenames += supp\n","    if use_chicago:\n","        data_filenames +=chicago\n","    print(\"Using TFRECORDS\")\n","    print(\"Supplemental:\",use_supplemental)\n","    print(\"Chicago:\",use_chicago)\n","\n","    valid_files = data_filenames[: config.num_eval]  # first part in sorted list\n","    train_files = data_filenames[config.num_eval :]\n","\n","    random.shuffle(train_files) # now shuffle only the train set.\n","    #print(valid_files)\n","    #exit()\n","    #assert False\n","\n","    #df1 = pd.read_csv(config.input_path + \"/asl-fingerspelling/train.csv\")\n","    #df2 = pd.read_csv(config.input_path + \"/asl-fingerspelling/supplemental_metadata.csv\")\n","    #df_info = pd.concat([df1, df2])\n","\n","    #ds = get_dataset(train_files, CFG.input_path,max_len=CFG.max_len, augment=False, batch_size=64)\n","    #print(ds)\n","    #for x,y in ds:\n","    #    print(x,y)\n","    #assert False\n","\n","    if (not use_supplemental) and (not use_chicago):\n","        num_train = 1912 * 32  # without supplemental\n","    elif use_supplemental and (not use_chicago):\n","        num_train = 3567 * 32  # with supplemental\n","    else:\n","        num_train=5505*32\n","\n","    num_valid=187*32\n","    #train_files=train_files[:6]\n","    #num_train=6000\n","\n","    train_run(\n","        train_files,\n","        valid_files,\n","        config,\n","        num_train,\n","        num_valid,\n","        summary=False,\n","        experiment_id=experiment_id,\n","        use_tfrecords=True,\n","        evaluate_only=evaluate_only\n","    )\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":517,"status":"ok","timestamp":1690422414124,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"lfC8z_wLYi19","trusted":true},"outputs":[],"source":["gc.collect()\n","tf.keras.backend.clear_session()"]},{"cell_type":"markdown","metadata":{"id":"BVIhC2jeYi19"},"source":["# Runnn!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5UdMh9V7Yi19","outputId":"d3d05a77-175e-4eb3-856d-1006ec987d08","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["using 8 replicas\n","batch size 1024\n","learning rate 0.0024\n","fp16=False\n","Using TFRECORDS\n","Supplemental: True\n","Chicago: False\n","\n","... TWO IMPORTANT ASPECTS OF THE GLOBAL MIXED PRECISION POLICY:\n","\t--> COMPUTE DTYPE  : float32\n","\t--> VARIABLE DTYPE : float32\n","shuffle_buffer 16384\n","Using AWP\n","---------experiment 0---------\n","train:114144 \n","\n","Epoch 1/300\n","    111/Unknown - 381s 265ms/step - loss: 207.1801 - final_loss: 209.1963 - internal_loss: 205.1638\n","Epoch 1: val_final_loss improved from inf to 302.52869, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 400s 443ms/step - loss: 207.1801 - final_loss: 209.1963 - internal_loss: 205.1638 - val_loss: 279.2009 - val_final_loss: 302.5287 - val_internal_loss: 255.8731\n","Epoch 2/300\n","111/111 [==============================] - ETA: 0s - loss: 121.9012 - final_loss: 102.6554 - internal_loss: 141.1470\n","Epoch 2: val_final_loss improved from 302.52869 to 100.63841, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 41s 309ms/step - loss: 121.9012 - final_loss: 102.6554 - internal_loss: 141.1470 - val_loss: 99.6063 - val_final_loss: 100.6384 - val_internal_loss: 98.5742\n","Epoch 3/300\n","111/111 [==============================] - ETA: 0s - loss: 102.5461 - final_loss: 79.9798 - internal_loss: 125.1124\n","Epoch 3: val_final_loss improved from 100.63841 to 71.70577, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 44s 343ms/step - loss: 102.5461 - final_loss: 79.9798 - internal_loss: 125.1124 - val_loss: 76.5745 - val_final_loss: 71.7058 - val_internal_loss: 81.4433\n","Epoch 4/300\n","111/111 [==============================] - ETA: 0s - loss: 93.7562 - final_loss: 78.8274 - internal_loss: 108.6851\n","Epoch 4: val_final_loss improved from 71.70577 to 69.19064, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 40s 307ms/step - loss: 93.7562 - final_loss: 78.8274 - internal_loss: 108.6851 - val_loss: 74.0785 - val_final_loss: 69.1906 - val_internal_loss: 78.9665\n","Epoch 5/300\n","111/111 [==============================] - ETA: 0s - loss: 88.5199 - final_loss: 77.3463 - internal_loss: 99.6934\n","Epoch 5: val_final_loss did not improve from 69.19064\n","111/111 [==============================] - 39s 293ms/step - loss: 88.5199 - final_loss: 77.3463 - internal_loss: 99.6934 - val_loss: 73.7747 - val_final_loss: 69.2874 - val_internal_loss: 78.2619\n","Epoch 6/300\n","111/111 [==============================] - ETA: 0s - loss: 84.4353 - final_loss: 75.5629 - internal_loss: 93.3078\n","Epoch 6: val_final_loss improved from 69.19064 to 66.03487, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 40s 304ms/step - loss: 84.4353 - final_loss: 75.5629 - internal_loss: 93.3078 - val_loss: 76.4890 - val_final_loss: 66.0349 - val_internal_loss: 86.9431\n","Epoch 7/300\n","111/111 [==============================] - ETA: 0s - loss: 88.1749 - final_loss: 75.0273 - internal_loss: 101.3225\n","Epoch 7: val_final_loss did not improve from 66.03487\n","111/111 [==============================] - 38s 288ms/step - loss: 88.1749 - final_loss: 75.0273 - internal_loss: 101.3225 - val_loss: 138.6921 - val_final_loss: 75.0828 - val_internal_loss: 202.3013\n","Epoch 8/300\n","111/111 [==============================] - ETA: 0s - loss: 88.6948 - final_loss: 74.7610 - internal_loss: 102.6286\n","Epoch 8: val_final_loss did not improve from 66.03487\n","111/111 [==============================] - 39s 291ms/step - loss: 88.6948 - final_loss: 74.7610 - internal_loss: 102.6286 - val_loss: 82.5920 - val_final_loss: 66.7447 - val_internal_loss: 98.4394\n","Epoch 9/300\n","111/111 [==============================] - ETA: 0s - loss: 80.1053 - final_loss: 73.9362 - internal_loss: 86.2743\n","Epoch 9: val_final_loss improved from 66.03487 to 63.32805, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 40s 307ms/step - loss: 80.1053 - final_loss: 73.9362 - internal_loss: 86.2743 - val_loss: 74.6603 - val_final_loss: 63.3281 - val_internal_loss: 85.9925\n","Epoch 10/300\n","111/111 [==============================] - ETA: 0s - loss: 76.6558 - final_loss: 73.0220 - internal_loss: 80.2897\n","Epoch 10: val_final_loss did not improve from 63.32805\n","111/111 [==============================] - 39s 292ms/step - loss: 76.6558 - final_loss: 73.0220 - internal_loss: 80.2897 - val_loss: 64.6529 - val_final_loss: 64.2161 - val_internal_loss: 65.0898\n","Epoch 11/300\n","111/111 [==============================] - ETA: 0s - loss: 73.6659 - final_loss: 71.9089 - internal_loss: 75.4228\n","Epoch 11: val_final_loss did not improve from 63.32805\n","111/111 [==============================] - 41s 311ms/step - loss: 73.6659 - final_loss: 71.9089 - internal_loss: 75.4228 - val_loss: 66.4446 - val_final_loss: 65.0830 - val_internal_loss: 67.8062\n","Epoch 12/300\n","111/111 [==============================] - ETA: 0s - loss: 68.6351 - final_loss: 69.9025 - internal_loss: 67.3677\n","Epoch 12: val_final_loss improved from 63.32805 to 59.18240, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 40s 306ms/step - loss: 68.6351 - final_loss: 69.9025 - internal_loss: 67.3677 - val_loss: 57.9175 - val_final_loss: 59.1824 - val_internal_loss: 56.6525\n","Epoch 13/300\n","111/111 [==============================] - ETA: 0s - loss: 59.9726 - final_loss: 62.3525 - internal_loss: 57.5926\n","Epoch 13: val_final_loss improved from 59.18240 to 54.24365, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 40s 309ms/step - loss: 59.9726 - final_loss: 62.3525 - internal_loss: 57.5926 - val_loss: 53.8821 - val_final_loss: 54.2437 - val_internal_loss: 53.5205\n","Epoch 14/300\n","111/111 [==============================] - ETA: 0s - loss: 49.1916 - final_loss: 48.9231 - internal_loss: 49.4602\n","Epoch 14: val_final_loss improved from 54.24365 to 36.76864, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 41s 311ms/step - loss: 49.1916 - final_loss: 48.9231 - internal_loss: 49.4602 - val_loss: 37.2752 - val_final_loss: 36.7686 - val_internal_loss: 37.7817\n","Epoch 15/300\n","111/111 [==============================] - ETA: 0s - loss: 44.1743 - final_loss: 43.1171 - internal_loss: 45.2315\n","Epoch 15: val_final_loss improved from 36.76864 to 33.61541, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 41s 313ms/step - loss: 44.1743 - final_loss: 43.1171 - internal_loss: 45.2315 - val_loss: 34.4713 - val_final_loss: 33.6154 - val_internal_loss: 35.3272\n","Epoch 16/300\n","111/111 [==============================] - ETA: 0s - loss: 85.7696 - final_loss: 85.4130 - internal_loss: 86.1262\n","Epoch 16: val_final_loss did not improve from 33.61541\n","111/111 [==============================] - 62s 495ms/step - loss: 85.7696 - final_loss: 85.4130 - internal_loss: 86.1262 - val_loss: 67.4139 - val_final_loss: 66.9857 - val_internal_loss: 67.8420\n","Epoch 17/300\n","111/111 [==============================] - ETA: 0s - loss: 73.5423 - final_loss: 79.3876 - internal_loss: 67.6970\n","Epoch 17: val_final_loss did not improve from 33.61541\n","111/111 [==============================] - 61s 496ms/step - loss: 73.5423 - final_loss: 79.3875 - internal_loss: 67.6970 - val_loss: 55.1742 - val_final_loss: 63.2186 - val_internal_loss: 47.1298\n","Epoch 18/300\n","111/111 [==============================] - ETA: 0s - loss: 65.7200 - final_loss: 76.9598 - internal_loss: 54.4801\n","Epoch 18: val_final_loss did not improve from 33.61541\n","111/111 [==============================] - 61s 495ms/step - loss: 65.7200 - final_loss: 76.9598 - internal_loss: 54.4801 - val_loss: 50.6745 - val_final_loss: 62.4307 - val_internal_loss: 38.9184\n","Epoch 19/300\n","111/111 [==============================] - ETA: 0s - loss: 62.3496 - final_loss: 74.9274 - internal_loss: 49.7718\n","Epoch 19: val_final_loss did not improve from 33.61541\n","111/111 [==============================] - 61s 494ms/step - loss: 62.3496 - final_loss: 74.9274 - internal_loss: 49.7718 - val_loss: 46.9051 - val_final_loss: 59.5438 - val_internal_loss: 34.2665\n","Epoch 20/300\n","111/111 [==============================] - ETA: 0s - loss: 60.5461 - final_loss: 73.4814 - internal_loss: 47.6108\n","Epoch 20: val_final_loss did not improve from 33.61541\n","111/111 [==============================] - 62s 498ms/step - loss: 60.5461 - final_loss: 73.4814 - internal_loss: 47.6108 - val_loss: 44.3158 - val_final_loss: 55.9614 - val_internal_loss: 32.6703\n","Epoch 21/300\n","111/111 [==============================] - ETA: 0s - loss: 50.9803 - final_loss: 56.0030 - internal_loss: 45.9576\n","Epoch 21: val_final_loss did not improve from 33.61541\n","111/111 [==============================] - 65s 530ms/step - loss: 50.9803 - final_loss: 56.0030 - internal_loss: 45.9576 - val_loss: 33.1453 - val_final_loss: 34.1566 - val_internal_loss: 32.1340\n","Epoch 22/300\n","111/111 [==============================] - ETA: 0s - loss: 45.5044 - final_loss: 46.7245 - internal_loss: 44.2843\n","Epoch 22: val_final_loss improved from 33.61541 to 30.15009, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 513ms/step - loss: 45.5044 - final_loss: 46.7245 - internal_loss: 44.2843 - val_loss: 30.0770 - val_final_loss: 30.1501 - val_internal_loss: 30.0039\n","Epoch 23/300\n","111/111 [==============================] - ETA: 0s - loss: 43.3066 - final_loss: 43.8576 - internal_loss: 42.7556\n","Epoch 23: val_final_loss improved from 30.15009 to 29.38880, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 64s 518ms/step - loss: 43.3066 - final_loss: 43.8576 - internal_loss: 42.7556 - val_loss: 29.5837 - val_final_loss: 29.3888 - val_internal_loss: 29.7787\n","Epoch 24/300\n","111/111 [==============================] - ETA: 0s - loss: 41.9223 - final_loss: 42.2037 - internal_loss: 41.6409\n","Epoch 24: val_final_loss improved from 29.38880 to 27.77993, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 514ms/step - loss: 41.9223 - final_loss: 42.2037 - internal_loss: 41.6409 - val_loss: 28.1711 - val_final_loss: 27.7799 - val_internal_loss: 28.5622\n","Epoch 25/300\n","111/111 [==============================] - ETA: 0s - loss: 40.7547 - final_loss: 40.8155 - internal_loss: 40.6940\n","Epoch 25: val_final_loss improved from 27.77993 to 27.24326, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 509ms/step - loss: 40.7547 - final_loss: 40.8155 - internal_loss: 40.6940 - val_loss: 27.6369 - val_final_loss: 27.2433 - val_internal_loss: 28.0306\n","Epoch 26/300\n","111/111 [==============================] - ETA: 0s - loss: 39.7588 - final_loss: 39.6934 - internal_loss: 39.8241\n","Epoch 26: val_final_loss improved from 27.24326 to 26.34171, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 513ms/step - loss: 39.7588 - final_loss: 39.6934 - internal_loss: 39.8241 - val_loss: 26.8994 - val_final_loss: 26.3417 - val_internal_loss: 27.4572\n","Epoch 27/300\n","111/111 [==============================] - ETA: 0s - loss: 38.9199 - final_loss: 38.7239 - internal_loss: 39.1158\n","Epoch 27: val_final_loss improved from 26.34171 to 26.10168, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 514ms/step - loss: 38.9199 - final_loss: 38.7239 - internal_loss: 39.1158 - val_loss: 26.7048 - val_final_loss: 26.1017 - val_internal_loss: 27.3078\n","Epoch 28/300\n","111/111 [==============================] - ETA: 0s - loss: 38.1048 - final_loss: 37.8204 - internal_loss: 38.3892\n","Epoch 28: val_final_loss improved from 26.10168 to 24.91094, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 509ms/step - loss: 38.1048 - final_loss: 37.8204 - internal_loss: 38.3892 - val_loss: 25.5302 - val_final_loss: 24.9109 - val_internal_loss: 26.1495\n","Epoch 29/300\n","111/111 [==============================] - ETA: 0s - loss: 37.3608 - final_loss: 37.0201 - internal_loss: 37.7016\n","Epoch 29: val_final_loss did not improve from 24.91094\n","111/111 [==============================] - 61s 494ms/step - loss: 37.3608 - final_loss: 37.0201 - internal_loss: 37.7016 - val_loss: 26.4062 - val_final_loss: 25.6954 - val_internal_loss: 27.1170\n","Epoch 30/300\n","111/111 [==============================] - ETA: 0s - loss: 36.8275 - final_loss: 36.4367 - internal_loss: 37.2182\n","Epoch 30: val_final_loss improved from 24.91094 to 24.04078, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 513ms/step - loss: 36.8275 - final_loss: 36.4367 - internal_loss: 37.2182 - val_loss: 24.7425 - val_final_loss: 24.0408 - val_internal_loss: 25.4442\n","Epoch 31/300\n","111/111 [==============================] - ETA: 0s - loss: 36.1796 - final_loss: 35.7392 - internal_loss: 36.6199\n","Epoch 31: val_final_loss improved from 24.04078 to 24.02923, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 64s 516ms/step - loss: 36.1796 - final_loss: 35.7392 - internal_loss: 36.6199 - val_loss: 24.8361 - val_final_loss: 24.0292 - val_internal_loss: 25.6429\n","Epoch 32/300\n","111/111 [==============================] - ETA: 0s - loss: 35.6356 - final_loss: 35.1267 - internal_loss: 36.1445\n","Epoch 32: val_final_loss improved from 24.02923 to 23.18792, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 509ms/step - loss: 35.6356 - final_loss: 35.1267 - internal_loss: 36.1445 - val_loss: 23.9705 - val_final_loss: 23.1879 - val_internal_loss: 24.7530\n","Epoch 33/300\n","111/111 [==============================] - ETA: 0s - loss: 35.0903 - final_loss: 34.5724 - internal_loss: 35.6082\n","Epoch 33: val_final_loss did not improve from 23.18792\n","111/111 [==============================] - 61s 494ms/step - loss: 35.0903 - final_loss: 34.5724 - internal_loss: 35.6082 - val_loss: 24.0425 - val_final_loss: 23.2875 - val_internal_loss: 24.7975\n","Epoch 34/300\n","111/111 [==============================] - ETA: 0s - loss: 34.5923 - final_loss: 34.0360 - internal_loss: 35.1487\n","Epoch 34: val_final_loss improved from 23.18792 to 22.40647, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 514ms/step - loss: 34.5923 - final_loss: 34.0360 - internal_loss: 35.1487 - val_loss: 23.2189 - val_final_loss: 22.4065 - val_internal_loss: 24.0313\n","Epoch 35/300\n","111/111 [==============================] - ETA: 0s - loss: 34.1558 - final_loss: 33.5709 - internal_loss: 34.7408\n","Epoch 35: val_final_loss did not improve from 22.40647\n","111/111 [==============================] - 61s 493ms/step - loss: 34.1558 - final_loss: 33.5709 - internal_loss: 34.7408 - val_loss: 23.9199 - val_final_loss: 22.9572 - val_internal_loss: 24.8826\n","Epoch 36/300\n","111/111 [==============================] - ETA: 0s - loss: 33.7771 - final_loss: 33.1500 - internal_loss: 34.4043\n","Epoch 36: val_final_loss improved from 22.40647 to 21.88062, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 509ms/step - loss: 33.7771 - final_loss: 33.1500 - internal_loss: 34.4043 - val_loss: 22.7251 - val_final_loss: 21.8806 - val_internal_loss: 23.5695\n","Epoch 37/300\n","111/111 [==============================] - ETA: 0s - loss: 37.3359 - final_loss: 36.4454 - internal_loss: 38.2263\n","Epoch 37: val_final_loss did not improve from 21.88062\n","111/111 [==============================] - 61s 493ms/step - loss: 37.3359 - final_loss: 36.4454 - internal_loss: 38.2263 - val_loss: 25.0133 - val_final_loss: 24.0562 - val_internal_loss: 25.9704\n","Epoch 38/300\n","111/111 [==============================] - ETA: 0s - loss: 34.3812 - final_loss: 33.6076 - internal_loss: 35.1548\n","Epoch 38: val_final_loss did not improve from 21.88062\n","111/111 [==============================] - 61s 495ms/step - loss: 34.3812 - final_loss: 33.6076 - internal_loss: 35.1548 - val_loss: 23.0475 - val_final_loss: 22.1832 - val_internal_loss: 23.9118\n","Epoch 39/300\n","111/111 [==============================] - ETA: 0s - loss: 33.4263 - final_loss: 32.6439 - internal_loss: 34.2087\n","Epoch 39: val_final_loss improved from 21.88062 to 21.83067, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 516ms/step - loss: 33.4263 - final_loss: 32.6439 - internal_loss: 34.2087 - val_loss: 22.6925 - val_final_loss: 21.8307 - val_internal_loss: 23.5544\n","Epoch 40/300\n","111/111 [==============================] - ETA: 0s - loss: 32.8373 - final_loss: 32.0547 - internal_loss: 33.6199\n","Epoch 40: val_final_loss improved from 21.83067 to 21.08129, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 512ms/step - loss: 32.8373 - final_loss: 32.0547 - internal_loss: 33.6199 - val_loss: 21.9831 - val_final_loss: 21.0813 - val_internal_loss: 22.8849\n","Epoch 41/300\n","111/111 [==============================] - ETA: 0s - loss: 32.3266 - final_loss: 31.5364 - internal_loss: 33.1168\n","Epoch 41: val_final_loss did not improve from 21.08129\n","111/111 [==============================] - 61s 495ms/step - loss: 32.3266 - final_loss: 31.5364 - internal_loss: 33.1168 - val_loss: 22.5468 - val_final_loss: 21.7319 - val_internal_loss: 23.3616\n","Epoch 42/300\n","111/111 [==============================] - ETA: 0s - loss: 32.0148 - final_loss: 31.1826 - internal_loss: 32.8471\n","Epoch 42: val_final_loss improved from 21.08129 to 20.86765, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 64s 515ms/step - loss: 32.0148 - final_loss: 31.1826 - internal_loss: 32.8471 - val_loss: 21.7798 - val_final_loss: 20.8677 - val_internal_loss: 22.6919\n","Epoch 43/300\n","111/111 [==============================] - ETA: 0s - loss: 31.5763 - final_loss: 30.7280 - internal_loss: 32.4247\n","Epoch 43: val_final_loss did not improve from 20.86765\n","111/111 [==============================] - 65s 524ms/step - loss: 31.5763 - final_loss: 30.7280 - internal_loss: 32.4247 - val_loss: 22.0679 - val_final_loss: 21.1059 - val_internal_loss: 23.0299\n","Epoch 44/300\n","111/111 [==============================] - ETA: 0s - loss: 31.2623 - final_loss: 30.3818 - internal_loss: 32.1427\n","Epoch 44: val_final_loss improved from 20.86765 to 20.29587, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 511ms/step - loss: 31.2623 - final_loss: 30.3818 - internal_loss: 32.1427 - val_loss: 21.1509 - val_final_loss: 20.2959 - val_internal_loss: 22.0059\n","Epoch 45/300\n","111/111 [==============================] - ETA: 0s - loss: 31.0012 - final_loss: 30.1378 - internal_loss: 31.8646\n","Epoch 45: val_final_loss did not improve from 20.29587\n","111/111 [==============================] - 61s 495ms/step - loss: 31.0012 - final_loss: 30.1378 - internal_loss: 31.8646 - val_loss: 21.9056 - val_final_loss: 20.9208 - val_internal_loss: 22.8904\n","Epoch 46/300\n","111/111 [==============================] - ETA: 0s - loss: 30.6642 - final_loss: 29.7251 - internal_loss: 31.6033\n","Epoch 46: val_final_loss improved from 20.29587 to 19.83657, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 515ms/step - loss: 30.6642 - final_loss: 29.7251 - internal_loss: 31.6033 - val_loss: 20.7211 - val_final_loss: 19.8366 - val_internal_loss: 21.6057\n","Epoch 47/300\n","111/111 [==============================] - ETA: 0s - loss: 30.3740 - final_loss: 29.4282 - internal_loss: 31.3199\n","Epoch 47: val_final_loss did not improve from 19.83657\n","111/111 [==============================] - 61s 495ms/step - loss: 30.3740 - final_loss: 29.4282 - internal_loss: 31.3199 - val_loss: 21.2613 - val_final_loss: 20.3393 - val_internal_loss: 22.1833\n","Epoch 48/300\n","111/111 [==============================] - ETA: 0s - loss: 30.0902 - final_loss: 29.1325 - internal_loss: 31.0478\n","Epoch 48: val_final_loss improved from 19.83657 to 19.53671, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 511ms/step - loss: 30.0902 - final_loss: 29.1325 - internal_loss: 31.0478 - val_loss: 20.4191 - val_final_loss: 19.5367 - val_internal_loss: 21.3014\n","Epoch 49/300\n","111/111 [==============================] - ETA: 0s - loss: 29.8549 - final_loss: 28.8660 - internal_loss: 30.8438\n","Epoch 49: val_final_loss did not improve from 19.53671\n","111/111 [==============================] - 61s 495ms/step - loss: 29.8549 - final_loss: 28.8660 - internal_loss: 30.8438 - val_loss: 21.1282 - val_final_loss: 20.1176 - val_internal_loss: 22.1387\n","Epoch 50/300\n","111/111 [==============================] - ETA: 0s - loss: 29.6310 - final_loss: 28.6103 - internal_loss: 30.6518\n","Epoch 50: val_final_loss improved from 19.53671 to 19.15991, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 513ms/step - loss: 29.6310 - final_loss: 28.6103 - internal_loss: 30.6518 - val_loss: 20.0686 - val_final_loss: 19.1599 - val_internal_loss: 20.9772\n","Epoch 51/300\n","111/111 [==============================] - ETA: 0s - loss: 29.3369 - final_loss: 28.3096 - internal_loss: 30.3642\n","Epoch 51: val_final_loss did not improve from 19.15991\n","111/111 [==============================] - 62s 497ms/step - loss: 29.3369 - final_loss: 28.3096 - internal_loss: 30.3642 - val_loss: 21.1585 - val_final_loss: 20.1410 - val_internal_loss: 22.1760\n","Epoch 52/300\n","111/111 [==============================] - ETA: 0s - loss: 29.1227 - final_loss: 28.0651 - internal_loss: 30.1803\n","Epoch 52: val_final_loss improved from 19.15991 to 19.06124, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 510ms/step - loss: 29.1227 - final_loss: 28.0651 - internal_loss: 30.1803 - val_loss: 19.9793 - val_final_loss: 19.0612 - val_internal_loss: 20.8973\n","Epoch 53/300\n","111/111 [==============================] - ETA: 0s - loss: 28.8722 - final_loss: 27.7914 - internal_loss: 29.9530\n","Epoch 53: val_final_loss did not improve from 19.06124\n","111/111 [==============================] - 61s 495ms/step - loss: 28.8722 - final_loss: 27.7914 - internal_loss: 29.9530 - val_loss: 20.4940 - val_final_loss: 19.5570 - val_internal_loss: 21.4311\n","Epoch 54/300\n","111/111 [==============================] - ETA: 0s - loss: 28.6703 - final_loss: 27.5672 - internal_loss: 29.7734\n","Epoch 54: val_final_loss improved from 19.06124 to 18.60744, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 64s 516ms/step - loss: 28.6703 - final_loss: 27.5672 - internal_loss: 29.7734 - val_loss: 19.5559 - val_final_loss: 18.6074 - val_internal_loss: 20.5043\n","Epoch 55/300\n","111/111 [==============================] - ETA: 0s - loss: 28.4625 - final_loss: 27.3379 - internal_loss: 29.5870\n","Epoch 55: val_final_loss did not improve from 18.60744\n","111/111 [==============================] - 61s 495ms/step - loss: 28.4625 - final_loss: 27.3379 - internal_loss: 29.5870 - val_loss: 20.4448 - val_final_loss: 19.4482 - val_internal_loss: 21.4414\n","Epoch 56/300\n","111/111 [==============================] - ETA: 0s - loss: 28.2965 - final_loss: 27.1556 - internal_loss: 29.4374\n","Epoch 56: val_final_loss improved from 18.60744 to 18.42546, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 510ms/step - loss: 28.2965 - final_loss: 27.1556 - internal_loss: 29.4374 - val_loss: 19.3582 - val_final_loss: 18.4255 - val_internal_loss: 20.2910\n","Epoch 57/300\n","111/111 [==============================] - ETA: 0s - loss: 28.0427 - final_loss: 26.8947 - internal_loss: 29.1907\n","Epoch 57: val_final_loss did not improve from 18.42546\n","111/111 [==============================] - 61s 495ms/step - loss: 28.0427 - final_loss: 26.8947 - internal_loss: 29.1907 - val_loss: 20.4355 - val_final_loss: 19.3655 - val_internal_loss: 21.5054\n","Epoch 58/300\n","111/111 [==============================] - ETA: 0s - loss: 27.8394 - final_loss: 26.6621 - internal_loss: 29.0167\n","Epoch 58: val_final_loss improved from 18.42546 to 18.27622, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 64s 516ms/step - loss: 27.8394 - final_loss: 26.6621 - internal_loss: 29.0167 - val_loss: 19.1899 - val_final_loss: 18.2762 - val_internal_loss: 20.1036\n","Epoch 59/300\n","111/111 [==============================] - ETA: 0s - loss: 27.6952 - final_loss: 26.4786 - internal_loss: 28.9118\n","Epoch 59: val_final_loss did not improve from 18.27622\n","111/111 [==============================] - 61s 497ms/step - loss: 27.6952 - final_loss: 26.4786 - internal_loss: 28.9118 - val_loss: 20.3901 - val_final_loss: 19.3615 - val_internal_loss: 21.4187\n","Epoch 60/300\n","111/111 [==============================] - ETA: 0s - loss: 27.5420 - final_loss: 26.3368 - internal_loss: 28.7471\n","Epoch 60: val_final_loss did not improve from 18.27622\n","111/111 [==============================] - 61s 494ms/step - loss: 27.5420 - final_loss: 26.3368 - internal_loss: 28.7471 - val_loss: 19.2543 - val_final_loss: 18.3210 - val_internal_loss: 20.1876\n","Epoch 61/300\n","111/111 [==============================] - ETA: 0s - loss: 27.3404 - final_loss: 26.1057 - internal_loss: 28.5751\n","Epoch 61: val_final_loss did not improve from 18.27622\n","111/111 [==============================] - 61s 496ms/step - loss: 27.3404 - final_loss: 26.1057 - internal_loss: 28.5751 - val_loss: 19.6723 - val_final_loss: 18.6960 - val_internal_loss: 20.6485\n","Epoch 62/300\n","111/111 [==============================] - ETA: 0s - loss: 27.1477 - final_loss: 25.8598 - internal_loss: 28.4356\n","Epoch 62: val_final_loss improved from 18.27622 to 17.94763, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 61s 511ms/step - loss: 27.1477 - final_loss: 25.8598 - internal_loss: 28.4356 - val_loss: 18.8552 - val_final_loss: 17.9476 - val_internal_loss: 19.7628\n","Epoch 63/300\n","111/111 [==============================] - ETA: 0s - loss: 27.0375 - final_loss: 25.7616 - internal_loss: 28.3134\n","Epoch 63: val_final_loss did not improve from 17.94763\n","111/111 [==============================] - 62s 501ms/step - loss: 27.0375 - final_loss: 25.7616 - internal_loss: 28.3134 - val_loss: 19.6808 - val_final_loss: 18.6750 - val_internal_loss: 20.6865\n","Epoch 64/300\n","111/111 [==============================] - ETA: 0s - loss: 26.8188 - final_loss: 25.5219 - internal_loss: 28.1158\n","Epoch 64: val_final_loss improved from 17.94763 to 17.91782, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 514ms/step - loss: 26.8188 - final_loss: 25.5219 - internal_loss: 28.1158 - val_loss: 18.8073 - val_final_loss: 17.9178 - val_internal_loss: 19.6967\n","Epoch 65/300\n","111/111 [==============================] - ETA: 0s - loss: 26.6550 - final_loss: 25.3644 - internal_loss: 27.9456\n","Epoch 65: val_final_loss did not improve from 17.91782\n","111/111 [==============================] - 61s 495ms/step - loss: 26.6550 - final_loss: 25.3644 - internal_loss: 27.9456 - val_loss: 19.8399 - val_final_loss: 18.8996 - val_internal_loss: 20.7802\n","Epoch 66/300\n","111/111 [==============================] - ETA: 0s - loss: 26.4553 - final_loss: 25.1359 - internal_loss: 27.7747\n","Epoch 66: val_final_loss improved from 17.91782 to 17.61784, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 511ms/step - loss: 26.4553 - final_loss: 25.1359 - internal_loss: 27.7747 - val_loss: 18.5178 - val_final_loss: 17.6178 - val_internal_loss: 19.4178\n","Epoch 67/300\n","111/111 [==============================] - ETA: 0s - loss: 26.3034 - final_loss: 24.9689 - internal_loss: 27.6380\n","Epoch 67: val_final_loss did not improve from 17.61784\n","111/111 [==============================] - 62s 501ms/step - loss: 26.3034 - final_loss: 24.9689 - internal_loss: 27.6380 - val_loss: 19.1958 - val_final_loss: 18.1934 - val_internal_loss: 20.1981\n","Epoch 68/300\n","111/111 [==============================] - ETA: 0s - loss: 26.2081 - final_loss: 24.8373 - internal_loss: 27.5788\n","Epoch 68: val_final_loss improved from 17.61784 to 17.57793, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 62s 514ms/step - loss: 26.2081 - final_loss: 24.8373 - internal_loss: 27.5788 - val_loss: 18.4966 - val_final_loss: 17.5779 - val_internal_loss: 19.4153\n","Epoch 69/300\n","111/111 [==============================] - ETA: 0s - loss: 26.0173 - final_loss: 24.6456 - internal_loss: 27.3889\n","Epoch 69: val_final_loss did not improve from 17.57793\n","111/111 [==============================] - 61s 498ms/step - loss: 26.0173 - final_loss: 24.6456 - internal_loss: 27.3889 - val_loss: 19.2455 - val_final_loss: 18.2595 - val_internal_loss: 20.2316\n","Epoch 70/300\n","111/111 [==============================] - ETA: 0s - loss: 25.9142 - final_loss: 24.4963 - internal_loss: 27.3322\n","Epoch 70: val_final_loss improved from 17.57793 to 17.42191, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 513ms/step - loss: 25.9142 - final_loss: 24.4963 - internal_loss: 27.3322 - val_loss: 18.2893 - val_final_loss: 17.4219 - val_internal_loss: 19.1566\n","Epoch 71/300\n","111/111 [==============================] - ETA: 0s - loss: 25.7334 - final_loss: 24.3084 - internal_loss: 27.1584\n","Epoch 71: val_final_loss did not improve from 17.42191\n","111/111 [==============================] - 60s 497ms/step - loss: 25.7334 - final_loss: 24.3084 - internal_loss: 27.1584 - val_loss: 18.9858 - val_final_loss: 17.9957 - val_internal_loss: 19.9759\n","Epoch 72/300\n","111/111 [==============================] - ETA: 0s - loss: 25.6220 - final_loss: 24.1629 - internal_loss: 27.0811\n","Epoch 72: val_final_loss improved from 17.42191 to 17.13612, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 516ms/step - loss: 25.6220 - final_loss: 24.1629 - internal_loss: 27.0811 - val_loss: 18.0192 - val_final_loss: 17.1361 - val_internal_loss: 18.9022\n","Epoch 73/300\n","111/111 [==============================] - ETA: 0s - loss: 25.4267 - final_loss: 23.9670 - internal_loss: 26.8864\n","Epoch 73: val_final_loss did not improve from 17.13612\n","111/111 [==============================] - 61s 496ms/step - loss: 25.4267 - final_loss: 23.9670 - internal_loss: 26.8864 - val_loss: 18.6748 - val_final_loss: 17.7164 - val_internal_loss: 19.6332\n","Epoch 74/300\n","111/111 [==============================] - ETA: 0s - loss: 25.3349 - final_loss: 23.8729 - internal_loss: 26.7969\n","Epoch 74: val_final_loss improved from 17.13612 to 17.10165, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 512ms/step - loss: 25.3349 - final_loss: 23.8729 - internal_loss: 26.7969 - val_loss: 18.0050 - val_final_loss: 17.1016 - val_internal_loss: 18.9083\n","Epoch 75/300\n","111/111 [==============================] - ETA: 0s - loss: 25.2479 - final_loss: 23.7707 - internal_loss: 26.7251\n","Epoch 75: val_final_loss did not improve from 17.10165\n","111/111 [==============================] - 62s 496ms/step - loss: 25.2479 - final_loss: 23.7707 - internal_loss: 26.7251 - val_loss: 18.9132 - val_final_loss: 17.9996 - val_internal_loss: 19.8268\n","Epoch 76/300\n","111/111 [==============================] - ETA: 0s - loss: 25.0713 - final_loss: 23.5697 - internal_loss: 26.5729\n","Epoch 76: val_final_loss improved from 17.10165 to 16.96911, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 514ms/step - loss: 25.0713 - final_loss: 23.5697 - internal_loss: 26.5729 - val_loss: 17.8548 - val_final_loss: 16.9691 - val_internal_loss: 18.7405\n","Epoch 77/300\n","111/111 [==============================] - ETA: 0s - loss: 24.9004 - final_loss: 23.3865 - internal_loss: 26.4144\n","Epoch 77: val_final_loss did not improve from 16.96911\n","111/111 [==============================] - 61s 497ms/step - loss: 24.9004 - final_loss: 23.3865 - internal_loss: 26.4144 - val_loss: 18.4974 - val_final_loss: 17.5919 - val_internal_loss: 19.4030\n","Epoch 78/300\n","111/111 [==============================] - ETA: 0s - loss: 24.9129 - final_loss: 23.3731 - internal_loss: 26.4528\n","Epoch 78: val_final_loss did not improve from 16.96911\n","111/111 [==============================] - 61s 497ms/step - loss: 24.9129 - final_loss: 23.3731 - internal_loss: 26.4528 - val_loss: 17.9856 - val_final_loss: 17.0483 - val_internal_loss: 18.9230\n","Epoch 79/300\n","111/111 [==============================] - ETA: 0s - loss: 24.7117 - final_loss: 23.1615 - internal_loss: 26.2618\n","Epoch 79: val_final_loss did not improve from 16.96911\n","111/111 [==============================] - 62s 497ms/step - loss: 24.7117 - final_loss: 23.1615 - internal_loss: 26.2618 - val_loss: 19.0589 - val_final_loss: 18.0832 - val_internal_loss: 20.0347\n","Epoch 80/300\n","111/111 [==============================] - ETA: 0s - loss: 24.5836 - final_loss: 23.0143 - internal_loss: 26.1528\n","Epoch 80: val_final_loss improved from 16.96911 to 16.77201, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 514ms/step - loss: 24.5836 - final_loss: 23.0143 - internal_loss: 26.1528 - val_loss: 17.6399 - val_final_loss: 16.7720 - val_internal_loss: 18.5077\n","Epoch 81/300\n","111/111 [==============================] - ETA: 0s - loss: 24.4640 - final_loss: 22.8754 - internal_loss: 26.0527\n","Epoch 81: val_final_loss did not improve from 16.77201\n","111/111 [==============================] - 62s 500ms/step - loss: 24.4640 - final_loss: 22.8754 - internal_loss: 26.0527 - val_loss: 18.4909 - val_final_loss: 17.5645 - val_internal_loss: 19.4172\n","Epoch 82/300\n","111/111 [==============================] - ETA: 0s - loss: 24.3788 - final_loss: 22.8021 - internal_loss: 25.9555\n","Epoch 82: val_final_loss improved from 16.77201 to 16.71303, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 510ms/step - loss: 24.3788 - final_loss: 22.8021 - internal_loss: 25.9555 - val_loss: 17.6126 - val_final_loss: 16.7130 - val_internal_loss: 18.5122\n","Epoch 83/300\n","111/111 [==============================] - ETA: 0s - loss: 24.2440 - final_loss: 22.6373 - internal_loss: 25.8507\n","Epoch 83: val_final_loss did not improve from 16.71303\n","111/111 [==============================] - 61s 495ms/step - loss: 24.2440 - final_loss: 22.6373 - internal_loss: 25.8507 - val_loss: 18.3575 - val_final_loss: 17.4084 - val_internal_loss: 19.3066\n","Epoch 84/300\n","111/111 [==============================] - ETA: 0s - loss: 24.1010 - final_loss: 22.4759 - internal_loss: 25.7261\n","Epoch 84: val_final_loss improved from 16.71303 to 16.62565, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 516ms/step - loss: 24.1010 - final_loss: 22.4759 - internal_loss: 25.7261 - val_loss: 17.5199 - val_final_loss: 16.6257 - val_internal_loss: 18.4141\n","Epoch 85/300\n","111/111 [==============================] - ETA: 0s - loss: 24.0537 - final_loss: 22.4178 - internal_loss: 25.6896\n","Epoch 85: val_final_loss did not improve from 16.62565\n","111/111 [==============================] - 62s 500ms/step - loss: 24.0537 - final_loss: 22.4178 - internal_loss: 25.6896 - val_loss: 18.4449 - val_final_loss: 17.5307 - val_internal_loss: 19.3590\n","Epoch 86/300\n","111/111 [==============================] - ETA: 0s - loss: 23.8862 - final_loss: 22.2246 - internal_loss: 25.5478\n","Epoch 86: val_final_loss improved from 16.62565 to 16.46918, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 513ms/step - loss: 23.8862 - final_loss: 22.2246 - internal_loss: 25.5478 - val_loss: 17.3094 - val_final_loss: 16.4692 - val_internal_loss: 18.1495\n","Epoch 87/300\n","111/111 [==============================] - ETA: 0s - loss: 23.8990 - final_loss: 22.2207 - internal_loss: 25.5774\n","Epoch 87: val_final_loss did not improve from 16.46918\n","111/111 [==============================] - 61s 495ms/step - loss: 23.8990 - final_loss: 22.2207 - internal_loss: 25.5774 - val_loss: 18.3116 - val_final_loss: 17.3304 - val_internal_loss: 19.2929\n","Epoch 88/300\n","111/111 [==============================] - ETA: 0s - loss: 23.7245 - final_loss: 22.0489 - internal_loss: 25.4000\n","Epoch 88: val_final_loss did not improve from 16.46918\n","111/111 [==============================] - 62s 500ms/step - loss: 23.7245 - final_loss: 22.0489 - internal_loss: 25.4000 - val_loss: 17.4408 - val_final_loss: 16.5647 - val_internal_loss: 18.3170\n","Epoch 89/300\n","111/111 [==============================] - ETA: 0s - loss: 23.5881 - final_loss: 21.8993 - internal_loss: 25.2769\n","Epoch 89: val_final_loss did not improve from 16.46918\n","111/111 [==============================] - 62s 497ms/step - loss: 23.5881 - final_loss: 21.8993 - internal_loss: 25.2769 - val_loss: 18.3049 - val_final_loss: 17.3744 - val_internal_loss: 19.2353\n","Epoch 90/300\n","111/111 [==============================] - ETA: 0s - loss: 23.5605 - final_loss: 21.8541 - internal_loss: 25.2669\n","Epoch 90: val_final_loss improved from 16.46918 to 16.44727, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 512ms/step - loss: 23.5605 - final_loss: 21.8541 - internal_loss: 25.2669 - val_loss: 17.2939 - val_final_loss: 16.4473 - val_internal_loss: 18.1405\n","Epoch 91/300\n","111/111 [==============================] - ETA: 0s - loss: 23.4715 - final_loss: 21.7462 - internal_loss: 25.1967\n","Epoch 91: val_final_loss did not improve from 16.44727\n","111/111 [==============================] - 61s 495ms/step - loss: 23.4715 - final_loss: 21.7462 - internal_loss: 25.1967 - val_loss: 18.1264 - val_final_loss: 17.1730 - val_internal_loss: 19.0797\n","Epoch 92/300\n","111/111 [==============================] - ETA: 0s - loss: 23.3386 - final_loss: 21.6127 - internal_loss: 25.0645\n","Epoch 92: val_final_loss improved from 16.44727 to 16.36501, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 512ms/step - loss: 23.3386 - final_loss: 21.6127 - internal_loss: 25.0645 - val_loss: 17.2006 - val_final_loss: 16.3650 - val_internal_loss: 18.0363\n","Epoch 93/300\n","111/111 [==============================] - ETA: 0s - loss: 23.1835 - final_loss: 21.4437 - internal_loss: 24.9233\n","Epoch 93: val_final_loss did not improve from 16.36501\n","111/111 [==============================] - 62s 503ms/step - loss: 23.1835 - final_loss: 21.4437 - internal_loss: 24.9233 - val_loss: 18.1731 - val_final_loss: 17.1991 - val_internal_loss: 19.1470\n","Epoch 94/300\n","111/111 [==============================] - ETA: 0s - loss: 23.1272 - final_loss: 21.3712 - internal_loss: 24.8831\n","Epoch 94: val_final_loss did not improve from 16.36501\n","111/111 [==============================] - 61s 494ms/step - loss: 23.1272 - final_loss: 21.3712 - internal_loss: 24.8831 - val_loss: 17.2506 - val_final_loss: 16.3660 - val_internal_loss: 18.1352\n","Epoch 95/300\n","111/111 [==============================] - ETA: 0s - loss: 23.0107 - final_loss: 21.2517 - internal_loss: 24.7696\n","Epoch 95: val_final_loss did not improve from 16.36501\n","111/111 [==============================] - 61s 494ms/step - loss: 23.0107 - final_loss: 21.2517 - internal_loss: 24.7696 - val_loss: 18.0360 - val_final_loss: 17.0308 - val_internal_loss: 19.0412\n","Epoch 96/300\n","111/111 [==============================] - ETA: 0s - loss: 22.9515 - final_loss: 21.1708 - internal_loss: 24.7323\n","Epoch 96: val_final_loss improved from 16.36501 to 16.13348, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 513ms/step - loss: 22.9515 - final_loss: 21.1708 - internal_loss: 24.7323 - val_loss: 17.0126 - val_final_loss: 16.1335 - val_internal_loss: 17.8917\n","Epoch 97/300\n","111/111 [==============================] - ETA: 0s - loss: 22.8519 - final_loss: 21.0731 - internal_loss: 24.6308\n","Epoch 97: val_final_loss did not improve from 16.13348\n","111/111 [==============================] - 62s 498ms/step - loss: 22.8519 - final_loss: 21.0731 - internal_loss: 24.6308 - val_loss: 17.6437 - val_final_loss: 16.7111 - val_internal_loss: 18.5763\n","Epoch 98/300\n","111/111 [==============================] - ETA: 0s - loss: 22.7475 - final_loss: 20.9475 - internal_loss: 24.5475\n","Epoch 98: val_final_loss did not improve from 16.13348\n","111/111 [==============================] - 62s 501ms/step - loss: 22.7475 - final_loss: 20.9475 - internal_loss: 24.5475 - val_loss: 17.1866 - val_final_loss: 16.3894 - val_internal_loss: 17.9838\n","Epoch 99/300\n","111/111 [==============================] - ETA: 0s - loss: 22.6419 - final_loss: 20.8344 - internal_loss: 24.4494\n","Epoch 99: val_final_loss did not improve from 16.13348\n","111/111 [==============================] - 61s 494ms/step - loss: 22.6419 - final_loss: 20.8344 - internal_loss: 24.4494 - val_loss: 18.0288 - val_final_loss: 17.0391 - val_internal_loss: 19.0186\n","Epoch 100/300\n","111/111 [==============================] - ETA: 0s - loss: 22.5690 - final_loss: 20.7599 - internal_loss: 24.3782\n","Epoch 100: val_final_loss improved from 16.13348 to 15.90949, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 512ms/step - loss: 22.5690 - final_loss: 20.7599 - internal_loss: 24.3782 - val_loss: 16.7769 - val_final_loss: 15.9095 - val_internal_loss: 17.6443\n","Epoch 101/300\n","111/111 [==============================] - ETA: 0s - loss: 22.4691 - final_loss: 20.6596 - internal_loss: 24.2786\n","Epoch 101: val_final_loss did not improve from 15.90949\n","111/111 [==============================] - 61s 496ms/step - loss: 22.4691 - final_loss: 20.6596 - internal_loss: 24.2786 - val_loss: 17.3730 - val_final_loss: 16.4544 - val_internal_loss: 18.2916\n","Epoch 102/300\n","111/111 [==============================] - ETA: 0s - loss: 22.4125 - final_loss: 20.5762 - internal_loss: 24.2488\n","Epoch 102: val_final_loss did not improve from 15.90949\n","111/111 [==============================] - 61s 498ms/step - loss: 22.4125 - final_loss: 20.5762 - internal_loss: 24.2488 - val_loss: 16.8992 - val_final_loss: 16.0688 - val_internal_loss: 17.7296\n","Epoch 103/300\n","111/111 [==============================] - ETA: 0s - loss: 22.3554 - final_loss: 20.4956 - internal_loss: 24.2152\n","Epoch 103: val_final_loss did not improve from 15.90949\n","111/111 [==============================] - 62s 499ms/step - loss: 22.3554 - final_loss: 20.4956 - internal_loss: 24.2152 - val_loss: 17.3616 - val_final_loss: 16.4754 - val_internal_loss: 18.2478\n","Epoch 104/300\n","111/111 [==============================] - ETA: 0s - loss: 22.2619 - final_loss: 20.4040 - internal_loss: 24.1198\n","Epoch 104: val_final_loss improved from 15.90949 to 15.89913, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 511ms/step - loss: 22.2619 - final_loss: 20.4040 - internal_loss: 24.1198 - val_loss: 16.7661 - val_final_loss: 15.8991 - val_internal_loss: 17.6331\n","Epoch 105/300\n","111/111 [==============================] - ETA: 0s - loss: 22.1577 - final_loss: 20.2970 - internal_loss: 24.0184\n","Epoch 105: val_final_loss did not improve from 15.89913\n","111/111 [==============================] - 61s 493ms/step - loss: 22.1577 - final_loss: 20.2970 - internal_loss: 24.0184 - val_loss: 17.9983 - val_final_loss: 17.0378 - val_internal_loss: 18.9587\n","Epoch 106/300\n","111/111 [==============================] - ETA: 0s - loss: 22.3592 - final_loss: 20.4640 - internal_loss: 24.2545\n","Epoch 106: val_final_loss did not improve from 15.89913\n","111/111 [==============================] - 61s 494ms/step - loss: 22.3592 - final_loss: 20.4640 - internal_loss: 24.2545 - val_loss: 17.4469 - val_final_loss: 16.5327 - val_internal_loss: 18.3611\n","Epoch 107/300\n","111/111 [==============================] - ETA: 0s - loss: 22.1107 - final_loss: 20.2220 - internal_loss: 23.9994\n","Epoch 107: val_final_loss did not improve from 15.89913\n","111/111 [==============================] - 62s 498ms/step - loss: 22.1107 - final_loss: 20.2220 - internal_loss: 23.9994 - val_loss: 17.7149 - val_final_loss: 16.8651 - val_internal_loss: 18.5648\n","Epoch 108/300\n","111/111 [==============================] - ETA: 0s - loss: 21.9823 - final_loss: 20.1021 - internal_loss: 23.8626\n","Epoch 108: val_final_loss improved from 15.89913 to 15.85901, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 64s 515ms/step - loss: 21.9823 - final_loss: 20.1021 - internal_loss: 23.8626 - val_loss: 16.7364 - val_final_loss: 15.8590 - val_internal_loss: 17.6137\n","Epoch 109/300\n","111/111 [==============================] - ETA: 0s - loss: 21.9460 - final_loss: 20.0507 - internal_loss: 23.8412\n","Epoch 109: val_final_loss did not improve from 15.85901\n","111/111 [==============================] - 65s 525ms/step - loss: 21.9460 - final_loss: 20.0507 - internal_loss: 23.8412 - val_loss: 17.3051 - val_final_loss: 16.4310 - val_internal_loss: 18.1792\n","Epoch 110/300\n","111/111 [==============================] - ETA: 0s - loss: 21.9200 - final_loss: 20.0023 - internal_loss: 23.8378\n","Epoch 110: val_final_loss improved from 15.85901 to 15.82694, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 61s 512ms/step - loss: 21.9200 - final_loss: 20.0023 - internal_loss: 23.8378 - val_loss: 16.6610 - val_final_loss: 15.8269 - val_internal_loss: 17.4950\n","Epoch 111/300\n","111/111 [==============================] - ETA: 0s - loss: 21.7595 - final_loss: 19.8468 - internal_loss: 23.6723\n","Epoch 111: val_final_loss did not improve from 15.82694\n","111/111 [==============================] - 62s 501ms/step - loss: 21.7595 - final_loss: 19.8468 - internal_loss: 23.6723 - val_loss: 17.1314 - val_final_loss: 16.3331 - val_internal_loss: 17.9297\n","Epoch 112/300\n","111/111 [==============================] - ETA: 0s - loss: 21.6473 - final_loss: 19.7331 - internal_loss: 23.5615\n","Epoch 112: val_final_loss improved from 15.82694 to 15.72769, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 64s 517ms/step - loss: 21.6473 - final_loss: 19.7331 - internal_loss: 23.5615 - val_loss: 16.5794 - val_final_loss: 15.7277 - val_internal_loss: 17.4311\n","Epoch 113/300\n","111/111 [==============================] - ETA: 0s - loss: 21.5813 - final_loss: 19.6454 - internal_loss: 23.5173\n","Epoch 113: val_final_loss did not improve from 15.72769\n","111/111 [==============================] - 61s 495ms/step - loss: 21.5813 - final_loss: 19.6454 - internal_loss: 23.5173 - val_loss: 17.3352 - val_final_loss: 16.4626 - val_internal_loss: 18.2078\n","Epoch 114/300\n","111/111 [==============================] - ETA: 0s - loss: 21.6085 - final_loss: 19.6751 - internal_loss: 23.5419\n","Epoch 114: val_final_loss improved from 15.72769 to 15.70606, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 508ms/step - loss: 21.6085 - final_loss: 19.6751 - internal_loss: 23.5419 - val_loss: 16.5773 - val_final_loss: 15.7061 - val_internal_loss: 17.4485\n","Epoch 115/300\n","111/111 [==============================] - ETA: 0s - loss: 21.4642 - final_loss: 19.5110 - internal_loss: 23.4173\n","Epoch 115: val_final_loss did not improve from 15.70606\n","111/111 [==============================] - 61s 497ms/step - loss: 21.4642 - final_loss: 19.5110 - internal_loss: 23.4173 - val_loss: 17.2933 - val_final_loss: 16.3846 - val_internal_loss: 18.2020\n","Epoch 116/300\n","111/111 [==============================] - ETA: 0s - loss: 21.4221 - final_loss: 19.4624 - internal_loss: 23.3819\n","Epoch 116: val_final_loss did not improve from 15.70606\n","111/111 [==============================] - 62s 499ms/step - loss: 21.4221 - final_loss: 19.4624 - internal_loss: 23.3819 - val_loss: 16.6858 - val_final_loss: 15.8299 - val_internal_loss: 17.5417\n","Epoch 117/300\n","111/111 [==============================] - ETA: 0s - loss: 23.0729 - final_loss: 21.2533 - internal_loss: 24.8924\n","Epoch 117: val_final_loss did not improve from 15.70606\n","111/111 [==============================] - 61s 498ms/step - loss: 23.0729 - final_loss: 21.2533 - internal_loss: 24.8924 - val_loss: 17.8226 - val_final_loss: 17.0950 - val_internal_loss: 18.5501\n","Epoch 118/300\n","111/111 [==============================] - ETA: 0s - loss: 21.6912 - final_loss: 19.7516 - internal_loss: 23.6308\n","Epoch 118: val_final_loss did not improve from 15.70606\n","111/111 [==============================] - 61s 494ms/step - loss: 21.6912 - final_loss: 19.7516 - internal_loss: 23.6308 - val_loss: 16.6540 - val_final_loss: 15.8215 - val_internal_loss: 17.4865\n","Epoch 119/300\n","111/111 [==============================] - ETA: 0s - loss: 21.3999 - final_loss: 19.4448 - internal_loss: 23.3549\n","Epoch 119: val_final_loss did not improve from 15.70606\n","111/111 [==============================] - 61s 496ms/step - loss: 21.3999 - final_loss: 19.4448 - internal_loss: 23.3549 - val_loss: 17.2411 - val_final_loss: 16.3424 - val_internal_loss: 18.1398\n","Epoch 120/300\n","111/111 [==============================] - ETA: 0s - loss: 21.2213 - final_loss: 19.2532 - internal_loss: 23.1894\n","Epoch 120: val_final_loss improved from 15.70606 to 15.53016, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 513ms/step - loss: 21.2213 - final_loss: 19.2532 - internal_loss: 23.1894 - val_loss: 16.3108 - val_final_loss: 15.5302 - val_internal_loss: 17.0915\n","Epoch 121/300\n","111/111 [==============================] - ETA: 0s - loss: 21.1493 - final_loss: 19.1574 - internal_loss: 23.1412\n","Epoch 121: val_final_loss did not improve from 15.53016\n","111/111 [==============================] - 62s 500ms/step - loss: 21.1493 - final_loss: 19.1574 - internal_loss: 23.1412 - val_loss: 17.3903 - val_final_loss: 16.5542 - val_internal_loss: 18.2263\n","Epoch 122/300\n","111/111 [==============================] - ETA: 0s - loss: 21.0150 - final_loss: 19.0271 - internal_loss: 23.0030\n","Epoch 122: val_final_loss did not improve from 15.53016\n","111/111 [==============================] - 62s 498ms/step - loss: 21.0150 - final_loss: 19.0271 - internal_loss: 23.0030 - val_loss: 16.4055 - val_final_loss: 15.6381 - val_internal_loss: 17.1729\n","Epoch 123/300\n","111/111 [==============================] - ETA: 0s - loss: 20.9570 - final_loss: 18.9536 - internal_loss: 22.9603\n","Epoch 123: val_final_loss did not improve from 15.53016\n","111/111 [==============================] - 61s 498ms/step - loss: 20.9570 - final_loss: 18.9536 - internal_loss: 22.9603 - val_loss: 16.8648 - val_final_loss: 16.0003 - val_internal_loss: 17.7294\n","Epoch 124/300\n","111/111 [==============================] - ETA: 0s - loss: 20.8827 - final_loss: 18.8786 - internal_loss: 22.8868\n","Epoch 124: val_final_loss did not improve from 15.53016\n","111/111 [==============================] - 61s 497ms/step - loss: 20.8827 - final_loss: 18.8786 - internal_loss: 22.8868 - val_loss: 16.4173 - val_final_loss: 15.6301 - val_internal_loss: 17.2045\n","Epoch 125/300\n","111/111 [==============================] - ETA: 0s - loss: 20.8965 - final_loss: 18.8743 - internal_loss: 22.9186\n","Epoch 125: val_final_loss did not improve from 15.53016\n","111/111 [==============================] - 61s 495ms/step - loss: 20.8965 - final_loss: 18.8743 - internal_loss: 22.9186 - val_loss: 17.0456 - val_final_loss: 16.1513 - val_internal_loss: 17.9399\n","Epoch 126/300\n","111/111 [==============================] - ETA: 0s - loss: 20.7751 - final_loss: 18.7503 - internal_loss: 22.8000\n","Epoch 126: val_final_loss improved from 15.53016 to 15.39680, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 64s 518ms/step - loss: 20.7751 - final_loss: 18.7503 - internal_loss: 22.8000 - val_loss: 16.2439 - val_final_loss: 15.3968 - val_internal_loss: 17.0911\n","Epoch 127/300\n","111/111 [==============================] - ETA: 0s - loss: 20.7068 - final_loss: 18.6868 - internal_loss: 22.7268\n","Epoch 127: val_final_loss did not improve from 15.39680\n","111/111 [==============================] - 61s 497ms/step - loss: 20.7068 - final_loss: 18.6868 - internal_loss: 22.7268 - val_loss: 16.8495 - val_final_loss: 15.9714 - val_internal_loss: 17.7275\n","Epoch 128/300\n","111/111 [==============================] - ETA: 0s - loss: 20.6705 - final_loss: 18.6403 - internal_loss: 22.7007\n","Epoch 128: val_final_loss did not improve from 15.39680\n","111/111 [==============================] - 62s 497ms/step - loss: 20.6705 - final_loss: 18.6403 - internal_loss: 22.7007 - val_loss: 16.3230 - val_final_loss: 15.5398 - val_internal_loss: 17.1063\n","Epoch 129/300\n","111/111 [==============================] - ETA: 0s - loss: 20.6140 - final_loss: 18.5660 - internal_loss: 22.6621\n","Epoch 129: val_final_loss did not improve from 15.39680\n","111/111 [==============================] - 61s 495ms/step - loss: 20.6140 - final_loss: 18.5660 - internal_loss: 22.6621 - val_loss: 16.8340 - val_final_loss: 15.9584 - val_internal_loss: 17.7096\n","Epoch 130/300\n","111/111 [==============================] - ETA: 0s - loss: 20.5793 - final_loss: 18.5317 - internal_loss: 22.6269\n","Epoch 130: val_final_loss improved from 15.39680 to 15.31661, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 515ms/step - loss: 20.5793 - final_loss: 18.5317 - internal_loss: 22.6269 - val_loss: 16.1340 - val_final_loss: 15.3166 - val_internal_loss: 16.9514\n","Epoch 131/300\n","111/111 [==============================] - ETA: 0s - loss: 20.4570 - final_loss: 18.3987 - internal_loss: 22.5153\n","Epoch 131: val_final_loss did not improve from 15.31661\n","111/111 [==============================] - 66s 533ms/step - loss: 20.4570 - final_loss: 18.3987 - internal_loss: 22.5153 - val_loss: 17.0437 - val_final_loss: 16.2123 - val_internal_loss: 17.8751\n","Epoch 132/300\n","111/111 [==============================] - ETA: 0s - loss: 20.5037 - final_loss: 18.4526 - internal_loss: 22.5548\n","Epoch 132: val_final_loss did not improve from 15.31661\n","111/111 [==============================] - 62s 499ms/step - loss: 20.5037 - final_loss: 18.4526 - internal_loss: 22.5548 - val_loss: 16.1682 - val_final_loss: 15.3941 - val_internal_loss: 16.9424\n","Epoch 133/300\n","111/111 [==============================] - ETA: 0s - loss: 20.4264 - final_loss: 18.3726 - internal_loss: 22.4803\n","Epoch 133: val_final_loss did not improve from 15.31661\n","111/111 [==============================] - 61s 498ms/step - loss: 20.4264 - final_loss: 18.3726 - internal_loss: 22.4803 - val_loss: 16.6376 - val_final_loss: 15.7931 - val_internal_loss: 17.4821\n","Epoch 134/300\n","111/111 [==============================] - ETA: 0s - loss: 20.3004 - final_loss: 18.2441 - internal_loss: 22.3568\n","Epoch 134: val_final_loss improved from 15.31661 to 15.31108, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n","111/111 [==============================] - 63s 517ms/step - loss: 20.3004 - final_loss: 18.2441 - internal_loss: 22.3568 - val_loss: 16.1416 - val_final_loss: 15.3111 - val_internal_loss: 16.9720\n","Epoch 135/300\n","111/111 [==============================] - ETA: 0s - loss: 20.2383 - final_loss: 18.1795 - internal_loss: 22.2970\n","Epoch 135: val_final_loss did not improve from 15.31108\n","111/111 [==============================] - 62s 500ms/step - loss: 20.2383 - final_loss: 18.1795 - internal_loss: 22.2970 - val_loss: 16.6388 - val_final_loss: 15.7264 - val_internal_loss: 17.5513\n","Epoch 136/300\n","111/111 [==============================] - ETA: 0s - loss: 20.4008 - final_loss: 18.3079 - internal_loss: 22.4936\n","Epoch 136: val_final_loss did not improve from 15.31108\n","111/111 [==============================] - 64s 518ms/step - loss: 20.4008 - final_loss: 18.3079 - internal_loss: 22.4936 - val_loss: 16.7845 - val_final_loss: 15.9427 - val_internal_loss: 17.6264\n","Epoch 137/300\n","111/111 [==============================] - ETA: 0s - loss: 20.4106 - final_loss: 18.2966 - internal_loss: 22.5246\n","Epoch 137: val_final_loss did not improve from 15.31108\n","111/111 [==============================] - 61s 497ms/step - loss: 20.4106 - final_loss: 18.2966 - internal_loss: 22.5246 - val_loss: 16.5633 - val_final_loss: 15.7617 - val_internal_loss: 17.3650\n","Epoch 138/300\n","111/111 [==============================] - ETA: 0s - loss: 20.2070 - final_loss: 18.1016 - internal_loss: 22.3125\n","Epoch 138: val_final_loss did not improve from 15.31108\n","111/111 [==============================] - 62s 498ms/step - loss: 20.2070 - final_loss: 18.1016 - internal_loss: 22.3125 - val_loss: 16.2648 - val_final_loss: 15.4763 - val_internal_loss: 17.0534\n","Epoch 139/300\n","111/111 [==============================] - ETA: 0s - loss: 20.1608 - final_loss: 18.0606 - internal_loss: 22.2609\n","Epoch 139: val_final_loss did not improve from 15.31108\n","111/111 [==============================] - 61s 497ms/step - loss: 20.1608 - final_loss: 18.0606 - internal_loss: 22.2609 - val_loss: 16.5181 - val_final_loss: 15.6474 - val_internal_loss: 17.3889\n","Epoch 140/300\n","111/111 [==============================] - ETA: 0s - loss: 20.0387 - final_loss: 17.9382 - internal_loss: 22.1391\n","Epoch 140: val_final_loss did not improve from 15.31108\n","111/111 [==============================] - 61s 500ms/step - loss: 20.0387 - final_loss: 17.9382 - internal_loss: 22.1391 - val_loss: 16.0929 - val_final_loss: 15.3152 - val_internal_loss: 16.8705\n","Epoch 141/300\n"]}],"source":["if 'config' not in globals():\n","  config=CFG()\n","tf.debugging.disable_traceback_filtering()\n","train(config,use_supplemental=True,use_chicago=False)\n","#assert False"]},{"cell_type":"markdown","metadata":{"id":"Cdtx5fW3-DPm"},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-uAqcPmSYi1-","trusted":true},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZWJM1IC5Yi1-","trusted":true},"outputs":[],"source":["class InferModel(tf.Module):\n","    def __init__(self, model,config=CFG):\n","        super().__init__()\n","\n","        self.model = model\n","        self.max_len=config.max_len\n","\n","    @tf.function(\n","        input_signature=[tf.TensorSpec(shape=(None,Constants.NUM_INPUT_FEATURES), dtype=tf.float32, name=\"inputs\")]\n","    )\n","    def __call__(self, inputs):\n","        \"\"\"\n","        Applies the feature generation model and main model to the input tensor.\n","\n","        Args:\n","            inputs: Input tensor with shape (T, F).\n","\n","        Returns:\n","            A dictionary with a single key 'outputs' and corresponding output tensor.\n","        \"\"\"\n","        x=tf.cast(inputs,tf.float32)\n","        x = x[None] # trick to deal with empty frames\n","        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, Constants.NUM_INPUT_FEATURES)), lambda: tf.identity(x))\n","        x = x[0]\n","        x = preprocess(x,max_len=self.max_len)\n","\n","        x = self.model(x[None],training=False)[0][0]\n","\n","        x=decode_phrase(x)\n","        x = tf.cond(tf.shape(x)[0] == 0, lambda: tf.zeros(1, tf.int32), lambda: tf.identity(x))\n","\n","        outputs=tf.one_hot(x,depth=59,dtype=tf.float32)\n","        #outputs=x\n","        return {\"outputs\": outputs}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7xpye2JQYi1_","trusted":true},"outputs":[],"source":["\n","config=CFG\n","\n","model = get_model(\n","    max_len=config.max_len,\n","    output_dim=config.output_dim,\n","    dim=config.dim,\n","    input_pad=Constants.INPUT_PAD,\n",")\n","experiment_id=0\n","\n","saved_based_model = f\"{config.input_path}/weights/{config.comment}-exp{experiment_id}-best.h5\"\n","model.load_weights(saved_based_model)\n","print(f\"model with weights {saved_based_model}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jwxBqovnYi1_","trusted":true},"outputs":[],"source":["# Sanity Check\n","import json\n","with open (config.input_path+\"/character_to_prediction_index.json\", \"r\") as f:\n","    character_map = json.load(f)\n","rev_character_map = {j:i for i,j in character_map.items()}\n","\n","infer_keras_model=InferModel(model)\n","\n","main_dir = config.input_path\n","path = f'{main_dir}/train_landmarks/5414471.parquet'\n","cols=selected_columns(path)\n","df = pd.read_parquet(path, engine = 'auto', columns = cols)\n","seq_id=1816796431\n","seq=df.loc[seq_id]\n","data = seq[cols].to_numpy()\n","print(f'input shape: {data.shape}, dtype: {data.dtype}')\n","output = infer_keras_model(data)[\"outputs\"]\n","prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output, axis=1)])\n","\n","print(prediction_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQM7GMtfYi1_","trusted":true},"outputs":[],"source":["SAVED_MODEL_PATH=config.output_path+\"/infer_model\"\n","#\n","tf.saved_model.save(infer_keras_model,SAVED_MODEL_PATH)\n","keras_model_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_PATH)\n","keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","keras_model_converter.target_spec.supported_types = [tf.float16]\n","\n","#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","#converter.allow_custom_ops=True\n","tflite_model = keras_model_converter.convert()\n","TFLITE_FILE_PATH=config.output_path+\"/model.tflite\"\n","with open(TFLITE_FILE_PATH, \"wb\") as f:\n","    f.write(tflite_model)\n","\n","with open(config.output_path+'/inference_args.json', 'w') as f:\n","     json.dump({ 'selected_columns': cols }, f)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2KNUGXjBYi2A","trusted":true},"outputs":[],"source":["interpreter = tf.lite.Interpreter(TFLITE_FILE_PATH)\n","REQUIRED_SIGNATURE = \"serving_default\"\n","REQUIRED_OUTPUT = \"outputs\"\n","found_signatures = list(interpreter.get_signature_list().keys())\n","if REQUIRED_SIGNATURE not in found_signatures:\n","    print(\"Required input signature not found.\")\n","\n","prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n","output = prediction_fn(inputs=data)\n","prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n","print(prediction_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aTeDBunuYi2A","trusted":true},"outputs":[],"source":["\n","!zip  submission.zip \"/kaggle/working/model.tflite\" \"/kaggle/working/inference_args.json\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZWcEOwClYi2A","trusted":true},"outputs":[],"source":["#!pip install /kaggle/input/tflite-wheels-2140/tflite_runtime_nightly-2.14.0.dev20230508-cp310-cp310-manylinux2014_x86_64.whl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6W11OFc7Yi2A","trusted":true},"outputs":[],"source":["import os\n","\n","\n","import json\n","import pandas as pd\n","import tflite_runtime.interpreter as tflite\n","import numpy as np\n","import time\n","from tqdm import tqdm\n","import Levenshtein as Lev\n","import glob\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zoac5lq4Yi2B","trusted":true},"outputs":[],"source":["SEL_FEATURES = json.load(open('/kaggle/working/inference_args.json'))['selected_columns']\n","\n","def load_relevant_data_subset(pq_path):\n","        return pd.read_parquet(pq_path, columns=SEL_FEATURES) #selected_columns)\n","\n","with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n","    character_map = json.load(f)\n","rev_character_map = {j:i for i,j in character_map.items()}\n","\n","\n","df_csv = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\n","\n","idx = 0\n","sample = df_csv.loc[idx]\n","loaded = load_relevant_data_subset('/kaggle/input/asl-fingerspelling/' + sample['path'])\n","loaded = loaded[loaded.index==sample['sequence_id']].values\n","print(loaded.shape)\n","frames = loaded\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNogvTNmcj-t","trusted":true},"outputs":[],"source":["\n","st = time.time()\n","count=0\n","model_time = 0\n","\n","N=0\n","D=0\n","\n","files=sorted(glob.glob('/kaggle/input/asl-fingerspelling/train_landmarks/*.parquet'))[:6]\n","for f in files:\n","    fid=int(f.split(\"/\")[-1].split(\".\")[0])\n","    df = load_relevant_data_subset(f)\n","    seq=df.index.drop_duplicates()\n","    for ind in tqdm(seq):\n","        sample=df_csv[(df_csv[\"sequence_id\"]==ind) & (df_csv[\"file_id\"]==fid)]\n","        #print(sample)\n","        loaded = df.loc[ind].values\n","        count+=1\n","        md_st = time.time()\n","\n","        # out = infer_keras_model(loaded)[\"outputs\"] # original model\n","\n","        out = prediction_fn(inputs=loaded)[REQUIRED_OUTPUT] # tflite\n","\n","        model_time += time.time() - md_st\n","\n","        prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(out, axis=1)])\n","        assert out.ndim==2\n","        assert out.shape[1]==59\n","        assert out.dtype==np.float32\n","        assert np.all(np.isfinite(out))\n","        s1=sample[\"phrase\"].item()\n","        s2=prediction_str\n","        n = len(s1)\n","        d = Lev.distance(s1,s2)\n","        N=N+n\n","        D=D+d\n","        #print(ind,s1,s2,n,d)\n","lev=(N-D)/N\n","print(f'Lev: {lev:.4f}')\n","print(f'Mean time: {(time.time() - st)/count:.3f}')\n","print(f'Mean time only infer: {model_time/count:.3f}')\n","\n"]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":["3wCTyTzWcj-f","eErQkrlpYi11","Vh1J_bDCYi13","VZZRnooBcj-n","eMpk8n7Ccj-o","5msRCmt0Yi16"],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
