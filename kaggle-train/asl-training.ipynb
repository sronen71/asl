{"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":["Vh1J_bDCYi13"],"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pips","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow-addons\n!pip install cached_property\n!pip intstall tensorflow==13.0","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:54:24.683762Z","iopub.execute_input":"2023-07-25T05:54:24.684213Z","iopub.status.idle":"2023-07-25T05:55:32.330429Z","shell.execute_reply.started":"2023-07-25T05:54:24.684161Z","shell.execute_reply":"2023-07-25T05:55:32.329085Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.10/site-packages (0.20.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons) (21.3)\nRequirement already satisfied: typeguard<3.0.0,>=2.7 in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons) (2.13.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow-addons) (3.0.9)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: cached_property in /opt/conda/lib/python3.10/site-packages (1.5.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mERROR: unknown command \"intstall\" - maybe you meant \"install\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# For Colab","metadata":{"id":"ENYLFLzurs3S"}},{"cell_type":"code","source":"import os\nIN_COLAB = 'COLAB_GPU' in os.environ\nif IN_COLAB:\n  from google.colab import auth\n  auth.authenticate_user()\n  drive.mount('/content/drive')","metadata":{"id":"agm4BN62kwwE","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:32.334167Z","iopub.execute_input":"2023-07-25T05:55:32.335908Z","iopub.status.idle":"2023-07-25T05:55:32.345665Z","shell.execute_reply.started":"2023-07-25T05:55:32.335869Z","shell.execute_reply":"2023-07-25T05:55:32.344443Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /kaggle/working","metadata":{"id":"TB_VhXDvhCT2","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:32.347072Z","iopub.execute_input":"2023-07-25T05:55:32.347438Z","iopub.status.idle":"2023-07-25T05:55:33.488191Z","shell.execute_reply.started":"2023-07-25T05:55:32.347398Z","shell.execute_reply":"2023-07-25T05:55:33.486645Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport numpy as np\nimport pandas as pd\n\nimport random\nimport psutil\nimport gc\nimport math\nimport tensorflow as tf\nimport tensorflow_addons as tfa\ngpus = tf.config.list_physical_devices(\"GPU\")\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth(gpu, True)\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eoljCzzvYi1z","outputId":"bcd5cad0-52be-41a7-dc75-07825ef1e51d","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:33.491165Z","iopub.execute_input":"2023-07-25T05:55:33.492155Z","iopub.status.idle":"2023-07-25T05:55:33.501422Z","shell.execute_reply.started":"2023-07-25T05:55:33.492106Z","shell.execute_reply":"2023-07-25T05:55:33.500165Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# CTC Loss","metadata":{}},{"cell_type":"code","source":"from __future__ import annotations\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Jul 18 20:29:39 2023\n\"\"\"\n\n# Copyright 2021 Alexey Tochin\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom typing import Union, Callable, List, Optional, Type\nimport tensorflow as tf\nimport numpy as np\n\nfrom abc import ABC, abstractmethod\nfrom cached_property import cached_property\n\n\ninf = tf.constant(np.inf)\n\n\ndef logit_to_logproba(logit: tf.Tensor, axis: int) -> tf.Tensor:\n    \"\"\"Converts logits to logarithmic probabilities:\n        logit_to_logproba(x) = x - log (sum along axis (exp(x))\n\n    Args:\n        logit:  tf.Tensor, dtype = tf.float32\n        axis: integer, like for tf.reduce_logsumexp\n\n    Returns:    tf.Tensor, of the same shape and size as input logit\n    \"\"\"\n    log_probas = logit - tf.reduce_logsumexp(input_tensor=logit, axis=axis, keepdims=True)\n    return log_probas\n\n\ndef apply_logarithmic_mask(tensor: tf.Tensor, mask: tf.Tensor) -> tf.Tensor:\n    \"\"\"Masks a logarithmic representation of a tensor, namely\n    1. Keeps the value of tensor unchanged for True values of mask\n    2. Replace the value of tensor by -tf.inf for False values of mask\n\n    Args:\n        tensor: tf.Tensor, dtype = tf.float32 of the same shape as mask or broadcastable\n        mask:   tf.Tensor, dbool = tf.float32 of the same shape as tensor or broadcastable\n\n    Returns:    tf.Tensor, dtype = tf.float32 of the same shape as tensor\n    \"\"\"\n    return tensor + tf.math.log(tf.cast(mask, dtype=tf.float32))\n\n\ndef logsumexp(x: tf.Tensor, y: tf.Tensor) -> tf.Tensor:\n    \"\"\"A numerically stable version of elementwise function\n        logsumexp(x, y) = log (e ** x + e ** y)\n\n    Args:\n        x:      tf.Tensor of the shape and size as y or broadcastable\n        y:      tf.Tensor of the shape and size as x or broadcastable\n\n    Returns:    tf.Tensor of the shape and size as x and y\n    \"\"\"\n    return tf.where(\n        condition=x < y,\n        x=y + tf.math.softplus(x - y),\n        y=tf.where(\n            condition=x > y,\n            x=x + tf.math.softplus(y - x),\n            y=x + np.log(2.)\n        ),\n    )\n\n\ndef subexp(x: tf.Tensor, y: tf.Tensor) -> tf.Tensor:\n    \"\"\"A numerically stable version of elementwise function\n        subexp(x,y) := exp x - exp y\n\n    Args:\n        x:      tf.Tensor, shape broadcastable to y\n        y:      tf.Tensor, shape broadcastable to x\n\n    Returns:    tf.Tensor, shape, the same as x and y\n    \"\"\"\n    return tf.where(\n        condition=x > y,\n        x=-tf.exp(x) * tf.math.expm1(y - x),\n        y=tf.where(\n            condition=x < y,\n            x=tf.exp(y) * tf.math.expm1(x - y),\n            y=tf.zeros_like(x),\n        ),\n    )\n\n\ndef unsorted_segment_logsumexp(data: tf.Tensor, segment_ids: tf.Tensor, num_segments: Union[int, tf.Tensor])\\\n        -> tf.Tensor:\n    \"\"\"Computes the logarithmic sum of exponents along segments of a tensor\n    like other operators from tf.math.unsorted_segment_* family.\n\n    Args:\n        data:           tf.Tensor,  shape = [...] + data_dims,\n        segment_ids:    tf.Tensor,  shape = [...], dtype = tf.int32\n        num_segments:   tf.Tensor,  shape = [], dtype = tf.int32\n\n    Returns:            tf.Tensor,  shape = [num_segments] + data_dims, for the same type as data\n    \"\"\"\n    data_max = tf.math.unsorted_segment_max(data=data, segment_ids=segment_ids, num_segments=num_segments)\n    data_normed = data - tf.gather(params=data_max, indices=segment_ids)\n    output = data_max + tf.math.log(tf.math.unsorted_segment_sum(\n        data=tf.exp(data_normed),\n        segment_ids=segment_ids,\n        num_segments=num_segments,\n    ))\n    return output\n\n\ndef pad_until(\n        tensor: tf.Tensor,\n        desired_size: Union[tf.Tensor, int],\n        axis: int,\n        pad_value: Union[tf.Tensor, int, float, bool] = 0\n) -> tf.Tensor:\n    \"\"\"Pads tensor until desired dimension from right,\n\n    Args:\n        tensor:         tf.Tensor, of any shape and type\n        desired_size:   tf.Tensor or pythonic static integer\n        axis:           pythonic static integer for pad axes\n        pad_value:      tf.Tensor or pythonic numerical for padding\n\n    Returns:            tf.Tensor, the same shape as tensor except axis that equals to desired_size.\n    \"\"\"\n    rank = len(tensor.shape)\n    if axis >= rank:\n        raise ValueError()\n\n    current_size = tf.shape(tensor)[axis]\n    paddings = [[0, 0]] * axis + [[0, desired_size - current_size]] + [[0, 0]] * (rank - axis - 1)\n    return tf.pad(tensor=tensor, paddings=paddings, constant_values=pad_value)\n\n\ndef insert_zeros(tensor: tf.Tensor, mask: tf.Tensor) -> tf.Tensor:\n    \"\"\"Inserts zeros into tensor before each masked element.\n    For example:\n    ```python\n        output = insert_zeros(\n            tensor =  tf.constant([[1, 2, 3, 4, 5], [10, 20, 30, 40, 50]], dtype = tf.int32),\n            mask = tf.constant([[False, True, False, False, True], [False, True,  True, True,  False]]),\n        )\n        # -> [[1, 0, 2, 3, 4, 0, 5, 0], [10, 0, 20, 0, 30, 0, 40, 50]]\n        # We insert 0s 2, 5, 20, 30, and 40 because their positions in input tensor corresponds to True value\n        in mask.\n    ```\n\n    Args:\n        tensor: tf.Tensor, shape = [batch, length], any type and the same shape as mask\n        mask:   tf.Tensor, shape = [batch, length], dtype = tf.bool and the same shape as tensor\n\n    Returns:    tf.Tensor, shape = [batch, length + max_num_insertions],\n                where max_num_insertions is the maximal number of True values along the 0 batch dimension of mask.\n                dtype = same as input tensor\n    \"\"\"\n    batch_size = tf.shape(tensor)[0]\n    length = tf.shape(mask)[1]\n\n    delta = tf.cumsum(tf.cast(mask, dtype=tf.int32), exclusive=False, axis=1)\n    max_num_insertions = tf.reduce_max(delta[:, -1])\n\n    y, x = tf.meshgrid(tf.range(length), tf.range(batch_size))\n    y = y + delta\n    indices = tf.reshape(tf.stack([x, y], 2), [-1, 2])\n\n    output = tf.scatter_nd(\n        indices=indices,\n        updates=tf.reshape(tensor, shape=[-1]),\n        shape=tf.stack([batch_size, length + max_num_insertions])\n    )\n\n    return output\n\n\ndef unfold(\n        init_tensor: tf.Tensor,\n        iterfunc: Callable[[tf.Tensor, tf.Tensor], tf.Tensor],\n        num_iters: Union[int, tf.Tensor],\n        d_i: int,\n        element_shape: tf.TensorShape,\n        swap_memory: bool = False,\n        name: str = \"unfold\",\n) -> tf.Tensor:\n    \"\"\"Calculates a tensor by iterations over i that is the concatenation\n        for d_i = +1:\n            init_tensor\n            iterfunc(init_tensor, 0)\n            iterfunc(iterfunc(init_tensor, 0), 1)\n            ...\n            ..., num_iters - 1)\n            ..., num_iters - 1), num_iters)\n        for d_i = -1:\n            ..., 2), 1), 0)\n            ..., 2), 1)\n            ...\n            iterfunc(iterfunc(init_tensor, num_iters - 1), num_iters - 2)\n            iterfunc(init_tensor, num_iters - 1)\n            init_tensor\n    For example:\n    ```python\n        unfold(\n            init_tensor=tf.constant(0),\n            iterfunc=lambda x, i: x + i,\n            num_iters=5,\n            d_i=1,\n            element_shape=tf.TensorShape([]),\n        )\n        # -> [0, 0, 1, 3, 6, 10]\n    ```\n\n    Args:\n        init_tensor:    tf.Tensor, of any shape that is the initial value of the iterations.\n        iterfunc:       tf.Tensor, tf.Tensor -> tf.Tensor, that is the iteration function\n                            from and onto the same shape as init_tensor\n        num_iters:      tf.Tensor or static integer that is the number of iterations\n        d_i:            either +1 or -1, where\n                            +1 corresponds for the iterations from 0 to num_iters inclusive\n                            -1 corresponds for the iterations from num_iters to 0 inclusive\n        element_shape:  tf.TensorShape([]) that is the shape of init_tensor\n        swap_memory:    the same as for tf.while_loop, argument\n        name:           str, local tensor names scope\n\n    Returns:            tf.Tensor, shape = [num_iters + 1] + init_tensor.shape\n                        dtype the same as init_tensor\n    \"\"\"\n    assert d_i in {-1, 1}\n    positive_direction = d_i == 1\n\n    with tf.name_scope(name):\n        num_iters = tf.convert_to_tensor(num_iters)\n\n        tensor_array = tf.TensorArray(\n            dtype=init_tensor.dtype,\n            size=num_iters + 1,\n            element_shape=element_shape,\n            clear_after_read=False,\n            infer_shape=True,\n            dynamic_size=False,\n        )\n        tensor_array = tensor_array.write(0 if positive_direction else num_iters, init_tensor)\n\n        def body(i, tensor_slice):\n            last_value = tensor_slice.read(i if positive_direction else i + 1)\n            new_value = iterfunc(last_value, i)\n            tensor_slice = tensor_slice.write(i + 1 if positive_direction else i, new_value)\n            return i + d_i, tensor_slice\n\n        n = tf.constant(0, dtype=tf.int32) if positive_direction else num_iters - 1\n        _, array_out = tf.while_loop(\n            cond=lambda i, _: tf.constant(True),\n            body=body,\n            loop_vars=(n, tensor_array),\n            maximum_iterations=num_iters,\n            swap_memory=swap_memory,\n            name=f\"unfold_while_loop\",\n        )\n        return array_out.stack()\n\n\ndef reduce_max_with_default(input_tensor: tf.Tensor, default: tf.Tensor) -> tf.Tensor:\n    \"\"\"A version of tf.reduce_max function that supports default values for zero size input.\n    Support axis=None case only that corresponds to scalar output\n\n    Args:\n        input_tensor:   tf.Tensor, of any shape and numerical type\n        default:        tf.Tensor, shape = [], dtype the same as input_tensor\n\n    Returns:            tf.Tensor, shape = [], dtype the same as input_tensor\n    \"\"\"\n    total_size = tf.shape(tf.reshape(input_tensor, [-1]))[0]\n    return tf.where(\n        condition=total_size > 0,\n        x=tf.reduce_max(input_tensor),\n        y=default\n    )\n\n\ndef expand_many_dims(input: tf.Tensor, axes: List[int]) -> tf.Tensor:\n    \"\"\"Analogous of tf.expand_dims for multiple new dimensions.\n    Like for tf.expand_dims no new memory allocated for the output tensor.\n\n    For example:\n        expand_many_dims(tf.zeros(shape=[5, 1, 3]), axes=[0, 4, 5]).shape\n        # -> [1, 5, 1, 3, 1, 1]\n\n    Args:\n        input:  tf.Tensor of any rank shape and type\n        axes:   list of integer that are supposed to be the indexes of new dimensions.\n\n    Returns:    tf.Tensor of the same type an input and rank = rank(input) + len(axes)\n    \"\"\"\n    tensor = input\n    for axis in axes:\n        tensor = tf.expand_dims(input=tensor, axis=axis)\n\n    return tensor\n\n\ndef smart_transpose(a: tf.Tensor, perm=List[int]) -> tf.Tensor:\n    \"\"\"Extension of tf.transpose.\n    Parameter perm may be shorter list than rank on input tensor a.\n    This case all dimensions that are beyond the list perm remain unchanged.\n\n    For example:\n        smart_transpose(tf.zeros(shape=[2, 3, 4, 5, 6]), [2, 1, 0]).shape\n        # -> [4, 3, 2, 5, 6]\n\n    Args:\n        a:      tf.Tensor of any rank shape and type\n        perm:   list of integers like for tf.transpose but in may be shorter than the shape of a.\n\n    Returns:    tf.Tensor of the same type and rank as th input tensor a.\n    \"\"\"\n    if len(perm) > len(a.shape):\n        raise ValueError(f\"Tensor with shape '{a.shape}' cannot be reshaped to '{perm}'\")\n    else:\n        perm_rest = list(range(len(perm), len(a.shape)))\n\n    return tf.transpose(a=a, perm=perm + perm_rest)\n\n\ndef smart_reshape(tensor: tf.Tensor, shape: List[Optional[Union[int, tf.Tensor]]]) -> tf.Tensor:\n    \"\"\"A version of tf.reshape.\n    1. The ouput tensor is always of the same rank as input tensor.\n    2. The parameter shape is supposed to be a list that is smaller or equal\n    than the tensor shape.\n    3. The list shape may contain None, that means \"keep this dimension unchanged\".\n    4. The list shape is appended with None value to be of the same length as the input tensor shape.\n    5. Like for tf.reshape output tensor does not requre new memory for allocation.\n\n    For example:\n    ```python\n        smart_reshape(\n            tensor=tf.zeros(shape=[2, 3, 4, 5]),\n            shape=[8, None, 1]\n        )\n        # -> tf.Tensor([8, 3, 1, 5])\n    ```\n\n    Args:\n        tensor: tf.Tensor of any shape and type\n        shape:  list of optional static of dynamic integrates\n\n    Returns:    tf.Tensor of the same typey and rank as the input tensor\n    \"\"\"\n    if len(shape) > len(tensor.shape):\n        raise ValueError(f\"Tensor with shape {tensor.shape} cannot be reshaped to {shape}.\")\n    else:\n        shape = shape + [None] * (len(tensor.shape) - len(shape))\n\n    original_shape = tf.shape(tensor)\n    new_shape = []\n    for index, dim in enumerate(shape):\n        if dim is None:\n            new_shape.append(original_shape[index])\n        else:\n            new_shape.append(dim)\n\n    return tf.reshape(tensor=tensor, shape=new_shape)\n\n\n\ndef ctc_loss(\n        labels: tf.Tensor,\n        logits: tf.Tensor,\n        label_length: tf.Tensor,\n        logit_length: tf.Tensor,\n        blank_index: Union[int, tf.Tensor],\n        ctc_loss_data_cls: Type[BaseCtcLossData],\n) -> tf.Tensor:\n    \"\"\"Computes a version of CTC loss from\n    http://www.cs.toronto.edu/~graves/icml_2006.pdf.\n\n    Args:\n        labels:             tf.Tensor, shape = [batch, max_label_length],       dtype = tf.int32\n        logits:             tf.Tensor, shape = [batch, max_length, mum_tokens], dtype = tf.float32\n        label_length:       tf.Tensor, shape = [batch],                         dtype = tf.int32\n        logit_length:       tf.Tensor, shape = [batch],                         dtype = tf.int32\n        blank_index:        static integer >= 0\n        ctc_loss_data_cls:  BaseCtcLossData class\n\n    Returns:                tf.Tensor, shape = [batch, max_length, mum_tokens], dtype = tf.float32\n    \"\"\"\n    log_probas = logit_to_logproba(logit=logits, axis=2)\n    loss = ctc_loss_from_logproba(\n        labels=labels,\n        logprobas=log_probas,\n        label_length=label_length,\n        logit_length=logit_length,\n        blank_index=blank_index,\n        ctc_loss_data_cls=ctc_loss_data_cls,\n    )\n    return loss\n\n\ndef ctc_loss_from_logproba(\n        labels: tf.Tensor,\n        logprobas: tf.Tensor,\n        label_length: tf.Tensor,\n        logit_length: tf.Tensor,\n        blank_index: Union[int, tf.Tensor],\n        ctc_loss_data_cls: Type[BaseCtcLossData],\n) -> tf.Tensor:\n    \"\"\"Computes a version of CTC loss from logarothmic probabilities considered as independent parameters.\n\n    Args:\n        labels:             tf.Tensor, shape = [batch, max_label_length],       dtype = tf.int32\n        logprobas:          tf.Tensor, shape = [batch, max_length, mum_tokens], dtype = tf.float32\n        label_length:       tf.Tensor, shape = [batch],                         dtype = tf.int32\n        logit_length:       tf.Tensor, shape = [batch],                         dtype = tf.int32\n        blank_index:        static integer >= 0\n        ctc_loss_data_cls:  BaseCtcLossData class\n\n    Returns:                tf.Tensor, shape = [batch, max_length, mum_tokens], dtype = tf.float32\n    \"\"\"\n    loss_data = ctc_loss_data_cls(\n        labels=labels,\n        logprobas=tf.stop_gradient(logprobas),\n        label_length=label_length,\n        logit_length=logit_length,\n        blank_index=blank_index,\n    )\n\n    return loss_data.forward_fn(logprobas)\n\n\nclass BaseCtcLossData(ABC):\n    \"\"\" Base class for CTC loss data. \"\"\"\n    def __init__(\n            self,\n            labels: tf.Tensor,\n            logprobas: tf.Tensor,\n            label_length: tf.Tensor,\n            logit_length: tf.Tensor,\n            blank_index: Union[int, tf.Tensor],\n            swap_memory: bool = False,\n            **kwargs\n    ):\n        super().__init__(**kwargs)\n        self._logprobas = logprobas\n        self._original_label = labels\n        self._logit_length = logit_length\n        self._original_label_length = label_length\n        self.max_label_length_plus_one = tf.shape(labels)[1]\n        self._verify_inputs()\n\n        if isinstance(blank_index, (tf.Tensor, tf.Variable)):\n            self._blank_index = blank_index\n        else:\n            self._blank_index = tf.constant(blank_index, dtype=tf.int32)\n\n        self._swap_memory = swap_memory\n\n    def _verify_inputs(self) -> None:\n        assert len(self._logprobas.shape) == 3\n        assert self._logprobas.dtype == tf.float32\n        assert len(self._original_label.shape) == 2\n        assert len(self._logit_length.shape) == 1\n        assert len(self._original_label_length.shape) == 1\n\n        assert self._logprobas.shape[0] == self._original_label.shape[0]\n        assert self._logprobas.shape[0] == self._logit_length.shape[0]\n        assert self._logprobas.shape[0] == self._original_label_length.shape[0]\n\n    @tf.custom_gradient\n    def forward_fn(self, unused_logprobas: tf.Tensor) -> tf.Tensor:\n        def backprop(d_loss):\n            return expand_many_dims(d_loss, axes=[1, 2]) * self.gradient_fn(unused_logprobas)\n\n        return self.loss, backprop\n\n    @tf.custom_gradient\n    def gradient_fn(self, unused_logprobas: tf.Tensor) -> tf.Tensor:\n        def backprop(d_gradient):\n            output = tf.reduce_sum(\n                input_tensor=expand_many_dims(d_gradient, axes=[1, 2]) * self.hessian_fn(unused_logprobas),\n                axis=[3, 4]\n            )\n            return output\n\n        return self.gradient, backprop\n\n    @tf.custom_gradient\n    def hessian_fn(self, unused_logprobas: tf.Tensor) -> tf.Tensor:\n        def backprop(d_hessian):\n            raise NotImplementedError(f\"Third order derivative over the ctc loss function is not implemented.\")\n\n        return self.hessian, backprop\n\n    @cached_property\n    def hessian(self) -> tf.Tensor:\n        \"\"\"Calculates Hessian of loss w.r.t. input logits.\n\n        Returns: tf.Tensor, shape = [batch_size, max_logit_length, num_tokens, max_logit_length, num_tokens]\n        \"\"\"\n        alpha_gamma_term = self.combine_transition_probabilities(a=self.alpha[:, :-1], b=self.gamma[:, 1:])\n        # shape = [batch_size, max_logit_length, num_tokens, max_logit_length + 1, max_label_length + 1]\n        alpha_gamma_beta_term = \\\n            self.combine_transition_probabilities(a=alpha_gamma_term[:, :, :, :-1], b=self.beta[:, 1:])\n        # shape = [batch_size, max_logit_length, num_tokens, max_logit_length, num_tokens]\n        alpha_gamma_beta_loss_term = expand_many_dims(self.loss, axes=[1, 2, 3, 4]) + alpha_gamma_beta_term\n        # shape = [batch_size, max_logit_length, num_tokens]\n        logit_length_x_num_tokens = self.max_logit_length * self.num_tokens\n        first_term = tf.reshape(\n            tf.linalg.set_diag(\n                input=tf.reshape(\n                    tensor=alpha_gamma_beta_loss_term,\n                    shape=[self.batch_size, logit_length_x_num_tokens, logit_length_x_num_tokens]\n                ),\n                diagonal=tf.reshape(\n                    tensor=self.logarithmic_logproba_gradient,\n                    shape=[self.batch_size, logit_length_x_num_tokens]\n                )\n            ),\n            shape=tf.shape(alpha_gamma_beta_term),\n        )\n\n        mask = expand_many_dims(\n            input=tf.linalg.band_part(tf.ones(shape=[self.max_logit_length] * 2, dtype=tf.bool), 0, -1),\n            axes=[0, 2, 4]\n        )\n        symmetrized_first_term = tf.where(\n            condition=mask,\n            x=first_term,\n            y=tf.transpose(first_term, [0, 3, 4, 1, 2]),\n        )\n        # shape = [batch_size, max_logit_length, num_tokens, max_logit_length, num_tokens]\n        hessian = \\\n            -tf.exp(symmetrized_first_term) \\\n            + expand_many_dims(self.gradient, [3, 4]) * expand_many_dims(self.gradient, [1, 2])\n        # shape = [batch_size, max_logit_length, num_tokens, max_logit_length, num_tokens]\n\n        # Filter out samples with infinite loss\n        hessian = tf.where(\n            condition=expand_many_dims(self.loss == inf, [1, 2, 3, 4]),\n            x=tf.zeros(shape=[1, 1, 1, 1, 1]),\n            y=hessian,\n        )\n        # shape = [batch_size, max_logit_length, num_tokens, max_logit_length, num_tokens]\n\n        # Filter out logits that beyond logits length\n        hessian = tf.where(\n            condition=expand_many_dims(self.logit_length_mask, axes=[2, 3, 4]),\n            x=hessian,\n            y=0.\n        )\n        hessian = tf.where(\n            condition=expand_many_dims(self.logit_length_mask, axes=[1, 2, 4]),\n            x=hessian,\n            y=0.\n        )\n\n        return hessian\n\n    @cached_property\n    def gradient(self) -> tf.Tensor:\n        # shape = [batch_size, max_logit_length, num_tokens]\n        return -tf.exp(self.logarithmic_logproba_gradient)\n\n    @cached_property\n    def logarithmic_logproba_gradient(self) -> tf.Tensor:\n        \"\"\"Calculates logarithmic gradient of log loss w.r.t. input logarithmic probabilities.\n\n        Returns: tf.Tensor, shape = [batch_size, max_logit_length, num_tokens]\n        \"\"\"\n        logarithmic_logproba_gradient = \\\n            tf.reshape(self.loss, [-1, 1, 1]) \\\n            + self.combine_transition_probabilities(a=self.alpha[:, :-1], b=self.beta[:, 1:])\n        # shape = [batch_size, max_logit_length, num_tokens]\n\n        # Filter out samples infinite loss\n        logarithmic_logproba_gradient = tf.where(\n            condition=expand_many_dims(self.loss == inf, [1, 2]),\n            x=-inf,\n            y=logarithmic_logproba_gradient,\n        )\n        # shape = [batch_size, max_logit_length, num_tokens]\n\n        # Filter out logits that beyond logits length\n        logarithmic_logproba_gradient = apply_logarithmic_mask(\n            tensor=logarithmic_logproba_gradient,\n            mask=tf.expand_dims(self.logit_length_mask, axis=2),\n        )\n        # shape = [batch_size, max_logit_length, num_tokens]\n\n        return logarithmic_logproba_gradient\n\n    @property\n    @abstractmethod\n    def alpha(self) -> tf.Tensor:\n        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, ...]\n        raise NotImplementedError()\n\n    @property\n    @abstractmethod\n    def beta(self) -> tf.Tensor:\n        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, ...]\n        raise NotImplementedError()\n\n    @property\n    @abstractmethod\n    def gamma(self) -> tf.Tensor:\n        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, ...,\n        #   max_logit_length + 1, max_label_length + 1, ...]\n        raise NotImplementedError()\n\n    @cached_property\n    def expected_token_logproba(self) -> tf.Tensor:\n        \"\"\"Logarithmic probability to predict label token.\n\n        Returns:shape = [batch_size, max_logit_length, max_label_length + 1]\n        \"\"\"\n        label_logproba = tf.gather(\n            params=self.logproba,\n            indices=self.label,\n            axis=2,\n            batch_dims=1,\n        )\n        expected_token_logproba = \\\n            apply_logarithmic_mask(label_logproba, tf.expand_dims(self.label_length_mask, axis=1))\n        # shape = [batch_size, max_logit_length, max_label_length + 1]\n        return expected_token_logproba\n\n    @property\n    @abstractmethod\n    def loss(self) -> tf.Tensor:\n        \"\"\"Samplewise loss function value that is minus logarithmic probability to predict label sequence.\n\n        Returns:    tf.Tensor, shape = [batch_size]\n        \"\"\"\n        raise NotImplementedError()\n\n    @cached_property\n    def label_token_logproba(self) -> tf.Tensor:\n        \"\"\" shape = [batch_size, max_logit_length, max_label_length + 1] \"\"\"\n        return tf.gather(\n            params=self.logproba,\n            indices=self.label,\n            axis=2,\n            batch_dims=1,\n        )\n\n    @cached_property\n    def blank_logproba(self):\n        \"\"\"Calculates logarithmic probability to predict blank token for given logit.\n\n        Returns:    tf.Tensor, shape = [batch_size, max_logit_length]\n        \"\"\"\n        return self.logproba[:, :, self.blank_token_index]\n\n    @cached_property\n    def input_proba(self) -> tf.Tensor:\n        \"\"\" shape = [batch_size, input_logit_tensor_length, num_tokens], dtype = tf.float32 \"\"\"\n        return tf.exp(self.logproba)\n\n    @cached_property\n    def logproba(self) -> tf.Tensor:\n        mask = tf.expand_dims(tf.sequence_mask(lengths=self._logit_length, maxlen=self.max_logit_length), 2)\n        blank_logprobas = tf.reshape(tf.math.log(tf.one_hot(self.blank_token_index, self.num_tokens)), shape=[1, 1, -1])\n        logprobas = tf.where(\n            condition=mask,\n            x=self._logprobas,\n            y=blank_logprobas,\n        )\n        return logprobas\n\n    '''\n    def cleaned_label(self) -> tf.Tensor:\n        \"\"\" shape = [batch, max_label_length + 1] \"\"\"\n        _ = self.max_label_length_plus_one\n    '''\n    @cached_property\n    def cleaned_label(self):\n        # Repair padding- apparently, TPU/ GPU jit cannot handle the padding here; I'm not sure why. Anyway, it does not seem necessary in our case.\n        labels = self._original_label[:, :self.max_label_length_plus_one]\n        '''\n        labels = tf.cond(\n            pred=tf.shape(self._original_label)[1] > self.max_label_length,\n            true_fn=lambda: self._original_label[:, :self.max_label_length_plus_one],\n            false_fn=lambda: pad_until(\n                tensor=self._original_label,\n                desired_size=self.max_label_length_plus_one,\n                pad_value=self.pad_token_index,\n                axis=1\n            )\n        )\n        '''\n        mask = tf.sequence_mask(lengths=self._original_label_length, maxlen=tf.shape(labels)[1])\n        blank_label = tf.ones_like(labels) * self.pad_token_index\n        cleaned_label = tf.where(\n            condition=mask,\n            x=labels,\n            y=blank_label,\n        )\n        return cleaned_label\n\n    def select_from_act(self, act: tf.Tensor, label: tf.Tensor) -> tf.Tensor:\n        \"\"\"Takes tensor of acts act_{b, a, t, u, ...} and labels label_{b,u},\n        where b is the batch index, t is the logit index, and u is the label index,\n        and returns for each token index k the tensor\n\n            output_{b,a,t,k,...} = logsumexp_u act_{b,a,t,u_k,...} * kroneker_delta(u_k = label_{b,u})\n\n        that is logarithmic sum of exponents of acts for all u_k = label_{b,u}, given b, t and k.\n\n        Args:\n            act:    tf.Tensor, shape = [batch_size, dim_a, max_logit_length, max_label_length + 1, ...]\n            label:  tf.Tensor, shape = [batch_size, max_label_length + 1]\n\n        Returns:    tf.Tensor, shape = [batch_size, max_label_length + 1, num_tokens, ...]\n        \"\"\"\n        data = smart_transpose(a=act, perm=[0, 3, 2, 1])\n        # shape = [batch_size, max_label_length + 1, max_logit_length, dim_a, ...]\n        data = tf.squeeze(\n            input=smart_reshape(\n                tensor=data,\n                shape=[1, self.batch_size * self.max_label_length_plus_one, self.max_logit_length]\n            ),\n            axis=0\n        )\n        # shape = [batch_size * (max_label_length + 1), max_logit_length, dim_a, ...]\n\n        segment_ids = tf.reshape(label + tf.expand_dims(tf.range(self.batch_size), 1) * self.num_tokens, shape=[-1])\n        # shape = [batch_size * (max_label_length + 1)]\n        num_segments = self.batch_size * self.num_tokens\n\n        output = unsorted_segment_logsumexp(data=data, segment_ids=segment_ids, num_segments=num_segments)\n        # shape = [batch_size * num_tokens, max_logit_length, dim_a, ...]\n        output = smart_reshape(tf.expand_dims(output, 0), [self.batch_size, self.num_tokens, self.max_logit_length])\n        # shape = [batch_size, num_tokens, max_logit_length, dim_a, ...]\n        output = smart_transpose(output, [0, 3, 2, 1])\n        # shape = [batch_size, dim_a, max_logit_length, num_tokens, ...]\n        return output\n\n    @cached_property\n    def max_logit_length_plus_one(self) -> tf.Tensor:\n        return self.max_logit_length + tf.constant(1, dtype=tf.int32)\n\n    @cached_property\n    def max_logit_length(self) -> tf.Tensor:\n        return tf.shape(self._logprobas)[1]\n\n    @cached_property\n    def max_label_length_plus_one(self) -> tf.Tensor:\n        return self.max_label_length + tf.constant(1, dtype=tf.int32)\n\n    @cached_property\n    def max_label_length(self) -> tf.Tensor:\n        return reduce_max_with_default(self._original_label_length, default=tf.constant(0, dtype=tf.int32))\n\n    @cached_property\n    def pad_token_index(self) -> tf.Tensor:\n        return self.blank_token_index\n\n    @cached_property\n    def num_tokens(self) -> tf.Tensor:\n        return tf.shape(self._logprobas)[2]\n\n    @cached_property\n    def blank_token_index(self) -> tf.Tensor:\n        return self._blank_index\n\n    @cached_property\n    def logit_length_mask(self) -> tf.Tensor:\n        \"\"\" shape = [batch_size, max_logit_length] \"\"\"\n        return tf.sequence_mask(\n            lengths=self._logit_length,\n            maxlen=self.max_logit_length,\n        )\n\n    @cached_property\n    def label_length_mask(self) -> tf.Tensor:\n        \"\"\" shape = [batch_size, max_label_length + 1], dtype = tf.bool \"\"\"\n        return tf.sequence_mask(lengths=self.label_length, maxlen=self.max_label_length_plus_one)\n\n    @property\n    def label_length(self) -> tf.Tensor:\n        return self._original_label_length\n\n    @cached_property\n    def preceded_label(self) -> tf.Tensor:\n        \"\"\"Preceded label. For example, for label \"abc_\" the sequence \"_abc\" is returned.\n\n        Returns:    tf.Tensor, shape = [batch_size, max_label_length + 1]\n        \"\"\"\n        return tf.roll(self.label, shift=1, axis=1)\n\n    @cached_property\n    def label(self) -> tf.Tensor:\n        \"\"\" shape = [batch, max_label_length + 1] \"\"\"\n        return self.cleaned_label\n\n    @cached_property\n    def batch_size(self) -> tf.Tensor:\n        return tf.shape(self._logprobas)[0]\n\n    @abstractmethod\n    def combine_transition_probabilities(self, a: tf.Tensor, b: tf.Tensor) -> tf.Tensor:\n        \"\"\"Given logarithmic probabilities a and b are merges like\n            a, b -> log( exp a exp p exp b )\n        \"\"\"\n        raise NotImplementedError()\n\n\ndef classic_ctc_loss(\n        labels: tf.Tensor,\n        logits: tf.Tensor,\n        label_length: tf.Tensor,\n        logit_length: tf.Tensor,\n        blank_index: Union[int, tf.Tensor] = 0,\n) -> tf.Tensor:\n    \"\"\"Computes CTC (Connectionist Temporal Classification) loss from\n    http://www.cs.toronto.edu/~graves/icml_2006.pdf.\n\n    Repeated non-blank labels will be merged.\n    For example, predicted sequence\n        a_bb_ccc_cc\n    corresponds to label\n        abcc\n    where \"_\" is the blank token.\n\n    If label length is longer then the logit length the output loss for the corresponding sample in the batch\n    is +tf.inf and the gradient is 0. For example, for label \"abb\" at least 4 tokens are needed.\n    It is because the output sequence must be at least \"ab_b\" in order to handle the repeated token.\n\n    Args:\n        labels:         tf.Tensor, shape = [batch, max_label_length],       dtype = tf.int32\n        logits:         tf.Tensor, shape = [batch, max_length, mum_tokens], dtype = tf.float32\n        label_length:   tf.Tensor, shape = [batch],                         dtype = tf.int32\n        logit_length:   tf.Tensor, shape = [batch],                         dtype = tf.int32\n        blank_index:    tf.Tensor or pythonic static integer between 0 <= blank_index < mum_tokens\n\n    Returns:            tf.Tensor, shape = [batch, max_length, mum_tokens], dtype = tf.float32\n    \"\"\"\n    return ctc_loss(\n        labels=labels,\n        logits=logits,\n        label_length=label_length,\n        logit_length=logit_length,\n        blank_index=blank_index,\n        ctc_loss_data_cls=ClassicCtcLossData\n    )\n\n\nclass ClassicCtcLossData(BaseCtcLossData):\n    \"\"\"Calculate loss data for CTC (Connectionist Temporal Classification) loss from\n    http://www.cs.toronto.edu/~graves/icml_2006.pdf.\n\n    This loss is actually the logarithmic likelihood for the classification task with multiple expected class.\n    All predicated sequences consist of tokens (denoted like \"a\", \"b\", ... below) and the blank \"_\".\n    The classic CTC decoding merges all repeated non-blank labels and removes the blank.\n    For example, predicted sequence\n        a_bb_ccc_c is decoded as \"abcc\".\n    All predicated sequences that coincided with the label after the decoding are the expected classes\n    in the logarithmic likelihood loss function.\n\n    Implementation:\n\n    We calculate alpha_{b,t,l,s} and beta_{b,t,l,s} that are the logarithmic probabilities similar to\n    this the ones from the sited paper and defined precisely below.\n    Here, b corresponds to batch, t to logit position, l to label index, and s=0,1 to state (see below for details).\n\n    During the decoding procedure, after handling of a part of the logit sequence,\n    we predict only a part of the target label tokens. We call this subsequence the in the target space as \"state\".\n    For example, two decode label \"abc\" we have to decode \"a\" first then add \"b\" and move tot the state \"ab\" and\n    then to the state \"abc\".\n\n    In order to handle the token duplication swap in the classic CTC loss we extend the set of all possible labels.\n    For each token sequence we define two sequences called \"closed\" and \"open\".\n    For example, for label \"abc\" we consider its two states denoted \"abc>\" (closed) and \"abc<\" (open).\n    The difference between them is in their behaviour with respect to the token appending. The rules are:\n        \"...a>\" + \"_\" -> \"...a>\",\n        \"...a<\" + \"_\" -> \"...a>\",\n        \"...a>\" + \"a\" -> \"...aa<\",\n        \"...a<\" + \"a\" -> \"...a<\",\n        \"...a>\" + \"b\" -> \"...ab<\",\n        \"...a<\" + \"b\" -> \"...ab<\",\n    for any different tokens \"a\" and \"b\" and any token sequence denoted by \"...\".\n    Namely, appending a token the is equal to the last one to an open state does not change this state.\n    Appending a blank to a state always males this state closed.\n\n    This is why alpha_{b,t,l,s} and beta_{b,t,l,s} in the code below are equipped with an additional index s=0,1.\n    Closed states corresponds s=0 and open ones to s=1.\n\n    In particular, the flowing identity is satisfied\n        sum_s sum_l exp alpha_{b,t,l,s} * exp beta_{b,t,l,s} = loss_{b}, for any b and t\n    \"\"\"\n    @cached_property\n    def diagonal_non_blank_grad_term(self) -> tf.Tensor:\n        \"\"\" shape = [batch_size, max_logit_length, num_tokens] \"\"\"\n        input_tensor = \\\n            self.alpha[:, :-1] \\\n            + self.any_to_open_diagonal_step_log_proba \\\n            + tf.roll(self.beta[:, 1:, :, 1:], shift=-1, axis=2)\n        # shape = [batch_size, max_logit_length, max_label_length + 1, states]\n        act = tf.reduce_logsumexp(\n            input_tensor=input_tensor,\n            axis=3,\n        )\n        # shape = [batch_size, max_logit_length, max_label_length + 1]\n        diagonal_non_blank_grad_term = self.select_from_act(act=act, label=self.label)\n        # shape = [batch_size, max_logit_length, num_tokens]\n        return diagonal_non_blank_grad_term\n\n    @cached_property\n    def horizontal_non_blank_grad_term(self) -> tf.Tensor:\n        \"\"\"Horizontal steps from repeated token: open alpha state to open beta state.\n\n        Returns: shape = [batch_size, max_logit_length, num_tokens]\n        \"\"\"\n        act = self.alpha[:, :-1, :, 1] + self.previous_label_token_log_proba + self.beta[:, 1:, :, 1]\n        # shape = [batch_size, max_logit_length, max_label_length + 1]\n        horizontal_non_blank_grad_term = self.select_from_act(act, self.preceded_label)\n        return horizontal_non_blank_grad_term\n\n    @cached_property\n    def loss(self) -> tf.Tensor:\n        \"\"\" shape = [batch_size] \"\"\"\n        params = tf.reduce_logsumexp(self.alpha[:, -1], -1)\n        # shape = [batch_size, max_label_length + 1]\n        loss = -tf.gather(\n            params=params,                # shape = [batch_size, max_label_length + 1]\n            indices=self.label_length,    # shape = [batch_size]\n            batch_dims=1,\n        )\n        return loss\n\n    @cached_property\n    def gamma(self) -> tf.Tensor:\n        \"\"\" shape = [\n                batch_size,\n                max_logit_length + 1,\n                max_label_length + 1,\n                state,\n                max_logit_length + 1,\n                max_label_length + 1,\n                state,\n            ],\n        \"\"\"\n        # This is to avoid InaccessibleTensorError in graph mode\n        _, _, _ = self.horizontal_step_log_proba, self.any_to_open_diagonal_step_log_proba, self.diagonal_gamma\n\n        gamma_forward_transposed = unfold(\n            init_tensor=self.diagonal_gamma,\n            # init_tensor=tf.tile(self.diagonal_gamma, [self.batch_size, self.max_logit_length_plus_one, 1, 1, 1, 1]),\n            iterfunc=self.gamma_step,\n            d_i=1,\n            num_iters=self.max_logit_length,\n            element_shape=tf.TensorShape([None, None, None, None, None, None]),\n            name=\"gamma_1\",\n        )\n        # shape = [max_logit_length + 1, batch_size, max_logit_length + 1, max_label_length + 1, state,\n        #   max_label_length + 1, state]\n\n        gamma_forward = tf.transpose(gamma_forward_transposed, [1, 2, 3, 4, 0, 5, 6])\n        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, state,\n        #   max_logit_length + 1, max_label_length + 1, state]\n\n        mask = expand_many_dims(\n            input=tf.linalg.band_part(tf.ones(shape=[self.max_logit_length_plus_one] * 2, dtype=tf.bool), 0, -1),\n            axes=[0, 2, 3, 5, 6]\n        )\n        # shape = [1, max_logit_length + 1, 1, 1, max_logit_length + 1, 1, 1]\n        gamma = apply_logarithmic_mask(gamma_forward, mask)\n        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, state,\n        #   max_logit_length + 1, max_label_length + 1, state]\n\n        return gamma\n\n    def gamma_step(\n        self,\n        previous_slice: tf.Tensor,\n        i: tf.Tensor,\n    ) -> tf.Tensor:\n        \"\"\"Args:\n            previous_slice: tf.Tensor,\n                            shape = [batch_size, max_logit_length + 1, max_label_length + 1, state,\n                                max_label_length + 1, state]\n            i:              tf.Tensor,\n                            shape = [], 0 <= i < max_logit_length + 1\n\n        Returns:            tf.Tensor,\n                            shape = [batch_size, max_logit_length + 1, max_label_length + 1, state,\n                                max_label_length + 1, state]\n        \"\"\"\n        horizontal_step_states = \\\n            expand_many_dims(self.horizontal_step_log_proba[:, i], axes=[1, 2, 3]) \\\n            + tf.expand_dims(previous_slice, 5)\n        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, state,\n        #          max_label_length + 1, next_state, previous_state]\n        horizontal_step = tf.reduce_logsumexp(horizontal_step_states, axis=6)\n        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, state, max_label_length + 1, state]\n\n        diagonal_step_log_proba = tf.reduce_logsumexp(\n            expand_many_dims(self.any_to_open_diagonal_step_log_proba[:, i], axes=[1, 2, 3]) + previous_slice,\n            axis=5\n        )\n        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, state, max_label_length + 1]\n\n        # We move by one token because it is a diagonal step\n        moved_diagonal_step_log_proba = tf.roll(diagonal_step_log_proba, shift=1, axis=4)\n        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, state, max_label_length + 1]\n\n        # Out state is always open:\n        diagonal_step = tf.pad(\n            tensor=tf.expand_dims(moved_diagonal_step_log_proba, 5),\n            paddings=[[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [1, 0]],\n            constant_values=-np.inf\n        )\n        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, state, max_label_length + 1, state]\n        new_gamma_slice = logsumexp(\n            x=horizontal_step,\n            y=diagonal_step,\n        )\n        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, state, max_label_length + 1, state]\n\n        condition = tf.reshape(tf.range(self.max_logit_length_plus_one) <= i, shape=[1, -1, 1, 1, 1, 1])\n        # shape = [1, max_logit_length + 1, 1, 1, 1, 1, 1]\n        output_slice = tf.where(\n            condition=condition,\n            x=new_gamma_slice,\n            y=self.diagonal_gamma,\n        )\n        # shape = [batch_size, max_logit_length + 1, max_label_length + 1, state, max_label_length + 1, state]\n\n        return output_slice\n\n    @cached_property\n    def diagonal_gamma(self) -> tf.Tensor:\n        \"\"\" shape = [batch_size, max_logit_length_plus_one, max_label_length + 1, state,\n                     max_label_length + 1, state]\n        \"\"\"\n        diagonal_gamma = tf.math.log(\n            tf.reshape(\n                tensor=tf.eye(self.max_label_length_plus_one * 2, dtype=tf.float32),\n                shape=[1, 1, self.max_label_length_plus_one, 2, self.max_label_length_plus_one, 2]\n            )\n        )\n        diagonal_gamma = tf.tile(diagonal_gamma, [self.batch_size, self.max_logit_length_plus_one, 1, 1, 1, 1])\n        return diagonal_gamma\n\n    @cached_property\n    def beta(self) -> tf.Tensor:\n        \"\"\"Calculates the beta_{b,t,l,s} that is logarithmic probability of sample 0 <= b < batch_size - 1 in the batch\n        with logit subsequence from\n            t, t + 1, ... max_logit_length - 2, max_logit_length - 1,\n        for t < max_logit_length\n        to predict the sequence of tokens\n            w_max_label_length, w_{max_label_length + 1}, ... w_{max_label_length - 2}, w_{max_label_length - 1}\n        for l < max_label_length\n        that is either closed s=0 or open s=1.\n        from label_b = [w_0, w_1, ... w_{max_label_length - 2}, w_{max_label_length - 1}].\n\n        This logarithmic probability is calculated by iterations\n            exp beta_{t-1,l} = p_horizontal_step_{t-1,l} * exp beta_{t,l} + p_diagonal_step_{t-1,l} * exp beta_{t,l+1},\n        for 0 <= t < max_logit_length,\n        where p_diagonal_step_{t,l} is the probability to predict label token w_l with logit l\n        and p_horizontal_step_{t,l} is the probability to skip token w_l prediction with logit l, for example, with\n        the blank prediction.\n\n        Returns:    tf.Tensor,  shape = [batch_size, max_logit_length + 1, max_label_length + 1, state],\n                    dtype = tf.float32\n        \"\"\"\n        # This is to avoid InaccessibleTensorError in graph mode\n        _, _ = self.horizontal_step_log_proba, self.any_to_open_diagonal_step_log_proba\n\n        beta = unfold(\n            init_tensor=self.last_beta_slice,\n            iterfunc=self.beta_step,\n            d_i=-1,\n            num_iters=self.max_logit_length,\n            element_shape=tf.TensorShape([None, None, 2]),\n            name=\"beta\",\n        )\n        # shape = [logit_length + 1, batch, label_length + 1, state]\n        return tf.transpose(beta, [1, 0, 2, 3])\n\n    def beta_step(self, previous_slice: tf.Tensor, i: tf.Tensor) -> tf.Tensor:\n        \"\"\" shape = [batch_size, max_label_length + 1, state] \"\"\"\n        horizontal_step = \\\n            tf.reduce_logsumexp(self.horizontal_step_log_proba[:, i] + tf.expand_dims(previous_slice, 3), 2)\n        # shape = [batch_size, max_label_length + 1, state]\n        diagonal_step = \\\n            self.any_to_open_diagonal_step_log_proba[:, i] + tf.roll(previous_slice[:, :, 1:], shift=-1, axis=1)\n        # shape = [batch_size, max_label_length + 1, state]\n        new_beta_slice = logsumexp(\n            x=horizontal_step,  # shape = [batch_size, max_label_length + 1, state]\n            y=diagonal_step,    # shape = [batch_size, max_label_length + 1, state]\n        )\n        # shape = [batch_size, max_label_length + 1, state]\n        return new_beta_slice\n\n    @cached_property\n    def last_beta_slice(self) -> tf.Tensor:\n        \"\"\" shape = [batch_size, max_label_length + 1, state] \"\"\"\n        beta_last = tf.math.log(tf.one_hot(indices=self.label_length, depth=self.max_label_length_plus_one))\n        beta_last = tf.tile(input=tf.expand_dims(beta_last, axis=2), multiples=[1, 1, 2])\n        return beta_last\n\n    @cached_property\n    def alpha(self) -> tf.Tensor:\n        \"\"\"Calculates the alpha_{b,t,l,s} that is\n        the logarithmic probability of sample 0 <= b < batch_size - 1 in the batch\n        with logits subsequence from 0, 1, 2, ... t - 2, t - 1, for t < max_logit_length\n        to predict the sequence of tokens w_0, w_1, w_2, ... w_{l-2}, w_{l-1} for l < max_label_length + 1\n        that is either closed s=0 or open s=1.\n        from label_b = [w_0, w_1, ... w_{max_label_length - 2}, w_{max_label_length - 1}].\n\n        This logarithmic probability is calculated by iterations\n            exp alpha_{t + 1,l} = p_horizontal_step_{t,l} * exp alpha_{t,l} + p_diagonal_step_{t,l} * exp alpha_{t,l-1},\n        for 0 <= t < max_logit_length,\n        where p_diagonal_step_{t,l} is the probability to predict label token w_l with logit l\n        and p_horizontal_step_{t,l} is the probability to skip token w_l prediction with logit l, for example, with\n        the blank prediction.\n\n        Returns:    tf.Tensor,  shape = [batch_size, max_logit_length + 1, max_label_length + 1, state],\n                    dtype = tf.float32\n        \"\"\"\n        # This is to avoid InaccessibleTensorError in graph mode\n        _, _ = self.horizontal_step_log_proba, self.any_to_open_diagonal_step_log_proba\n\n        alpha = unfold(\n            init_tensor=self.first_alpha_slice,\n            iterfunc=self.alpha_step,\n            d_i=1,\n            num_iters=self.max_logit_length,\n            element_shape=tf.TensorShape([None, None, 2]),\n            name=\"alpha\",\n        )\n        # shape = [logit_length + 1, batch_size, label_length + 1, state]\n        return tf.transpose(alpha, [1, 0, 2, 3])\n\n    def alpha_step(self, previous_slice: tf.Tensor, i: tf.Tensor) -> tf.Tensor:\n        \"\"\"Args:\n            previous_slice: shape = [batch_size, max_label_length + 1, state]\n            i:\n\n        Returns:            shape = [batch_size, max_label_length + 1, state]\n        \"\"\"\n        temp = self.horizontal_step_log_proba[:, i] + tf.expand_dims(previous_slice, 2)\n        # shape = [batch_size, max_label_length + 1, next_state, previous_state]\n        horizontal_step = tf.reduce_logsumexp(temp, 3)\n        # shape = [batch_size, max_label_length + 1, state]\n        diagonal_step_log_proba = \\\n            tf.reduce_logsumexp(self.any_to_open_diagonal_step_log_proba[:, i] + previous_slice, 2)\n        # shape = [batch_size, max_label_length + 1]\n\n        # We move by one token because it is a diagonal step\n        moved_diagonal_step_log_proba = tf.roll(diagonal_step_log_proba, shift=1, axis=1)\n        # shape = [batch_size, max_label_length + 1]\n\n        # Out state is always open:\n        diagonal_step = tf.pad(\n            tensor=tf.expand_dims(moved_diagonal_step_log_proba, 2),\n            paddings=[[0, 0], [0, 0], [1, 0]],\n            constant_values=-np.inf\n        )\n        # shape = [batch_size, max_label_length + 1, state]\n        new_alpha_slice = logsumexp(\n            x=horizontal_step,\n            y=diagonal_step,\n        )\n        # shape = [batch_size, max_label_length + 1, state]\n        return new_alpha_slice\n\n    @cached_property\n    def first_alpha_slice(self) -> tf.Tensor:\n        \"\"\" shape = [batch_size, max_label_length + 1, state] \"\"\"\n        alpha_0 = tf.math.log(tf.one_hot(indices=0, depth=self.max_label_length_plus_one * 2))\n        alpha_0 = tf.tile(input=tf.reshape(alpha_0, [1, -1, 2]), multiples=[self.batch_size, 1, 1])\n        return alpha_0\n\n    @cached_property\n    def any_to_open_diagonal_step_log_proba(self) -> tf.Tensor:\n        \"\"\"Logarithmic probability to make a diagonal step from given state to an open state\n\n        Returns:shape = [batch_size, max_logit_length, max_label_length + 1, state]\n        \"\"\"\n        return tf.stack(\n            values=[self.closed_to_open_diagonal_step_log_proba, self.open_to_open_diagonal_step_log_proba],\n            axis=3\n        )\n\n    @cached_property\n    def open_to_open_diagonal_step_log_proba(self) -> tf.Tensor:\n        \"\"\"Logarithmic probability to make a diagonal step from an open state to an open state\n        with expected token prediction that is different from the previous one.\n\n        Returns:shape = [batch_size, max_logit_length, max_label_length + 1]\n        \"\"\"\n        # We check that the predicting token does not equal to previous one\n        token_repetition_mask = self.label != tf.roll(self.label, shift=1, axis=1)\n        # shape = [batch_size, max_label_length + 1]\n        open_diagonal_step_log_proba = \\\n            apply_logarithmic_mask(\n                self.closed_to_open_diagonal_step_log_proba,\n                tf.expand_dims(token_repetition_mask, axis=1)\n            )\n        return open_diagonal_step_log_proba\n\n    @cached_property\n    def closed_to_open_diagonal_step_log_proba(self) -> tf.Tensor:\n        \"\"\"Logarithmic probability to make a diagonal step from a closed state to an open state\n        with expected token prediction.\n\n        Returns:shape = [batch_size, max_logit_length, max_label_length + 1]\n        \"\"\"\n        return self.expected_token_logproba\n\n    @cached_property\n    def horizontal_step_log_proba(self) -> tf.Tensor:\n        \"\"\"Calculates logarithmic probability of the horizontal step for given logit x label position.\n\n        This is possible in two alternative cases:\n        1. Blank\n        2. Not blank token from previous label position.\n\n        Returns: tf.Tensor, shape = [batch_size, max_logit_length, max_label_length + 1, next_state, previous_state]\n        \"\"\"\n        # We map closed and open states to closed states\n        blank_term = tf.tile(\n            input=tf.expand_dims(tf.expand_dims(self.blank_logproba, 2), 3),\n            multiples=[1, 1, self.max_label_length_plus_one, 2]\n        )\n        # shape = [batch_size, max_logit_length, max_label_length + 1, 2]\n        non_blank_term = tf.pad(\n            tf.expand_dims(self.not_blank_horizontal_step_log_proba, 3),\n            paddings=[[0, 0], [0, 0], [0, 0], [1, 0]],\n            constant_values=tf.constant(-np.inf),\n        )\n        # shape = [batch_size, max_logit_length, max_label_length + 1, 2]\n        horizontal_step_log_proba = tf.stack([blank_term, non_blank_term], axis=3)\n        return horizontal_step_log_proba\n\n    @cached_property\n    def not_blank_horizontal_step_log_proba(self) -> tf.Tensor:\n        \"\"\" shape = [batch_size, max_logit_length, max_label_length + 1] \"\"\"\n        mask = tf.reshape(1 - tf.one_hot(self.blank_token_index, depth=self.num_tokens), shape=[1, 1, -1])\n        not_blank_log_proba = apply_logarithmic_mask(self.logproba, mask)\n        not_blank_horizontal_step_log_proba = tf.gather(\n            params=not_blank_log_proba,\n            indices=tf.roll(self.label, shift=1, axis=1),\n            axis=2,\n            batch_dims=1,\n        )\n        # shape = [batch_size, max_logit_length, max_label_length + 1]\n        return not_blank_horizontal_step_log_proba\n\n    @cached_property\n    def previous_label_token_log_proba(self) -> tf.Tensor:\n        \"\"\"Calculates the probability to predict token that preceded to label token.\n\n        Returns:    tf.Tensor,  shape = [batch_size, max_logit_length, max_label_length + 1]\n        \"\"\"\n        previous_label_token_log_proba = tf.gather(\n            params=self.logproba,\n            indices=self.preceded_label,\n            axis=2,\n            batch_dims=1,\n        )\n        # shape = [batch_size, max_logit_length, max_label_length + 1]\n        return previous_label_token_log_proba\n\n    @cached_property\n    def blank_logproba(self) -> tf.Tensor:\n        \"\"\" shape = [batch_size, max_logit_length] \"\"\"\n        return self.logproba[:, :, self.blank_token_index]\n\n    def combine_transition_probabilities(self, a: tf.Tensor, b: tf.Tensor) -> tf.Tensor:\n        \"\"\"Transforms logarithmic transition probabilities a and b.\n\n        Args:\n            a:      shape = [batch, DIMS_A, max_logit_length, max_label_length + 1, state]\n            b:      shape = [batch, max_logit_length, max_label_length + 1, state, DIMS_B]\n\n        Returns:    shape = [batch, DIMS_A, max_logit_length, num_tokens, DIMS_B]\n        \"\"\"\n        assert len(a.shape) >= 4\n        assert len(b.shape) >= 4\n        assert a.shape[-1] == 2\n        assert b.shape[3] == 2\n\n        dims_a = tf.shape(a)[1:-3]\n        dims_b = tf.shape(b)[4:]\n        a = tf.reshape(a, shape=[self.batch_size, -1, self.max_logit_length, self.max_label_length_plus_one, 2, 1])\n        # shape = [batch_size, dims_a, max_logit_length, max_label_length + 1, state, 1]\n        b = tf.reshape(b, shape=[self.batch_size, 1, self.max_logit_length, self.max_label_length_plus_one, 2, -1])\n        # shape = [batch_size, 1, max_logit_length, max_label_length + 1, state, dims_b]\n\n        # Either open or closed state from alpha and only closed state from beta\n        ab_term = tf.reduce_logsumexp(a, 4) + b[:, :, :, :, 0]\n        # shape = [batch_size, dims_a, max_logit_length, max_label_length + 1, dims_b]\n\n        horizontal_blank_grad_term = \\\n            expand_many_dims(self.blank_logproba, axes=[1, 3]) + tf.reduce_logsumexp(ab_term, axis=3)\n        # shape = [batch_size, dims_a, max_logit_length, dims_b]\n\n        act = a[:, :, :, :, 1] + expand_many_dims(self.previous_label_token_log_proba, axes=[1, 4]) + b[:, :, :, :, 1]\n        # shape = [batch_size, dim_a, max_logit_length, max_label_length + 1, dim_b]\n\n        horizontal_non_blank_grad_term = self.select_from_act(act, self.preceded_label)\n        # shape = [batch_size, dim_a, max_logit_length, num_tokens, dim_b]\n\n        input_tensor = a + expand_many_dims(self.any_to_open_diagonal_step_log_proba, axes=[1, 5]) + \\\n            tf.roll(b[:, :, :, :, 1:], shift=-1, axis=3)\n        # shape = [batch_size, dim_a, max_logit_length, max_label_length + 1, states, dim_b]\n\n        act = tf.reduce_logsumexp(input_tensor=input_tensor, axis=4)\n        # shape = [batch_size, dim_a, max_logit_length, max_label_length + 1, dim_b]\n\n        diagonal_non_blank_grad_term = self.select_from_act(act=act, label=self.label)\n        # shape = [batch_size, dim_a, max_logit_length, num_tokens, dim_b]\n\n        non_blank_grad_term = logsumexp(horizontal_non_blank_grad_term, diagonal_non_blank_grad_term)\n        # shape = [batch_size, dim_a, max_logit_length, num_tokens, dim_b]\n\n        blank_mask = self.blank_token_index == tf.range(self.num_tokens)\n        # shape = [num_tokens]\n\n        output = tf.where(\n            condition=expand_many_dims(blank_mask, axes=[0, 1, 2, 4]),\n            x=tf.expand_dims(horizontal_blank_grad_term, 3),\n            y=non_blank_grad_term,\n        )\n        # shape = [batch, dim_a, max_logit_length, num_tokens, dim_b]\n        output_shape = tf.concat(\n            [\n                tf.expand_dims(self.batch_size, axis=0),\n                dims_a,\n                tf.expand_dims(self.max_logit_length, axis=0),\n                tf.expand_dims(self.num_tokens, axis=0),\n                dims_b\n            ],\n            axis=0\n        )\n        output_reshaped = tf.reshape(output, shape=output_shape)\n        # shape = [batch, DIMS_A, max_logit_length, num_tokens, DIMS_B]\n\n        return output_reshaped","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:33.507607Z","iopub.execute_input":"2023-07-25T05:55:33.508059Z","iopub.status.idle":"2023-07-25T05:55:33.705149Z","shell.execute_reply.started":"2023-07-25T05:55:33.508028Z","shell.execute_reply":"2023-07-25T05:55:33.704002Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Strategy","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\n\ntpu_strategy=None\ntry:\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n\n  tf.config.experimental_connect_to_cluster(tpu)\n  tf.tpu.experimental.initialize_tpu_system(tpu)\n  tpu_strategy = tf.distribute.TPUStrategy(tpu)\n\nexcept ValueError:\n  print(\"Not using TPU\")\n  #raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n\n\n#from tensorflow.python.framework.ops import disable_eager_execution\n#disable_eager_execution()  # LSTM layer can't use bfloat16 unless we do this.","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Asq1MGhs41ZX","outputId":"8d162085-b861-4603-f075-3a9e984d595c","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:33.706925Z","iopub.execute_input":"2023-07-25T05:55:33.707354Z","iopub.status.idle":"2023-07-25T05:55:33.716810Z","shell.execute_reply.started":"2023-07-25T05:55:33.707312Z","shell.execute_reply":"2023-07-25T05:55:33.715508Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Not using TPU\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"TensorFlow v\" + tf.__version__)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rZpjjT02Yi10","outputId":"f4cc9531-4099-4772-c46f-389f5d6f97a9","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:33.718635Z","iopub.execute_input":"2023-07-25T05:55:33.719358Z","iopub.status.idle":"2023-07-25T05:55:33.735504Z","shell.execute_reply.started":"2023-07-25T05:55:33.719317Z","shell.execute_reply":"2023-07-25T05:55:33.733857Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"TensorFlow v2.12.0\n","output_type":"stream"}]},{"cell_type":"code","source":"class MemoryUsageCallbackExtended(tf.keras.callbacks.Callback):\n    \"\"\"Monitor memory usage on epoch begin and end, collect garbage\"\"\"\n\n    # def on_epoch_begin(self, epoch, logs=None):\n    #    print(\"**Epoch {}**\".format(epoch))\n    #    print(\n    #        f\"Memory usage on epoch begin: {int(psutil.Process(os.getpid()).memory_info().rss)/1e9:.2f GB}\"\n    #    )\n\n    def on_epoch_end(self, epoch, logs=None):\n        print(\n            f\"Memory usage on epoch end: {int(psutil.Process(os.getpid()).memory_info().rss)/1e9:.2f} GB\"\n        )\n        gc.collect()","metadata":{"id":"0gUj7mAgYi10","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:33.737640Z","iopub.execute_input":"2023-07-25T05:55:33.738275Z","iopub.status.idle":"2023-07-25T05:55:33.749186Z","shell.execute_reply.started":"2023-07-25T05:55:33.738146Z","shell.execute_reply":"2023-07-25T05:55:33.747957Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# Scheduler","metadata":{"id":"eErQkrlpYi11"}},{"cell_type":"code","source":"\nclass CosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n    \"\"\"A LearningRateSchedule that uses a cosine decay with optional warmup.\n\n    See [Loshchilov & Hutter, ICLR2016](https://arxiv.org/abs/1608.03983),\n    SGDR: Stochastic Gradient Descent with Warm Restarts.\n\n    For the idea of a linear warmup of our learning rate,\n    see [Goyal et al.](https://arxiv.org/pdf/1706.02677.pdf).\n\n    When we begin training a model, we often want an initial increase in our\n    learning rate followed by a decay. If `warmup_target` is an int, this\n    schedule applies a linear increase per optimizer step to our learning rate\n    from `initial_learning_rate` to `warmup_target` for a duration of\n    `warmup_steps`. Afterwards, it applies a cosine decay function taking our\n    learning rate from `warmup_target` to `alpha` for a duration of\n    `decay_steps`. If `warmup_target` is None we skip warmup and our decay\n    will take our learning rate from `initial_learning_rate` to `alpha`.\n    It requires a `step` value to  compute the learning rate. You can\n    just pass a TensorFlow variable that you increment at each training step.\n\n    The schedule is a 1-arg callable that produces a warmup followed by a\n    decayed learning rate when passed the current optimizer step. This can be\n    useful for changing the learning rate value across different invocations of\n    optimizer functions.\n\n    Our warmup is computed as:\n\n    ```python\n    def warmup_learning_rate(step):\n        completed_fraction = step / warmup_steps\n        total_delta = target_warmup - initial_learning_rate\n        return completed_fraction * total_delta\n    ```\n\n    And our decay is computed as:\n\n    ```python\n    if warmup_target is None:\n        initial_decay_lr = initial_learning_rate\n    else:\n        initial_decay_lr = warmup_target\n\n    def decayed_learning_rate(step):\n        step = min(step, decay_steps)\n        cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))\n        decayed = (1 - alpha) * cosine_decay + alpha\n        return initial_decay_lr * decayed\n    ```\n\n    Example usage without warmup:\n\n    ```python\n    decay_steps = 1000\n    initial_learning_rate = 0.1\n    lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate, decay_steps)\n    ```\n\n    Example usage with warmup:\n\n    ```python\n    decay_steps = 1000\n    initial_learning_rate = 0\n    warmup_steps = 1000\n    target_learning_rate = 0.1\n    lr_warmup_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n        initial_learning_rate, decay_steps, warmup_target=target_learning_rate,\n        warmup_steps=warmup_steps\n    )\n    ```\n\n    You can pass this schedule directly into a `tf.keras.optimizers.Optimizer`\n    as the learning rate. The learning rate schedule is also serializable and\n    deserializable using `tf.keras.optimizers.schedules.serialize` and\n    `tf.keras.optimizers.schedules.deserialize`.\n\n    Returns:\n      A 1-arg callable learning rate schedule that takes the current optimizer\n      step and outputs the decayed learning rate, a scalar `Tensor` of the same\n      type as `initial_learning_rate`.\n    \"\"\"\n\n    def __init__(\n        self,\n        initial_learning_rate,\n        decay_steps,\n        alpha=0.0,\n        name=None,\n        warmup_target=None,\n        warmup_steps=0,\n    ):\n        \"\"\"Applies cosine decay to the learning rate.\n\n        Args:\n          initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a\n            Python int. The initial learning rate.\n          decay_steps: A scalar `int32` or `int32` `Tensor` or a Python int.\n            Number of steps to decay over.\n          alpha: A scalar `float32` or `float64` `Tensor` or a Python int.\n            Minimum learning rate value for decay as a fraction of\n            `initial_learning_rate`.\n          name: String. Optional name of the operation.  Defaults to\n            'CosineDecay'.\n          warmup_target: None or a scalar `float32` or `float64` `Tensor` or a\n            Python int. The target learning rate for our warmup phase. Will cast\n            to the `initial_learning_rate` datatype. Setting to None will skip\n            warmup and begins decay phase from `initial_learning_rate`.\n            Otherwise scheduler will warmup from `initial_learning_rate` to\n            `warmup_target`.\n          warmup_steps: A scalar `int32` or `int32` `Tensor` or a Python int.\n            Number of steps to warmup over.\n        \"\"\"\n        super().__init__()\n\n        self.initial_learning_rate = initial_learning_rate\n        self.decay_steps = decay_steps\n        self.alpha = alpha\n        self.name = name\n        self.warmup_steps = warmup_steps\n        self.warmup_target = warmup_target\n\n    def _decay_function(self, step, decay_steps, decay_from_lr, dtype):\n        with tf.name_scope(self.name or \"CosineDecay\"):\n            completed_fraction = step / decay_steps\n            tf_pi = tf.constant(math.pi, dtype=dtype)\n            cosine_decayed = 0.5 * (1.0 + tf.cos(tf_pi * completed_fraction))\n            decayed = (1 - self.alpha) * cosine_decayed + self.alpha\n            return tf.multiply(decay_from_lr, decayed)\n\n    def _warmup_function(self, step, warmup_steps, warmup_target, initial_learning_rate):\n        with tf.name_scope(self.name or \"CosineDecay\"):\n            completed_fraction = step / warmup_steps\n            total_step_delta = warmup_target - initial_learning_rate\n            return total_step_delta * completed_fraction + initial_learning_rate\n\n    def __call__(self, step):\n        with tf.name_scope(self.name or \"CosineDecay\"):\n            initial_learning_rate = tf.convert_to_tensor(\n                self.initial_learning_rate, name=\"initial_learning_rate\"\n            )\n            dtype = initial_learning_rate.dtype\n            decay_steps = tf.cast(self.decay_steps, dtype)\n            global_step_recomp = tf.cast(step, dtype)\n\n            if self.warmup_target is None:\n                global_step_recomp = tf.minimum(global_step_recomp, decay_steps)\n                return self._decay_function(\n                    global_step_recomp,\n                    decay_steps,\n                    initial_learning_rate,\n                    dtype,\n                )\n\n            warmup_target = tf.cast(self.warmup_target, dtype)\n            warmup_steps = tf.cast(self.warmup_steps, dtype)\n\n            global_step_recomp = tf.minimum(global_step_recomp, decay_steps + warmup_steps)\n\n            return tf.cond(\n                global_step_recomp < warmup_steps,\n                lambda: self._warmup_function(\n                    global_step_recomp,\n                    warmup_steps,\n                    warmup_target,\n                    initial_learning_rate,\n                ),\n                lambda: self._decay_function(\n                    global_step_recomp - warmup_steps,\n                    decay_steps,\n                    warmup_target,\n                    dtype,\n                ),\n            )\n\n    def get_config(self):\n        return {\n            \"initial_learning_rate\": self.initial_learning_rate,\n            \"decay_steps\": self.decay_steps,\n            \"alpha\": self.alpha,\n            \"name\": self.name,\n            \"warmup_target\": self.warmup_target,\n            \"warmup_steps\": self.warmup_steps,\n        }\n","metadata":{"id":"umV9OdZCYi11","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:33.751649Z","iopub.execute_input":"2023-07-25T05:55:33.752717Z","iopub.status.idle":"2023-07-25T05:55:33.778908Z","shell.execute_reply.started":"2023-07-25T05:55:33.752675Z","shell.execute_reply":"2023-07-25T05:55:33.777466Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Constants","metadata":{"id":"Vh1J_bDCYi13"}},{"cell_type":"code","source":"def get_char_dict():\n    char_dict = {\n        \" \": 0,\n        \"!\": 1,\n        \"#\": 2,\n        \"$\": 3,\n        \"%\": 4,\n        \"&\": 5,\n        \"'\": 6,\n        \"(\": 7,\n        \")\": 8,\n        \"*\": 9,\n        \"+\": 10,\n        \",\": 11,\n        \"-\": 12,\n        \".\": 13,\n        \"/\": 14,\n        \"0\": 15,\n        \"1\": 16,\n        \"2\": 17,\n        \"3\": 18,\n        \"4\": 19,\n        \"5\": 20,\n        \"6\": 21,\n        \"7\": 22,\n        \"8\": 23,\n        \"9\": 24,\n        \":\": 25,\n        \";\": 26,\n        \"=\": 27,\n        \"?\": 28,\n        \"@\": 29,\n        \"[\": 30,\n        \"_\": 31,\n        \"a\": 32,\n        \"b\": 33,\n        \"c\": 34,\n        \"d\": 35,\n        \"e\": 36,\n        \"f\": 37,\n        \"g\": 38,\n        \"h\": 39,\n        \"i\": 40,\n        \"j\": 41,\n        \"k\": 42,\n        \"l\": 43,\n        \"m\": 44,\n        \"n\": 45,\n        \"o\": 46,\n        \"p\": 47,\n        \"q\": 48,\n        \"r\": 49,\n        \"s\": 50,\n        \"t\": 51,\n        \"u\": 52,\n        \"v\": 53,\n        \"w\": 54,\n        \"x\": 55,\n        \"y\": 56,\n        \"z\": 57,\n        \"~\": 58,\n    }\n    char_dict[\"P\"] = 59\n    #char_dict[\"SOS\"] = 60\n    #char_dict[\"EOS\"] = 61\n    return char_dict\n\n\nclass Constants:\n    ROWS_PER_FRAME = 543\n    MAX_STRING_LEN = 50\n    INPUT_PAD = -100.0\n    char_dict = get_char_dict()\n    LABEL_PAD = char_dict[\"P\"]\n    inv_dict = {v: k for k, v in char_dict.items()}\n    NOSE = [1, 2, 98, 327]\n\n    REYE = [33, 7, 163, 144, 145, 153, 154, 155, 133, 246, 161, 160, 159, 158, 157, 173]\n    LEYE = [263, 249, 390, 373, 374, 380, 381, 382, 362, 466, 388, 387, 386, 385, 384, 398]\n\n    LHAND = list(range(468, 489))\n    RHAND = list(range(522, 543))\n\n    LNOSE = [98]\n    RNOSE = [327]\n\n    LLIP = [84, 181, 91, 146, 61, 185, 40, 39, 37, 87, 178, 88, 95, 78, 191, 80, 81, 82]\n    RLIP = [\n        314,\n        405,\n        321,\n        375,\n        291,\n        409,\n        270,\n        269,\n        267,\n        317,\n        402,\n        318,\n        324,\n        308,\n        415,\n        310,\n        311,\n        312,\n    ]\n    POSE = [500, 502, 504, 501, 503, 505, 512, 513]\n    LPOSE = [513, 505, 503, 501]\n    RPOSE = [512, 504, 502, 500]\n\n    POINT_LANDMARKS_PARTS = [LHAND, RHAND, LLIP, RLIP, LPOSE, RPOSE, NOSE, REYE, LEYE]\n    # POINT_LANDMARKS_PARTS = [LHAND, RHAND, NOSE]\n    POINT_LANDMARKS = [item for sublist in POINT_LANDMARKS_PARTS for item in sublist]\n    parts = {\n        \"LLIP\": LLIP,\n        \"RLIP\": RLIP,\n        \"LHAND\": LHAND,\n        \"RHAND\": RHAND,\n        \"LPOSE\": LPOSE,\n        \"RPOSE\": RPOSE,\n        \"LNOSE\": LNOSE,\n        \"RNOSE\": RNOSE,\n        \"REYE\": REYE,\n        \"LEYE\": LEYE,\n    }\n\n    LANDMARK_INDICES = {}  # type: ignore\n    for part in parts:\n        LANDMARK_INDICES[part] = []\n        for landmark in parts[part]:\n            if landmark in POINT_LANDMARKS:\n                LANDMARK_INDICES[part].append(POINT_LANDMARKS.index(landmark))\n\n    CENTER_LANDMARKS = LNOSE + RNOSE\n    CENTER_INDICES = LANDMARK_INDICES[\"LNOSE\"] + LANDMARK_INDICES[\"RNOSE\"]\n\n    NUM_NODES = len(POINT_LANDMARKS)\n    NUM_INPUT_FEATURES = 2 * NUM_NODES # (x,y)\n    CHANNELS = 6 * NUM_NODES #(x,y,dx,dy,dx2,dy2)\n","metadata":{"id":"5icpG0BeYi13","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:33.781508Z","iopub.execute_input":"2023-07-25T05:55:33.782048Z","iopub.status.idle":"2023-07-25T05:55:33.809978Z","shell.execute_reply.started":"2023-07-25T05:55:33.782007Z","shell.execute_reply":"2023-07-25T05:55:33.808594Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{"id":"MMEA9LflYi14"}},{"cell_type":"code","source":"\n# Seed all random number generators\ndef seed_everything(seed=42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n\ndef selected_columns(file_example):\n    df = pd.read_parquet(file_example)\n    selected_x = df.columns[[x + 1 for x in Constants.POINT_LANDMARKS]].tolist()\n    selected_y = [c.replace(\"x\", \"y\") for c in selected_x]\n    selected = []\n    for i in range(Constants.NUM_NODES):\n        selected.append(selected_x[i])\n        selected.append(selected_y[i])\n    return selected  # x1,y1,x2,y2,...\n\n\n\ndef num_to_char_fn(y):\n    return [Constants.inv_dict.get(x, \"\") for x in y]\n\n\n# A callback class to output a few transcriptions during training\nclass CallbackEval(tf.keras.callbacks.Callback):\n    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n\n    def __init__(self, model, dataset):\n        super().__init__()\n        self.dataset = dataset\n        self.model = model\n\n    def on_epoch_end(self, epoch: int, logs=None):\n        predictions = []\n        targets = []\n        for batch in self.dataset:\n            X, y = batch\n            batch_predictions = self.model(X)\n            batch_predictions = decode_batch_predictions(batch_predictions)\n            predictions.extend(batch_predictions)\n            for label in y:\n                label = \"\".join(num_to_char_fn(label.numpy()))\n                targets.append(label)\n        print(\"-\" * 100)\n        # for i in np.random.randint(0, len(predictions), 2):\n        for i in range(10):\n            print(f\"Target    : {targets[i]}\")\n            print(f\"Prediction: {predictions[i]}, len: {len(predictions[i])}\")\n            print(\"-\" * 100)\n\n\ndef decode_phrase(pred):\n    # decode cts prediction by prunning\n    # (T,CHAR_NUMS)\n    x = tf.argmax(pred, axis=1) # (T,)\n    paddings = tf.constant(\n        [\n            [0, 1],\n        ]\n    )\n    x = tf.pad(x, paddings)\n    diff = tf.not_equal(x[:-1], x[1:])\n    adjacent_indices = tf.where(diff)[:, 0]\n    x = tf.gather(x, adjacent_indices)\n    mask = x != Constants.LABEL_PAD\n    x = tf.boolean_mask(x, mask, axis=0)\n    return x\n\n\n# A utility function to decode the output of the network\ndef decode_batch_predictions(pred):\n    output_text = []\n    for result in pred:\n        result = \"\".join(num_to_char_fn(decode_phrase(result).numpy()))\n        output_text.append(result)\n    return output_text\n\n\n\n\ndef code_to_label(label_code):\n    label = [Constants.inv_dict[x] for x in label_code if Constants.inv_dict[x] != \"P\"]\n    label = \"\".join(label)\n    return label\n\n\ndef convert_to_strings(batch_label_code):\n    output = []\n    for label_code in batch_label_code:\n        output.append(code_to_label(label_code))\n    return output\n\n\ndef global_metric(val_ds, model):\n    global_N, global_D = 0, 0\n    count = 0\n    metric = LevDistanceMetric()\n    for batch in val_ds:\n        count += 1\n        print(count)\n        feature, label = batch\n        logits = model(feature)\n        _, _, D = batch_edit_distance(label, logits)\n        metric.update_state(label, logits)\n\n    result = metric.result().numpy()\n\n    return result\n\n\ndef sparse_from_dense_ignore_value(dense_tensor):\n    mask = tf.not_equal(dense_tensor, Constants.LABEL_PAD)\n    indices = tf.where(mask)\n    values = tf.boolean_mask(dense_tensor, mask)\n    \n    return tf.SparseTensor(indices, values, tf.shape(dense_tensor, out_type=tf.int64))\n\n\ndef batch_edit_distance(y_true, y_logits):\n    blank = Constants.LABEL_PAD\n    #y_true=tf.ensure_shape(y_true,(128,Constants.MAX_STRING_LEN))\n    #y_logits=tf.ensure_shape(y_logits,(128,128,60))\n    #tf.print(\"edit distance true shape\",tf.shape(y_true))\n    #tf.print(\"edit distance logits shape\",tf.shape(y_logits))\n\n    B = tf.shape(y_logits)[0]\n    seq_length = tf.shape(y_logits)[1]\n    to_decode = tf.transpose(y_logits, perm=[1, 0, 2])\n    sequence_length = tf.fill(dims=[B], value=seq_length)\n    hypothesis = tf.nn.ctc_greedy_decoder(\n        tf.cast(to_decode, tf.float32), sequence_length, blank_index=blank\n    )[0][\n        0\n    ]  # full is [B,...]\n    \n    truth = sparse_from_dense_ignore_value(y_true)  # full is [B,...]\n    truth = tf.cast(truth, hypothesis.dtype)\n    edit_dist = tf.edit_distance(hypothesis, truth, normalize=False)\n\n    non_ignore_mask = tf.not_equal(y_true, blank)\n    N = tf.reduce_sum(tf.cast(non_ignore_mask, tf.float32))\n    D = tf.reduce_sum(edit_dist)\n    result = (N - D) / N\n    result = tf.clip_by_value(result, 0.0, 1.0)\n    return result, N, D\n\n\nclass LevDistanceMetric(tf.keras.metrics.Metric):\n    def __init__(self, name=\"Lev\", **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.distance = self.add_weight(name=\"dist\", initializer=\"zeros\")\n        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n\n    def update_state(self, y_true, y_logits, sample_weight=None):\n        # if using with keras compile, make sure the model outputs logits, not softmax probabilities\n        _, N, D = batch_edit_distance(y_true, y_logits)\n        self.distance.assign_add(D)\n        self.count.assign_add(N)\n\n    def result(self):\n        result = (self.count - self.distance) / self.count\n        result = tf.clip_by_value(result, 0.0, 1.0)\n        return result\n\n    def reset_state(self):\n        self.count.assign(0.0)\n        self.distance.assign(0.0)\n\n\n\nclass SWA(tf.keras.callbacks.Callback):\n    # Stochastic Weight Averaging\n    def __init__(\n        self,\n        save_name,\n        swa_epochs=[],\n        strategy=None,\n        train_ds=None,\n        valid_ds=None,\n        train_steps=1000,\n    ):\n        super().__init__()\n        self.swa_epochs = swa_epochs\n        self.swa_weights = None\n        self.save_name = save_name\n        self.train_ds = train_ds\n        self.valid_ds = valid_ds\n        self.train_steps = train_steps\n        self.strategy = strategy\n\n    def train_step(self, iterator):\n        \"\"\"The step function for one training step.\"\"\"\n\n        def step_fn(inputs):\n            \"\"\"The computation to run on each device.\"\"\"\n            x, y = inputs\n            _ = self.model(x, training=True)\n\n        for x in iterator:\n            self.strategy.run(step_fn, args=(x,))\n\n    def on_epoch_end(self, epoch, logs=None):\n        if epoch in self.swa_epochs:\n            if self.swa_weights is None:\n                self.swa_weights = self.model.get_weights()\n            else:\n                w = self.model.get_weights()\n                for i in range(len(self.swa_weights)):\n                    self.swa_weights[i] += w[i]\n\n    def on_train_end(self, logs=None):\n        if len(self.swa_epochs):\n            print(\"applying SWA...\")\n            for i in range(len(self.swa_weights)):\n                self.swa_weights[i] = self.swa_weights[i] / len(self.swa_epochs)\n            self.model.set_weights(self.swa_weights)\n            if self.train_ds is not None:  # for the re-calculation of running mean and var\n                self.train_step(self.train_ds.take(self.train_steps))\n            print(f\"save SWA weights to {self.save_name}-SWA.h5\")\n            self.model.save_weights(f\"{self.save_name}-SWA.h5\")\n            if self.valid_ds is not None:\n                self.model.evaluate(self.valid_ds)\n\n\nclass AWP(tf.keras.Model):\n    # Adversarial Weight Perturbation\n    def __init__(self, *args, delta=0.1, eps=1e-4, start_step=0, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.delta = delta\n        self.eps = eps\n        self.start_step = start_step\n\n    def train_step_awp(self, data):\n        # Unpack the data. Its structure depends on your model and\n        # on what you pass to `fit()`.\n        x, y = data\n\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n        params = self.trainable_variables\n        params_gradients = tape.gradient(loss, self.trainable_variables)\n        for i in range(len(params_gradients)):\n            grad = tf.zeros_like(params[i]) + params_gradients[i]\n            delta = tf.math.divide_no_nan(\n                self.delta * grad, tf.math.sqrt(tf.reduce_sum(grad**2)) + self.eps\n            )\n            self.trainable_variables[i].assign_add(delta)\n        with tf.GradientTape() as tape2:\n            y_pred = self(x, training=True)\n            new_loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n            if hasattr(self.optimizer, \"get_scaled_loss\"):\n                new_loss = self.optimizer.get_scaled_loss(new_loss)\n\n        gradients = tape2.gradient(new_loss, self.trainable_variables)\n        if hasattr(self.optimizer, \"get_unscaled_gradients\"):\n            gradients = self.optimizer.get_unscaled_gradients(gradients)\n        for i in range(len(params_gradients)):\n            grad = tf.zeros_like(params[i]) + params_gradients[i]\n            delta = tf.math.divide_no_nan(\n                self.delta * grad, tf.math.sqrt(tf.reduce_sum(grad**2)) + self.eps\n            )\n            self.trainable_variables[i].assign_sub(delta)\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n        # self_loss.update_state(loss)\n        self.compiled_metrics.update_state(y, y_pred)\n        return {m.name: m.result() for m in self.metrics}\n\n    def train_step(self, data):\n        return tf.cond(\n            self._train_counter < self.start_step,\n            lambda: super(AWP, self).train_step(data),\n            lambda: self.train_step_awp(data),\n        )\n","metadata":{"id":"P-kprZ1_Yi14","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:33.812365Z","iopub.execute_input":"2023-07-25T05:55:33.813141Z","iopub.status.idle":"2023-07-25T05:55:33.873833Z","shell.execute_reply.started":"2023-07-25T05:55:33.813095Z","shell.execute_reply":"2023-07-25T05:55:33.872582Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Lev Callback","metadata":{}},{"cell_type":"code","source":"import Levenshtein as lev\nimport json\nwith open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n    character_map = json.load(f)\nrev_character_map = {j:i for i,j in character_map.items()}\nclass val_lev_callback(tf.keras.callbacks.Callback):\n    def __init__(self,val_set):\n        super().__init__()\n        self.val_set=val_set\n    def on_epoch_end(self, epoch: int, logs=None):\n        calculate_val_lev(self.model,self.val_set)\n        \ndef calculate_val_lev(model,val_set):\n    preds = []\n    targets = []\n    for batch_idx in range(len(val_set)):\n        preds_batch = model.predict(val_set[batch_idx][0],verbose=0)[0]\n        targets_batch = val_set[batch_idx][1]\n        for pred_idx in range(len(preds_batch)):\n            preds.append(\"\".join([rev_character_map.get(s, \"\") for s in decode_phrase(preds_batch[pred_idx]).numpy()]))\n            targets.append(\"\".join([rev_character_map.get(s, \"\") for s in targets_batch[pred_idx].numpy()]))\n\n    N = [len(phrase) for phrase in targets]\n    lev_dist = [lev.distance(preds[i], targets[i]) for i in range(len(targets))]\n    N=np.sum(N)\n    D=np.sum(lev_dist)\n    lev_d=(N-D)/N\n    print(\"\")    \n    print('Lev distance: ',lev_d)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:33.875507Z","iopub.execute_input":"2023-07-25T05:55:33.876227Z","iopub.status.idle":"2023-07-25T05:55:33.893634Z","shell.execute_reply.started":"2023-07-25T05:55:33.876185Z","shell.execute_reply":"2023-07-25T05:55:33.892774Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"YQAPz56nYi15"}},{"cell_type":"code","source":"\nclass CTCLossWrap(tf.keras.losses.Loss):\n    def __init__(self, pad_token_idx,batch_size,max_string_len,output_dim,output_steps,replicas):\n        self.pad_token_idx = pad_token_idx\n        self.batch_size=batch_size\n        self.max_string_len=max_string_len\n        self.output_steps=output_steps\n        self.output_dim=output_dim\n        self.replicas=replicas\n        super().__init__()\n\n    def call(self, labels, logits):\n\n        #logits=tf.ensure_shape(logits,(self.batch_size//self.replicas,self.output_steps,self.output_dim))\n        #labels=tf.ensure_shape(labels,(self.batch_size//self.replicas,self.max_string_len))\n        label_length = tf.reduce_sum(tf.cast(labels != self.pad_token_idx, tf.int32), axis=-1)\n        logit_length = tf.ones(tf.shape(logits)[0], dtype=tf.int32) * tf.shape(logits)[1]\n\n        #ctc_loss_fn = tf.nn.ctc_loss( \n        ctc_loss_fn=classic_ctc_loss(\n            labels=labels,\n            logits=logits,\n            label_length=label_length,\n            logit_length=logit_length,\n            blank_index=self.pad_token_idx,\n        )\n\n        return ctc_loss_fn\n\n\nclass ECA(tf.keras.layers.Layer):\n    # Efficient Channel Attention\n    def __init__(self, kernel_size=5, **kwargs):\n        super().__init__(**kwargs)\n        self.supports_masking = True\n        self.kernel_size = kernel_size\n        self.conv = tf.keras.layers.Conv1D(\n            1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False\n        )\n\n    def call(self, inputs, mask=None):\n        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n        nn = tf.expand_dims(nn, -1)\n        nn = self.conv(nn)\n        nn = tf.squeeze(nn, -1)\n        nn = tf.nn.sigmoid(nn)\n        nn = nn[:, None, :]\n        return inputs * nn\n\n\nclass LateDropout(tf.keras.layers.Layer):\n    def __init__(self, rate, noise_shape=None, start_step=0, **kwargs):\n        super().__init__(**kwargs)\n        self.supports_masking = True\n        self.rate = rate\n        self.start_step = start_step\n        self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n\n    def build(self, input_shape):\n        super().build(input_shape)\n        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n        self._train_counter = tf.Variable(0, dtype=\"int32\", aggregation=agg, trainable=False)\n\n    def call(self, inputs, training=False):\n        x = tf.cond(\n            self._train_counter < self.start_step,\n            lambda: inputs,\n            lambda: self.dropout(inputs, training=training),\n        )\n        if training:\n            self._train_counter.assign_add(1)\n        return x\n\n\nclass CausalDWConv1D(tf.keras.layers.Layer):\n    # Causal Depth Wise Convolution\n    def __init__(\n        self,\n        kernel_size=17,\n        dilation_rate=1,\n        use_bias=False,\n        depthwise_initializer=\"glorot_uniform\",\n        name=\"\",\n        **kwargs,\n    ):\n        super().__init__(name=name, **kwargs)\n        self.causal_pad = tf.keras.layers.ZeroPadding1D(\n            (dilation_rate * (kernel_size - 1), 0), name=name + \"_pad\"\n        )\n        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n            kernel_size,\n            strides=1,\n            dilation_rate=dilation_rate,\n            padding=\"valid\",\n            use_bias=use_bias,\n            depthwise_initializer=depthwise_initializer,\n            name=name + \"_dwconv\",\n        )\n        self.supports_masking = True\n\n    def call(self, inputs):\n        x = self.causal_pad(inputs)\n        x = self.dw_conv(x)\n        return x\n    \n\n\ndef Conv1DBlock(\n    channel_size,\n    kernel_size,\n    dilation_rate=1,\n    drop_rate=0.0,\n    expand_ratio=2,\n    # se_ratio=0.25,\n    activation=\"swish\",\n    name=None,\n):\n    \"\"\"\n    efficient conv1d block, @hoyso48\n    \"\"\"\n    if name is None:\n        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n\n    # Expansion phase\n    def apply(inputs):\n        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n        channels_expand = channels_in * expand_ratio\n\n        skip = inputs\n\n        x = tf.keras.layers.Dense(\n            channels_expand, use_bias=True, activation=activation, name=name + \"_expand_conv\"\n        )(inputs)\n\n        # Depthwise Convolution\n        x = CausalDWConv1D(\n            kernel_size, dilation_rate=dilation_rate, use_bias=False, name=name + \"_dwconv\"\n        )(x)\n\n        #x = tf.keras.layers.LayerNormalization(name=name + \"_bn\")(x)\n        x = tf.keras.layers.BatchNormalization(name=name + \"_bn\")(x)\n\n        x = ECA()(x)  # efficient channel attention\n\n        x = tf.keras.layers.Dense(channel_size, use_bias=True, name=name + \"_project_conv\")(x)\n\n        if drop_rate > 0:\n            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + \"_drop\")(x)\n\n        if channels_in == channel_size:\n            x = tf.keras.layers.add([x, skip], name=name + \"_add\")\n        return x\n\n    return apply\n\n\nclass MultiHeadSelfAttention(tf.keras.layers.Layer):\n    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n        super().__init__(**kwargs)\n        self.dim = dim\n        self.scale = self.dim**-0.5\n        self.num_heads = num_heads\n        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n        self.drop1 = tf.keras.layers.Dropout(dropout)\n        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n        self.supports_masking = True\n\n    def call(self, inputs, mask=None):\n        qkv = self.qkv(inputs)\n        qkv = tf.keras.layers.Permute((2, 1, 3))(\n            tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv)\n        )\n        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n\n        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n\n        if mask is not None:\n            mask = mask[:, None, None, :]\n\n        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n        attn = self.drop1(attn)\n\n        x = attn @ v\n        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n        x = self.proj(x)\n        return x\n\n\ndef TransformerBlock(\n    dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation=\"swish\"\n):\n    def apply(inputs):\n        x = inputs\n        x = tf.keras.layers.LayerNormalization()(x)\n        x = MultiHeadSelfAttention(dim=dim, num_heads=num_heads, dropout=attn_dropout)(x)\n        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1))(x)\n        x = tf.keras.layers.Add()([inputs, x])\n        attn_out = x\n\n        x = tf.keras.layers.LayerNormalization()(x)\n        x = tf.keras.layers.Dense(dim * expand, use_bias=False, activation=activation)(x)\n        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1))(x)\n        x = tf.keras.layers.Add()([attn_out, x])\n        return x\n\n    return apply\n\ndef build_model1(\n    output_dim,\n    max_len=64,\n    dropout_step=0,\n    dim=192,\n    input_pad=-100,\n    with_transformer=False,\n    drop_rate=0.2,\n):\n    inp = tf.keras.Input(shape=(max_len, Constants.CHANNELS), dtype=tf.float32, name=\"inputs\")\n    x = tf.keras.layers.Masking(mask_value=input_pad, input_shape=(max_len, Constants.CHANNELS))(\n        inp\n    )\n    ksize = 17\n    x = tf.keras.layers.Dense(dim, use_bias=False, name=\"stem_conv\")(x)\n    #x = tf.keras.layers.LayerNormalization(name=\"stem_bn\")(x)\n    x = tf.keras.layers.BatchNormalization(name=\"stem_bn\")(x)\n\n    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n    if with_transformer:\n        x = TransformerBlock(dim, expand=2)(x)\n\n    #x = tf.keras.layers.AvgPool1D(2, 2)(x)\n\n\n    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n    if with_transformer:\n        x = TransformerBlock(dim, expand=2)(x)\n\n    x = tf.keras.layers.AvgPool1D(2, 2)(x) # [B,T,dim]\n\n    lstm1 = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(units=dim//2), return_sequences=True)\n    #lstm1=tf.keras.layers.LSTM(units=dim//2,return_sequences=True)\n    x2 = tf.keras.layers.Bidirectional(lstm1)(x) #[B,T,dim]\n\n    x2=tf.keras.layers.BatchNormalization()(x2)\n    x2=tf.keras.layers.Dense(output_dim)(x2)\n    soft=tf.keras.layers.Activation('softmax', dtype='float32')(x2)\n    logsoft=tf.keras.layers.Activation('log_softmax',dtype='float32',name=\"internal\")(x2)\n\n    x=tf.keras.layers.Dense(dim)(soft)+x\n    x=tf.keras.layers.BatchNormalization()(x)\n    if dim == 384:  # for the 4x sized model\n        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n        if with_transformer:\n            x = TransformerBlock(dim, expand=2)(x)\n\n        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n        if with_transformer:\n            x = TransformerBlock(dim, expand=2)(x)\n\n    \n    lstm2 = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(units=dim//2), return_sequences=True)\n    #lstm2=tf.keras.layers.LSTM(units=dim//2,return_sequences=True)\n    x = tf.keras.layers.Bidirectional(lstm2)(x)\n\n    x = LateDropout(0.8, start_step=dropout_step)(x)\n\n    x=tf.keras.layers.Dense(output_dim)(x)\n    output = tf.keras.layers.Activation(\"log_softmax\",name=\"final\",dtype=\"float32\")(x)  # logits\n\n    model = tf.keras.Model(inp, outputs=[output,logsoft])\n    return model\n\n\ndef get_model(output_dim, max_len, dim, input_pad,dropout_step=0,drop_rate=0.):\n\n    model = build_model1(output_dim, max_len=max_len, input_pad=input_pad, dim=dim,  dropout_step=dropout_step,drop_rate=drop_rate)\n\n    return model\n","metadata":{"id":"wnoZyLAcYi15","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:33.895772Z","iopub.execute_input":"2023-07-25T05:55:33.896532Z","iopub.status.idle":"2023-07-25T05:55:33.961078Z","shell.execute_reply.started":"2023-07-25T05:55:33.896474Z","shell.execute_reply":"2023-07-25T05:55:33.959349Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"from functools import lru_cache\n\n@lru_cache(maxsize=None)\ndef get_strategy():\n    logical_devices = tf.config.list_logical_devices()\n    # Check if TPU is available\n\n    gpu_available = any(\"GPU\" in device.name for device in logical_devices)\n    strategy = None\n    is_tpu = False\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print(\"Running on TPU \", tpu.master())\n        is_tpu = True\n    except ValueError:\n        is_tpu = False\n\n    if is_tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n\n        print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n\n        strategy = tf.distribute.TPUStrategy(tpu)\n        #disable_eager_execution()  # LSTM layer can't use bfloat16 unless we do this.\n\n    else:\n        if gpu_available:\n\n            ngpu = len(gpus)\n            print(\"Num GPUs Available: \", ngpu)\n            if ngpu > 1:\n                strategy = tf.distribute.MirroredStrategy()\n            else:\n                strategy = tf.distribute.get_strategy()\n\n        else:\n            print(\"Runing on CPU\")\n            strategy = tf.distribute.get_strategy()\n    replicas = strategy.num_replicas_in_sync\n\n    print(f\"get strategy replicas: {replicas}\")\n\n    return strategy, replicas, is_tpu\n\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:33.968820Z","iopub.execute_input":"2023-07-25T05:55:33.970901Z","iopub.status.idle":"2023-07-25T05:55:33.982867Z","shell.execute_reply.started":"2023-07-25T05:55:33.970851Z","shell.execute_reply":"2023-07-25T05:55:33.981693Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"\nclass CFG:\n    # These 3 variables are update dynamically later by calling update_config_with_strategy.\n    strategy = None  # type: ignore\n    replicas = 1\n    is_tpu = False\n\n    save_output = True\n    input_path = \"/kaggle/input\"\n    output_path = \"/kaggle/working\"\n\n    seed = 42\n    verbose = 1  # 0) silent 1) progress bar 2) one line per epoch\n\n    # max number of frames\n    max_len = 256\n    replicas = 1\n\n    lr = 4e-4   # 5e-4\n    weight_decay = 1e-4  # 4e-4\n    epochs = 300\n\n    batch_size=128\n\n    snapshot_epochs = []  # type: ignore\n    swa_epochs = list(range(3*(epochs//4),epochs+1))\n\n    fp16=True\n\n    awp = False\n    awp_lambda = 0.15\n    awp_start_epoch = 15\n    dropout_start_epoch = 15\n    resume = 0\n\n    dim = 384\n\n    comment = f\"model-{dim}-seed{seed}\"\n    output_dim = 60\n    num_eval = 6\n\n\n","metadata":{"id":"7e1LGyFWYi16","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:33.985375Z","iopub.execute_input":"2023-07-25T05:55:33.986204Z","iopub.status.idle":"2023-07-25T05:55:33.999130Z","shell.execute_reply.started":"2023-07-25T05:55:33.986154Z","shell.execute_reply":"2023-07-25T05:55:33.997948Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"\ndef update_config_with_strategy(config):\n    # cfg is configuration instance\n    #strategy, replicas, is_tpu = get_strategy()\n    if tpu_strategy is not None:\n      strategy=tpu_strategy\n      replicas=8\n      is_tpu=True\n      if IN_COLAB:\n          config.input_path=config.input_path.replace(\"/kaggle\",\"gs://asl-bucket71\")\n          config.output_path = config.output_path.replace(\"/kaggle\",\"/content/drive/MyDrive/kaggle\")\n\n    else:\n      strategy,replicas,is_tpu=get_strategy()\n    print(\"Strategy\",strategy)\n\n    config.strategy = strategy\n    config.replicas = replicas\n    config.is_tpu = is_tpu\n    config.lr = config.lr * replicas\n    config.batch_size = config.batch_size * replicas\n    return config","metadata":{"id":"HCKh-AxTYi16","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:34.002579Z","iopub.execute_input":"2023-07-25T05:55:34.003699Z","iopub.status.idle":"2023-07-25T05:55:34.016065Z","shell.execute_reply.started":"2023-07-25T05:55:34.003599Z","shell.execute_reply":"2023-07-25T05:55:34.014930Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"id":"5msRCmt0Yi16"}},{"cell_type":"markdown","source":"","metadata":{"id":"nrUlKKykrs3c"}},{"cell_type":"code","source":"\ndef count_data_items(dataset):\n    dataset_size = 0\n    for _ in dataset:\n        dataset_size += 1\n    return dataset_size\n\n\ndef interp1d_(x, target_len):\n    target_len = tf.maximum(1, target_len)\n    x = tf.image.resize(x, (target_len, tf.shape(x)[1]))\n    return x\n\n\ndef tf_nan_mean(x, axis=0, keepdims=False):\n    return tf.reduce_sum(\n        tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims\n    ) / tf.reduce_sum(\n        tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims\n    )\n\n\ndef tf_nan_std(x, center=None, axis=0, keepdims=False):\n    if center is None:\n        center = tf_nan_mean(x, axis=axis, keepdims=True)\n    d = x - center\n    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n\n\ndef flip_lr(x):\n    if x.shape[1] == Constants.ROWS_PER_FRAME:\n        LHAND = Constants.LHAND\n        RHAND = Constants.RHAND\n        LLIP = Constants.LLIP\n        RLIP = Constants.RLIP\n        LEYE = Constants.LEYE\n        REYE = Constants.REYE\n        LNOSE = Constants.LNOSE\n        RNOSE = Constants.RNOSE\n        LPOSE = Constants.LPOSE\n        RPOSE = Constants.RPOSE\n    else:\n        LHAND = Constants.LANDMARK_INDICES[\"LHAND\"]\n        RHAND = Constants.LANDMARK_INDICES[\"RHAND\"]\n        LLIP = Constants.LANDMARK_INDICES[\"LLIP\"]\n        RLIP = Constants.LANDMARK_INDICES[\"RLIP\"]\n        LEYE = Constants.LANDMARK_INDICES[\"LEYE\"]\n        REYE = Constants.LANDMARK_INDICES[\"REYE\"]\n        LNOSE = Constants.LANDMARK_INDICES[\"LNOSE\"]\n        RNOSE = Constants.LANDMARK_INDICES[\"RNOSE\"]\n        LPOSE = Constants.LANDMARK_INDICES[\"LPOSE\"]\n        RPOSE = Constants.LANDMARK_INDICES[\"RPOSE\"]\n\n    x, y = tf.unstack(x, axis=-1)\n    x = 1 - x\n    new_x = tf.stack([x, y], -1)\n    new_x = tf.transpose(new_x, [1, 0, 2])\n    lhand = tf.gather(new_x, LHAND, axis=0)\n    rhand = tf.gather(new_x, RHAND, axis=0)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LHAND)[..., None], rhand)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RHAND)[..., None], lhand)\n    llip = tf.gather(new_x, LLIP, axis=0)\n    rlip = tf.gather(new_x, RLIP, axis=0)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LLIP)[..., None], rlip)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RLIP)[..., None], llip)\n    lpose = tf.gather(new_x, LPOSE, axis=0)\n    rpose = tf.gather(new_x, RPOSE, axis=0)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LPOSE)[..., None], rpose)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RPOSE)[..., None], lpose)\n    leye = tf.gather(new_x, LEYE, axis=0)\n    reye = tf.gather(new_x, REYE, axis=0)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LEYE)[..., None], reye)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(REYE)[..., None], leye)\n    lnose = tf.gather(new_x, LNOSE, axis=0)\n    rnose = tf.gather(new_x, RNOSE, axis=0)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LNOSE)[..., None], rnose)\n    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RNOSE)[..., None], lnose)\n    new_x = tf.transpose(new_x, [1, 0, 2])\n    return new_x\n\n\ndef resample(x, rate=(0.8, 1.2)):\n    rate = tf.random.uniform((), rate[0], rate[1])\n    length = tf.shape(x)[0]\n    new_size = tf.cast(rate * tf.cast(length, tf.float32), tf.int32)\n    new_x = interp1d_(x, new_size)\n    return new_x\n\n\ndef spatial_random_affine(\n    xyz,\n    scale=(0.8, 1.2),\n    shear=(-0.1, 0.1),\n    shift=(-0.1, 0.1),\n    degree=(-20, 20),\n):\n    center = tf.constant([0.5, 0.5])\n    if degree is not None:\n        xy = xyz[..., :2]\n        z = xyz[..., 2:]\n        xy -= center\n        degree = tf.random.uniform((), *degree)\n        radian = degree / 180 * np.pi\n        c = tf.math.cos(radian)\n        s = tf.math.sin(radian)\n        rotate_mat = tf.identity(\n            [\n                [c, s],\n                [-s, c],\n            ]\n        )\n        xy = xy @ rotate_mat\n        xy = xy + center\n        xyz = tf.concat([xy, z], axis=-1)\n\n    if scale is not None:\n        scale = tf.random.uniform((), *scale)\n        xyz = scale * xyz\n\n    if shear is not None:\n        xy = xyz[..., :2]\n        z = xyz[..., 2:]\n        shear_x = shear_y = tf.random.uniform((), *shear)\n        if tf.random.uniform(()) < 0.5:\n            shear_x = 0.0\n        else:\n            shear_y = 0.0\n        shear_mat = tf.identity([[1.0, shear_x], [shear_y, 1.0]])\n        xy = xy @ shear_mat\n        xyz = tf.concat([xy, z], axis=-1)\n\n    if shift is not None:\n        shift = tf.random.uniform((), *shift)\n        xyz = xyz + shift\n\n    return xyz\n\n\ndef temporal_mask(x, size=[1, 15], mask_value=float(\"nan\")):\n    l0 = tf.shape(x)[0]\n    if size[1] > l0 // 8:\n        size[1] = l0 // 8\n        if size[1] <= 1:\n            size[1] = 2\n    mask_size = tf.random.uniform((), *size, dtype=tf.int32)\n    mask_offset = tf.random.uniform((), 0, tf.clip_by_value(l0 - mask_size, 1, l0), dtype=tf.int32)\n    x = tf.tensor_scatter_nd_update(\n        x,\n        tf.range(mask_offset, mask_offset + mask_size)[..., None],\n        tf.fill([mask_size, tf.shape(x)[1], 2], mask_value),\n    )\n    return x\n\n\ndef spatial_mask(x, size=(0.05, 0.2), mask_value=float(\"nan\")):\n    mask_offset_y = tf.random.uniform(())\n    mask_offset_x = tf.random.uniform(())\n    mask_size = tf.random.uniform((), *size)\n    mask_x = (mask_offset_x < x[..., 0]) & (x[..., 0] < mask_offset_x + mask_size)\n    mask_y = (mask_offset_y < x[..., 1]) & (x[..., 1] < mask_offset_y + mask_size)\n    mask = mask_x & mask_y\n    x = tf.where(mask[..., None], mask_value, x)\n    return x\n\n\n\ndef augment_fn(x):\n    # shape (T,F)\n    x = tf.reshape(x, (tf.shape(x)[0], -1, 2))\n    if tf.random.uniform(()) < 0.8:\n        x = resample(x, (0.5, 1.5))\n    if tf.random.uniform(()) < 0.5:\n        x = flip_lr(x)\n    if tf.random.uniform(()) < 0.75:\n        x = spatial_random_affine(x)\n    if tf.random.uniform(()) < 0.5:\n        x = temporal_mask(x)\n    if tf.random.uniform(()) < 0.5:\n        x = spatial_mask(x)\n    x = tf.reshape(x, (tf.shape(x)[0], -1))\n    return x\n","metadata":{"id":"j_tHWSs8Yi16","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:34.018932Z","iopub.execute_input":"2023-07-25T05:55:34.019987Z","iopub.status.idle":"2023-07-25T05:55:34.068565Z","shell.execute_reply.started":"2023-07-25T05:55:34.019943Z","shell.execute_reply":"2023-07-25T05:55:34.067296Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"\nclass Preprocess(tf.keras.layers.Layer):\n    def __init__(self, max_len, normalize=False, **kwargs):\n        super().__init__(**kwargs)\n        self.max_len = max_len\n        self.center = Constants.CENTER_INDICES\n        self.normalize = normalize\n\n    # preprocess a batch of data\n    def call(self, x):\n        # rank is 3: [B,T,F]\n        # if your input is just [T,F], extend its dimesnion before calling.\n\n        x = tf.reshape(x, (tf.shape(x)[0], tf.shape(x)[1], Constants.NUM_NODES, 2))\n        # dimensions now are [B,T,F//2,2]\n\n        x_selected = x\n        if self.normalize:\n            mean = tf_nan_mean(tf.gather(x, self.center, axis=2), axis=[1, 2], keepdims=True)\n            mean = tf.where(tf.math.is_nan(mean), tf.constant(0.5, x.dtype), mean)\n            std = tf_nan_std(x_selected, center=mean, axis=[1, 2], keepdims=True)\n            x = (x_selected - mean) / std\n        else:\n            x = x_selected\n\n        dx = tf.cond(\n            tf.shape(x)[1] > 1,\n            lambda: tf.pad(x[:, 1:] - x[:, :-1], [[0, 0], [0, 1], [0, 0], [0, 0]]),\n            lambda: tf.zeros_like(x),\n        )\n\n        dx2 = tf.cond(\n            tf.shape(x)[1] > 2,\n            lambda: tf.pad(x[:, 2:] - x[:, :-2], [[0, 0], [0, 2], [0, 0], [0, 0]]),\n            lambda: tf.zeros_like(x),\n        )\n        length = tf.shape(x)[1]\n\n        x = tf.concat(\n            [\n                tf.reshape(x, (-1, length, 2 * Constants.NUM_NODES)),  # x1,y1,x2,y2,...\n                tf.reshape(dx, (-1, length, 2 * Constants.NUM_NODES)),\n                tf.reshape(dx2, (-1, length, 2 * Constants.NUM_NODES)),\n            ],\n            axis=-1,\n        )\n\n        # x1,y1,x2,y2,...dx1,dy1,dx2,dy2,...\n        x = tf.where(tf.math.is_nan(x), tf.constant(0.0, x.dtype), x)\n        return x\n\n\ndef pad_if_short(x, max_len):\n    # shape (T,F)\n    pad_len = max_len - tf.shape(x)[0]\n    padding = tf.ones((pad_len, tf.shape(x)[1]), dtype=x.dtype) * Constants.INPUT_PAD\n    x = tf.concat([x, padding], axis=0)\n    return x\n\n\ndef shrink_if_long(x, max_len):\n    # shape is [T,F]\n    if tf.shape(x)[0] > max_len:\n        # we need to extend the dimension to [T,F,channels]  for tf.image.resize\n        x = tf.image.resize(x[..., None], (max_len, tf.shape(x)[1]))\n        x = tf.squeeze(x, axis=2)\n\n    return x\n\ndef preprocess(x, max_len, do_pad=True):\n    # shape (T,F)\n    x = shrink_if_long(x, max_len=max_len)\n    # Preprocess expects a batch, so we extend the dimension to (None,T,F), then reduce the output back to (T,F).\n    x = tf.cast(Preprocess(max_len=max_len)(x[None, ...])[0], tf.float32)\n\n    if do_pad:  # we can avoid this step if there is batch padding\n        x = pad_if_short(x, max_len=max_len)\n        #x=tf.ensure_shape(x,(max_len,Constants.CHANNELS))\n    else:\n        #x=tf.ensure_shape(x,(None,Constants.CHANNELS))\n        pass\n    return x","metadata":{"id":"jYUGdwEDYi17","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:34.070671Z","iopub.execute_input":"2023-07-25T05:55:34.071981Z","iopub.status.idle":"2023-07-25T05:55:34.097611Z","shell.execute_reply.started":"2023-07-25T05:55:34.071938Z","shell.execute_reply":"2023-07-25T05:55:34.096207Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"\ndef decode_tfrec(record_bytes):\n    features = tf.io.parse_single_example(\n        record_bytes,\n        {\n            \"coordinates\": tf.io.VarLenFeature(tf.float32),\n            \"label\": tf.io.VarLenFeature(tf.int64),\n        },\n    )\n    coords = tf.sparse.to_dense(features[\"coordinates\"])\n    coords = tf.reshape(coords, (-1, Constants.NUM_INPUT_FEATURES))\n    label = tf.sparse.to_dense(features[\"label\"])\n    label=tf.cast(label,dtype=tf.int32)\n\n    #coords=tf.ensure_shape(coords,(None,Constants.NUM_INPUT_FEATURES))\n    #label=tf.ensure_shape(label,(None,))\n\n\n    return (coords, label)\n\ndef ensure_shapes(x,y,batch_size,max_len):\n  x=tf.ensure_shape(x,(batch_size,max_len,Constants.CHANNELS))\n  y=tf.ensure_shape(y,(batch_size,Constants.MAX_STRING_LEN))\n  tf.print(\"ensure\",tf.shape(x),tf.shape(y))\n  return x,y","metadata":{"id":"FdtpVVSbYi17","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:34.101452Z","iopub.execute_input":"2023-07-25T05:55:34.102810Z","iopub.status.idle":"2023-07-25T05:55:34.116165Z","shell.execute_reply.started":"2023-07-25T05:55:34.102775Z","shell.execute_reply":"2023-07-25T05:55:34.114931Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"\ndef get_dataset(\n    filenames,\n    input_path,\n    max_len,\n    batch_size=64,\n    drop_remainder=False,\n    augment=False,\n    shuffle_buffer=None,\n    repeat=False,\n    use_tfrecords=True,\n):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False\n\n\n    ds = tf.data.TFRecordDataset(\n        filenames, num_parallel_reads=tf.data.AUTOTUNE, compression_type=\"GZIP\"\n    )\n    ds.with_options(ignore_order)\n    ds = ds.map(decode_tfrec, tf.data.AUTOTUNE)\n\n    if augment:\n        ds = ds.map(lambda x, y: (augment_fn(x), y), tf.data.AUTOTUNE)\n\n    ds = ds.map(lambda x, y: (preprocess(x, max_len=max_len, do_pad=False), y), tf.data.AUTOTUNE)\n    #if repeat:\n    #    ds = ds.repeat()\n\n    if shuffle_buffer is not None:\n        ds = ds.shuffle(shuffle_buffer)\n\n    ds = ds.padded_batch(\n        batch_size,\n        padding_values=(\n            tf.constant(Constants.INPUT_PAD, dtype=tf.float32),\n            tf.constant(Constants.LABEL_PAD, dtype=tf.int32),\n        ),\n        padded_shapes=([max_len, Constants.CHANNELS], [Constants.MAX_STRING_LEN]),\n        drop_remainder=drop_remainder,\n    )\n\n    #tf.data.experimental.assert_cardinality(len(labels) // BATCH_SIZE)\n    ds.map(lambda x,y: ensure_shapes(x,y,batch_size,max_len),tf.data.AUTOTUNE)\n    ds = ds.prefetch(tf.data.AUTOTUNE)\n\n    return ds\n","metadata":{"id":"ImAgehPoYi18","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:34.120230Z","iopub.execute_input":"2023-07-25T05:55:34.121383Z","iopub.status.idle":"2023-07-25T05:55:34.135588Z","shell.execute_reply.started":"2023-07-25T05:55:34.121348Z","shell.execute_reply":"2023-07-25T05:55:34.134225Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"\ndef train_run(train_files, valid_files, config, num_train, num_valid,experiment_id=0, use_tfrecords=True,summary=False,evaluate_only=False):\n    #gc.collect()\n    #tf.keras.backend.clear_session()\n\n\n    if config.fp16:\n        if config.is_tpu:\n            policy = \"mixed_bfloat16\"\n        else:\n            policy = \"mixed_float16\"\n    else:\n        policy = \"float32\"\n\n\n    tf.keras.mixed_precision.set_global_policy(policy)\n    print(f\"\\n... TWO IMPORTANT ASPECTS OF THE GLOBAL MIXED PRECISION POLICY:\")\n    print(f'\\t--> COMPUTE DTYPE  : {tf.keras.mixed_precision.global_policy().compute_dtype}')\n    print(f'\\t--> VARIABLE DTYPE : {tf.keras.mixed_precision.global_policy().variable_dtype}')\n    augment_train= True\n    repeat_train = True\n    if config.is_tpu:\n      shuffle_buffer = 16384 #4096\n    else:\n      shuffle_buffer=4096\n    print(\"shuffle_buffer\",shuffle_buffer)\n    train_ds = get_dataset(\n        train_files,\n        input_path=config.input_path,\n        max_len=config.max_len,\n        batch_size=config.batch_size,\n        drop_remainder=True,\n        augment=augment_train,\n        repeat=repeat_train,\n        shuffle_buffer=shuffle_buffer,\n        use_tfrecords=True,\n    )\n    if valid_files is not None:\n        valid_ds = get_dataset(\n            valid_files,\n            input_path=config.input_path,\n            max_len=config.max_len,\n            batch_size=config.batch_size,\n            use_tfrecords=True,\n            drop_remainder=True\n        )\n    else:\n        valid_ds = None\n        valid_files = []\n\n    valid_set_memory=[x for x in valid_ds]\n    \n    #num_train = count_data_items(train_ds)\n    #num_valid = count_data_items(valid_ds)\n    #print(\"num_train batches\",num_train, \"num_valid batches\",num_valid,\"batch_size\", config.batch_size)\n    #assert False\n\n    steps_per_epoch = num_train // config.batch_size\n    dropout_step = config.dropout_start_epoch * steps_per_epoch\n    strategy = config.strategy\n    with strategy.scope():\n        model = get_model(\n            max_len=config.max_len,\n            output_dim=config.output_dim,\n            input_pad=Constants.INPUT_PAD,\n            dim=config.dim,\n            dropout_step=dropout_step,\n            drop_rate=0.2\n        )\n\n        base_lr = config.lr\n        lr_schedule = CosineDecay(\n            initial_learning_rate=base_lr,\n            decay_steps=int(steps_per_epoch * config.epochs),\n            alpha=0.005,\n            name=None,\n            warmup_target=None,\n            warmup_steps=0\n        )\n\n        #opt = tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=config.weight_decay)\n        radam=tfa.optimizers.RectifiedAdam(learning_rate=lr_schedule,weight_decay=config.weight_decay)\n        ranger = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)\n        opt=ranger\n        awp_step = config.awp_start_epoch * steps_per_epoch\n        if config.awp:\n            model = AWP(model.input, model.output, delta=config.awp_lambda, eps=0., start_step=awp_step)\n            print(\"Using AWP\")\n\n        ctc_loss1 = CTCLossWrap(pad_token_idx=Constants.LABEL_PAD,batch_size=config.batch_size,\n                           max_string_len=Constants.MAX_STRING_LEN,\n                           output_dim=config.output_dim,\n                           output_steps=config.max_len//2,replicas=config.replicas)\n        ctc_loss2 = CTCLossWrap(pad_token_idx=Constants.LABEL_PAD,batch_size=config.batch_size,\n                           max_string_len=Constants.MAX_STRING_LEN,\n                           output_dim=config.output_dim,\n                           output_steps=config.max_len//2,replicas=config.replicas)\n\n        if not config.is_tpu:\n          metrics=metrics= [LevDistanceMetric(),]\n        else:\n          metrics=None\n        model.compile(\n          optimizer=opt,\n          loss=[ctc_loss1,ctc_loss2],\n          loss_weights=[0.5,0.5],\n          metrics= metrics,\n          #steps_per_execution=16\n        )\n\n\n\n    if summary:\n        print()\n        model.summary()\n        print()\n        print(train_ds, valid_ds)\n        print()\n    print(f\"---------experiment {experiment_id}---------\")\n    print(f\"train:{num_train} \")\n    print()\n\n    if evaluate_only:\n        model.load_weights(f\"{config.output_path}/{config.comment}-exp{experiment_id}-best.h5\")\n        cv=model.evaluate(valid_ds,verbose=config.verbose)\n        return model,cv,None\n\n    if config.resume:\n        print(f\"resume from epoch{config.resume}\")\n        model.load_weights(f\"{config.output_path}/{config.comment}-exp{experiment_id}-last.h5\")\n        if train_ds is not None:\n            model.evaluate(train_ds.take(steps_per_epoch))\n        if valid_ds is not None:\n            model.evaluate(valid_ds)\n\n    tb_logger = tf.keras.callbacks.TensorBoard(\n        log_dir=config.output_path,\n    )\n    sv_loss = tf.keras.callbacks.ModelCheckpoint(\n        f\"{config.output_path}/{config.comment}-exp{experiment_id}-best.h5\",\n        monitor=\"val_final_loss\",\n        verbose=1,\n        save_best_only=True,\n        save_weights_only=True,\n        mode=\"min\",\n        save_freq=\"epoch\",\n    )\n\n    # Callback function to check transcription on the val set.\n    # validation_callback = CallbackEval(model, valid_ds)\n    memory_usage = MemoryUsageCallbackExtended()\n    swa = SWA(\n        f\"{config.output_path}/{config.comment}-exp{experiment_id}\",\n        config.swa_epochs,\n        strategy=strategy,\n        train_ds=train_ds,\n        valid_ds=valid_ds,\n    )\n    val_lev=val_lev_callback(valid_set_memory)\n    callbacks = []\n    if config.save_output:\n        #callbacks.append(tb_logger)\n        callbacks.append(val_lev)\n        callbacks.append(swa)\n        callbacks.append(sv_loss)\n    #callbacks.append(memory_usage)\n        callbacks.append(tf.keras.callbacks.TerminateOnNaN())\n    # callbacks.append(validation_callback)\n\n    history = model.fit(\n        train_ds,\n        epochs=config.epochs - config.resume,\n        #steps_per_epoch=steps_per_epoch,\n        #validation_steps=num_valid // config.batch_size,\n        callbacks=callbacks,\n        validation_data=valid_ds,\n        verbose=config.verbose,\n    )\n\n    if config.save_output:  # reload the saved best weights checkpoint\n        saved_based_model = f\"{config.output_path}/{config.comment}-exp{experiment_id}-best.h5\"\n        if os.path.exists(saved_based_model):\n            model.load_weights(saved_based_model)\n        else:\n            print(f\"Warning: could not find {saved_based_model}\")\n    if valid_ds is not None:\n        cv = model.evaluate(valid_ds, verbose=config.verbose)\n    else:\n        cv = None\n    return model, cv, history\n\n","metadata":{"id":"4HDYkwwgYi18","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:34.137900Z","iopub.execute_input":"2023-07-25T05:55:34.138409Z","iopub.status.idle":"2023-07-25T05:55:34.173215Z","shell.execute_reply.started":"2023-07-25T05:55:34.138366Z","shell.execute_reply":"2023-07-25T05:55:34.172089Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"\ndef train(config, experiment_id=0, use_supplemental=True,use_chicago=True,evaluate_only=False):\n    #tf.keras.backend.clear_session()\n    if config.strategy is None:\n      update_config_with_strategy(config)\n    print(f\"using {config.replicas} replicas\")\n    print(f\"batch size {config.batch_size}\")\n    print(f\"learning rate {config.lr}\")\n    print(f\"fp16={config.fp16}\")\n    seed_everything(config.seed)\n\n\n    #all_filenames = tf.io.gfile.glob(config.input_path+\"/asl-preprocessing/records/*.tfrecord\")\n    \n    all_filenames = tf.io.gfile.glob(config.input_path+\"/sign-tfrecords/*.tfrecord\")\n    \n    regular = [x for x in all_filenames if \"train\" in x]\n    supp = [x for x in all_filenames if \"supp\" in x]\n    chicago = [x for x in all_filenames if \"chicago\" in x]\n    all_filenames=sorted(regular)+sorted(supp)+sorted(chicago)\n\n    data_filenames = regular\n    if use_supplemental:\n        data_filenames += supp\n    if use_chicago:\n        data_filenames +=chicago\n    print(\"Using TFRECORDS\")\n\n    valid_files = data_filenames[: config.num_eval]  # first part in sorted list\n    train_files = data_filenames[config.num_eval :]\n\n    random.shuffle(train_files) # now shuffle only the train set.\n    \n\n    #df1 = pd.read_csv(config.input_path + \"/asl-fingerspelling/train.csv\")\n    #df2 = pd.read_csv(config.input_path + \"/asl-fingerspelling/supplemental_metadata.csv\")\n    #df_info = pd.concat([df1, df2])\n\n    #ds = get_dataset(train_files, CFG.input_path,max_len=CFG.max_len, augment=False, batch_size=64)\n    #print(ds)\n    #for x,y in ds:\n    #    print(x,y)\n    #assert False\n\n    if (not use_supplemental) and (not use_chicago):\n        num_train = 1912 * 32  # without supplemental\n    elif use_supplemental and (not use_chicago):\n        num_train = 3567 * 32  # with supplemental\n    else:\n        num_train=5505*32\n\n    num_valid=187*32\n    #train_files=train_files[:6]\n    #num_train=6000\n\n    train_run(\n        train_files,\n        valid_files,\n        config,\n        num_train,\n        num_valid,\n        summary=False,\n        experiment_id=experiment_id,\n        use_tfrecords=True,\n        evaluate_only=evaluate_only\n    )\n\n","metadata":{"id":"YpxT9BTFYi18","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:34.177038Z","iopub.execute_input":"2023-07-25T05:55:34.177552Z","iopub.status.idle":"2023-07-25T05:55:34.193219Z","shell.execute_reply.started":"2023-07-25T05:55:34.177497Z","shell.execute_reply":"2023-07-25T05:55:34.192088Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ntf.keras.backend.clear_session()","metadata":{"id":"lfC8z_wLYi19","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-07-25T05:55:34.195412Z","iopub.execute_input":"2023-07-25T05:55:34.195915Z","iopub.status.idle":"2023-07-25T05:55:35.052440Z","shell.execute_reply.started":"2023-07-25T05:55:34.195877Z","shell.execute_reply":"2023-07-25T05:55:35.051323Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"# Runnn!","metadata":{"id":"BVIhC2jeYi19"}},{"cell_type":"code","source":"if 'config' not in globals():\n  config=CFG()\ntf.debugging.disable_traceback_filtering()\ntrain(config,use_supplemental=True)\n#assert False","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"5UdMh9V7Yi19","outputId":"d51989b7-b5b4-45e1-c2c9-1541942fd4c3","execution":{"iopub.status.busy":"2023-07-25T05:55:35.055329Z","iopub.execute_input":"2023-07-25T05:55:35.056121Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"using 1 replicas\nbatch size 128\nlearning rate 0.0004\nfp16=True\nUsing TFRECORDS\n\n... TWO IMPORTANT ASPECTS OF THE GLOBAL MIXED PRECISION POLICY:\n\t--> COMPUTE DTYPE  : float16\n\t--> VARIABLE DTYPE : float32\nshuffle_buffer 4096\n---------experiment 0---------\ntrain:176160 \n\nEpoch 1/300\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Inference","metadata":{"id":"Cdtx5fW3-DPm"}},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"id":"-uAqcPmSYi1-","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class InferModel(tf.Module):\n    def __init__(self, model,config=CFG):\n        super().__init__()\n\n        self.model = model\n        self.max_len=config.max_len\n\n    @tf.function(\n        input_signature=[tf.TensorSpec(shape=(None,Constants.NUM_INPUT_FEATURES), dtype=tf.float32, name=\"inputs\")]\n    )\n    def __call__(self, inputs):\n        \"\"\"\n        Applies the feature generation model and main model to the input tensor.\n\n        Args:\n            inputs: Input tensor with shape (T, F).\n\n        Returns:\n            A dictionary with a single key 'outputs' and corresponding output tensor.\n        \"\"\"\n        x=tf.cast(inputs,tf.float32)\n        x = x[None] # trick to deal with empty frames\n        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, Constants.NUM_INPUT_FEATURES)), lambda: tf.identity(x))\n        x = x[0]\n        x = preprocess(x,max_len=self.max_len)\n\n        x = self.model(x[None],training=False)[0][0]\n\n        x=decode_phrase(x)\n        x = tf.cond(tf.shape(x)[0] == 0, lambda: tf.zeros(1, tf.int32), lambda: tf.identity(x))\n\n        outputs=tf.one_hot(x,depth=59,dtype=tf.float32)\n        #outputs=x\n        return {\"outputs\": outputs}\n","metadata":{"id":"ZWJM1IC5Yi1-","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nconfig=CFG\n\nmodel = get_model(\n    max_len=config.max_len,\n    output_dim=config.output_dim,\n    dim=config.dim,\n    input_pad=Constants.INPUT_PAD,\n)\nexperiment_id=0\n\nsaved_based_model = f\"{config.input_path}/weights/{config.comment}-exp{experiment_id}-best.h5\"\nmodel.load_weights(saved_based_model)\nprint(f\"model with weights {saved_based_model}\")","metadata":{"id":"7xpye2JQYi1_","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sanity Check\nimport json\nwith open (config.input_path+\"/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n    character_map = json.load(f)\nrev_character_map = {j:i for i,j in character_map.items()}\n\ninfer_keras_model=InferModel(model)\n\nmain_dir = config.input_path+'/asl-fingerspelling'\npath = f'{main_dir}/train_landmarks/5414471.parquet'\ncols=selected_columns(path)\ndf = pd.read_parquet(path, engine = 'auto', columns = cols)\nseq_id=1816796431\nseq=df.loc[seq_id]\ndata = seq[cols].to_numpy()\nprint(f'input shape: {data.shape}, dtype: {data.dtype}')\noutput = infer_keras_model(data)[\"outputs\"]\nprediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output, axis=1)])\n\nprint(prediction_str)","metadata":{"id":"jwxBqovnYi1_","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SAVED_MODEL_PATH=config.output_path+\"/infer_model\"\n\ntf.saved_model.save(infer_keras_model,SAVED_MODEL_PATH)\nkeras_model_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_PATH)\nkeras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\nkeras_model_converter.target_spec.supported_types = [tf.float16]\n\n#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n#converter.allow_custom_ops=True\ntflite_model = keras_model_converter.convert()\nTFLITE_FILE_PATH=config.output_path+\"/model.tflite\"\nwith open(TFLITE_FILE_PATH, \"wb\") as f:\n    f.write(tflite_model)\n\nwith open(config.output_path+'/inference_args.json', 'w') as f:\n     json.dump({ 'selected_columns': cols }, f)\n\n","metadata":{"id":"YQM7GMtfYi1_","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interpreter = tf.lite.Interpreter(TFLITE_FILE_PATH)\nREQUIRED_SIGNATURE = \"serving_default\"\nREQUIRED_OUTPUT = \"outputs\"\nfound_signatures = list(interpreter.get_signature_list().keys())\nif REQUIRED_SIGNATURE not in found_signatures:\n    print(\"Required input signature not found.\")\n\nprediction_fn = interpreter.get_signature_runner(\"serving_default\")\noutput = prediction_fn(inputs=data)\nprediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\nprint(prediction_str)","metadata":{"id":"2KNUGXjBYi2A","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n!zip  submission.zip \"/kaggle/working/model.tflite\" \"/kaggle/working/inference_args.json\"","metadata":{"id":"aTeDBunuYi2A","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install /kaggle/input/tflite-wheels-2140/tflite_runtime_nightly-2.14.0.dev20230508-cp310-cp310-manylinux2014_x86_64.whl","metadata":{"id":"ZWcEOwClYi2A","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\n\nimport json\nimport pandas as pd\nimport tflite_runtime.interpreter as tflite\nimport numpy as np\nimport time\nfrom tqdm import tqdm\nimport Levenshtein as Lev\nimport glob\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"","metadata":{"id":"6W11OFc7Yi2A","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEL_FEATURES = json.load(open('/kaggle/working/inference_args.json'))['selected_columns']\n\ndef load_relevant_data_subset(pq_path):\n        return pd.read_parquet(pq_path, columns=SEL_FEATURES) #selected_columns)\n\nwith open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n    character_map = json.load(f)\nrev_character_map = {j:i for i,j in character_map.items()}\n\n\ndf_csv = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\n\nidx = 0\nsample = df_csv.loc[idx]\nloaded = load_relevant_data_subset('/kaggle/input/asl-fingerspelling/' + sample['path'])\nloaded = loaded[loaded.index==sample['sequence_id']].values\nprint(loaded.shape)\nframes = loaded\n","metadata":{"id":"zoac5lq4Yi2B","jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nst = time.time()\ncount=0\nmodel_time = 0\n\nN=0\nD=0\n\nfiles=sorted(glob.glob('/kaggle/input/asl-fingerspelling/train_landmarks/*.parquet'))[:6]\nfor f in files:\n    fid=int(f.split(\"/\")[-1].split(\".\")[0])\n    df = load_relevant_data_subset(f)\n    seq=df.index.drop_duplicates()\n    for ind in tqdm(seq):\n        sample=df_csv[(df_csv[\"sequence_id\"]==ind) & (df_csv[\"file_id\"]==fid)]\n        #print(sample)\n        loaded = df.loc[ind].values\n        count+=1\n        md_st = time.time()\n        \n        # out = infer_keras_model(loaded)[\"outputs\"] # original model\n\n        out = prediction_fn(inputs=loaded)[REQUIRED_OUTPUT] # tflite\n        \n        model_time += time.time() - md_st\n\n        prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(out, axis=1)])\n        assert out.ndim==2\n        assert out.shape[1]==59\n        assert out.dtype==np.float32\n        assert np.all(np.isfinite(out))\n        s1=sample[\"phrase\"].item()\n        s2=prediction_str\n        n = len(s1)\n        d = Lev.distance(s1,s2)\n        N=N+n\n        D=D+d\n        #print(ind,s1,s2,n,d)\nlev=(N-D)/N\nprint(f'Lev: {lev:.4f}')\nprint(f'Mean time: {(time.time() - st)/count:.3f}')\nprint(f'Mean time only infer: {model_time/count:.3f}')\n        \n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}