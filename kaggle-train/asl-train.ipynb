{"cells":[{"cell_type":"markdown","metadata":{},"source":["# For Colab"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":164,"status":"ok","timestamp":1689879102184,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"mDo2J7GkYk_-"},"outputs":[],"source":["from os import path\n","import sys\n","\n","IN_COLAB = 'google.colab' in sys.modules\n","drive_exists = path.exists(\"/content/drive\")\n","\n","if  IN_COLAB and (not drive_exists):\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":286177,"status":"ok","timestamp":1689879408289,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"rQXpEAvz38wK"},"outputs":[],"source":["input_exists=path.exists(\"/kaggle/input\")\n","if IN_COLAB and not input_exists:\n","    !mkdir -p \"/kaggle/input\"\n","    !cp -r /content/drive/MyDrive/kaggle-asl/*  /kaggle/input/\n","    "]},{"cell_type":"markdown","metadata":{"id":"Z-x7GxsSYi1y"},"source":["# Import the libraries"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-18T09:04:40.055733Z","iopub.status.busy":"2023-07-18T09:04:40.054786Z","iopub.status.idle":"2023-07-18T09:04:43.829827Z","shell.execute_reply":"2023-07-18T09:04:43.828466Z","shell.execute_reply.started":"2023-07-18T09:04:40.055676Z"},"executionInfo":{"elapsed":14532,"status":"ok","timestamp":1689879460111,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"eoljCzzvYi1z","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-07-21 06:50:47.200742: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:8893] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2023-07-21 06:50:47.200767: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2023-07-21 06:50:47.202516: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2023-07-21 06:50:47.435601: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-07-21 06:50:48.679440: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From /home/sronen/code/.venv/lib/python3.10/site-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n","Instructions for updating:\n","The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n","WARNING:tensorflow:From /home/sronen/code/.venv/lib/python3.10/site-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n","Instructions for updating:\n","The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import random\n","import psutil\n","import glob\n","import gc\n","import math\n","#from tensorflow.python.framework.ops import disable_eager_execution\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.832765Z","iopub.status.busy":"2023-07-18T09:04:43.832015Z","iopub.status.idle":"2023-07-18T09:04:43.840057Z","shell.execute_reply":"2023-07-18T09:04:43.83867Z","shell.execute_reply.started":"2023-07-18T09:04:43.832729Z"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1689879460112,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"UV3hsk-bYi1z","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-07-21 06:50:49.900209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-21 06:50:49.963675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-21 06:50:49.963898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"]}],"source":["gpus = tf.config.list_physical_devices(\"GPU\")\n","for gpu in gpus:\n","    tf.config.experimental.set_memory_growth(gpu, True)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-07-18T09:04:43.841953Z","iopub.status.busy":"2023-07-18T09:04:43.84152Z","iopub.status.idle":"2023-07-18T09:04:43.853631Z","shell.execute_reply":"2023-07-18T09:04:43.852101Z","shell.execute_reply.started":"2023-07-18T09:04:43.84192Z"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1689879460112,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"rZpjjT02Yi10","outputId":"d6aa95c0-6ccf-4a48-8b13-f58581fd8015","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["TensorFlow v2.14.0-dev20230718\n"]}],"source":["print(\"TensorFlow v\" + tf.__version__)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.856382Z","iopub.status.busy":"2023-07-18T09:04:43.855886Z","iopub.status.idle":"2023-07-18T09:04:43.869418Z","shell.execute_reply":"2023-07-18T09:04:43.868074Z","shell.execute_reply.started":"2023-07-18T09:04:43.856337Z"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1689879460112,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"0gUj7mAgYi10","trusted":true},"outputs":[],"source":["class MemoryUsageCallbackExtended(tf.keras.callbacks.Callback):\n","    \"\"\"Monitor memory usage on epoch begin and end, collect garbage\"\"\"\n","\n","    # def on_epoch_begin(self, epoch, logs=None):\n","    #    print(\"**Epoch {}**\".format(epoch))\n","    #    print(\n","    #        f\"Memory usage on epoch begin: {int(psutil.Process(os.getpid()).memory_info().rss)/1e9:.2f GB}\"\n","    #    )\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        print(\n","            f\"Memory usage on epoch end: {int(psutil.Process(os.getpid()).memory_info().rss)/1e9:.2f} GB\"\n","        )\n","        gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"eErQkrlpYi11"},"source":["# Scheduler"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.87415Z","iopub.status.busy":"2023-07-18T09:04:43.873737Z","iopub.status.idle":"2023-07-18T09:04:43.90059Z","shell.execute_reply":"2023-07-18T09:04:43.898986Z","shell.execute_reply.started":"2023-07-18T09:04:43.874119Z"},"executionInfo":{"elapsed":187,"status":"ok","timestamp":1689879460286,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"umV9OdZCYi11","trusted":true},"outputs":[],"source":["\n","class CosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    \"\"\"A LearningRateSchedule that uses a cosine decay with optional warmup.\n","\n","    See [Loshchilov & Hutter, ICLR2016](https://arxiv.org/abs/1608.03983),\n","    SGDR: Stochastic Gradient Descent with Warm Restarts.\n","\n","    For the idea of a linear warmup of our learning rate,\n","    see [Goyal et al.](https://arxiv.org/pdf/1706.02677.pdf).\n","\n","    When we begin training a model, we often want an initial increase in our\n","    learning rate followed by a decay. If `warmup_target` is an int, this\n","    schedule applies a linear increase per optimizer step to our learning rate\n","    from `initial_learning_rate` to `warmup_target` for a duration of\n","    `warmup_steps`. Afterwards, it applies a cosine decay function taking our\n","    learning rate from `warmup_target` to `alpha` for a duration of\n","    `decay_steps`. If `warmup_target` is None we skip warmup and our decay\n","    will take our learning rate from `initial_learning_rate` to `alpha`.\n","    It requires a `step` value to  compute the learning rate. You can\n","    just pass a TensorFlow variable that you increment at each training step.\n","\n","    The schedule is a 1-arg callable that produces a warmup followed by a\n","    decayed learning rate when passed the current optimizer step. This can be\n","    useful for changing the learning rate value across different invocations of\n","    optimizer functions.\n","\n","    Our warmup is computed as:\n","\n","    ```python\n","    def warmup_learning_rate(step):\n","        completed_fraction = step / warmup_steps\n","        total_delta = target_warmup - initial_learning_rate\n","        return completed_fraction * total_delta\n","    ```\n","\n","    And our decay is computed as:\n","\n","    ```python\n","    if warmup_target is None:\n","        initial_decay_lr = initial_learning_rate\n","    else:\n","        initial_decay_lr = warmup_target\n","\n","    def decayed_learning_rate(step):\n","        step = min(step, decay_steps)\n","        cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))\n","        decayed = (1 - alpha) * cosine_decay + alpha\n","        return initial_decay_lr * decayed\n","    ```\n","\n","    Example usage without warmup:\n","\n","    ```python\n","    decay_steps = 1000\n","    initial_learning_rate = 0.1\n","    lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n","        initial_learning_rate, decay_steps)\n","    ```\n","\n","    Example usage with warmup:\n","\n","    ```python\n","    decay_steps = 1000\n","    initial_learning_rate = 0\n","    warmup_steps = 1000\n","    target_learning_rate = 0.1\n","    lr_warmup_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n","        initial_learning_rate, decay_steps, warmup_target=target_learning_rate,\n","        warmup_steps=warmup_steps\n","    )\n","    ```\n","\n","    You can pass this schedule directly into a `tf.keras.optimizers.Optimizer`\n","    as the learning rate. The learning rate schedule is also serializable and\n","    deserializable using `tf.keras.optimizers.schedules.serialize` and\n","    `tf.keras.optimizers.schedules.deserialize`.\n","\n","    Returns:\n","      A 1-arg callable learning rate schedule that takes the current optimizer\n","      step and outputs the decayed learning rate, a scalar `Tensor` of the same\n","      type as `initial_learning_rate`.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        initial_learning_rate,\n","        decay_steps,\n","        alpha=0.0,\n","        name=None,\n","        warmup_target=None,\n","        warmup_steps=0,\n","    ):\n","        \"\"\"Applies cosine decay to the learning rate.\n","\n","        Args:\n","          initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a\n","            Python int. The initial learning rate.\n","          decay_steps: A scalar `int32` or `int64` `Tensor` or a Python int.\n","            Number of steps to decay over.\n","          alpha: A scalar `float32` or `float64` `Tensor` or a Python int.\n","            Minimum learning rate value for decay as a fraction of\n","            `initial_learning_rate`.\n","          name: String. Optional name of the operation.  Defaults to\n","            'CosineDecay'.\n","          warmup_target: None or a scalar `float32` or `float64` `Tensor` or a\n","            Python int. The target learning rate for our warmup phase. Will cast\n","            to the `initial_learning_rate` datatype. Setting to None will skip\n","            warmup and begins decay phase from `initial_learning_rate`.\n","            Otherwise scheduler will warmup from `initial_learning_rate` to\n","            `warmup_target`.\n","          warmup_steps: A scalar `int32` or `int64` `Tensor` or a Python int.\n","            Number of steps to warmup over.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.initial_learning_rate = initial_learning_rate\n","        self.decay_steps = decay_steps\n","        self.alpha = alpha\n","        self.name = name\n","        self.warmup_steps = warmup_steps\n","        self.warmup_target = warmup_target\n","\n","    def _decay_function(self, step, decay_steps, decay_from_lr, dtype):\n","        with tf.name_scope(self.name or \"CosineDecay\"):\n","            completed_fraction = step / decay_steps\n","            tf_pi = tf.constant(math.pi, dtype=dtype)\n","            cosine_decayed = 0.5 * (1.0 + tf.cos(tf_pi * completed_fraction))\n","            decayed = (1 - self.alpha) * cosine_decayed + self.alpha\n","            return tf.multiply(decay_from_lr, decayed)\n","\n","    def _warmup_function(self, step, warmup_steps, warmup_target, initial_learning_rate):\n","        with tf.name_scope(self.name or \"CosineDecay\"):\n","            completed_fraction = step / warmup_steps\n","            total_step_delta = warmup_target - initial_learning_rate\n","            return total_step_delta * completed_fraction + initial_learning_rate\n","\n","    def __call__(self, step):\n","        with tf.name_scope(self.name or \"CosineDecay\"):\n","            initial_learning_rate = tf.convert_to_tensor(\n","                self.initial_learning_rate, name=\"initial_learning_rate\"\n","            )\n","            dtype = initial_learning_rate.dtype\n","            decay_steps = tf.cast(self.decay_steps, dtype)\n","            global_step_recomp = tf.cast(step, dtype)\n","\n","            if self.warmup_target is None:\n","                global_step_recomp = tf.minimum(global_step_recomp, decay_steps)\n","                return self._decay_function(\n","                    global_step_recomp,\n","                    decay_steps,\n","                    initial_learning_rate,\n","                    dtype,\n","                )\n","\n","            warmup_target = tf.cast(self.warmup_target, dtype)\n","            warmup_steps = tf.cast(self.warmup_steps, dtype)\n","\n","            global_step_recomp = tf.minimum(global_step_recomp, decay_steps + warmup_steps)\n","\n","            return tf.cond(\n","                global_step_recomp < warmup_steps,\n","                lambda: self._warmup_function(\n","                    global_step_recomp,\n","                    warmup_steps,\n","                    warmup_target,\n","                    initial_learning_rate,\n","                ),\n","                lambda: self._decay_function(\n","                    global_step_recomp - warmup_steps,\n","                    decay_steps,\n","                    warmup_target,\n","                    dtype,\n","                ),\n","            )\n","\n","    def get_config(self):\n","        return {\n","            \"initial_learning_rate\": self.initial_learning_rate,\n","            \"decay_steps\": self.decay_steps,\n","            \"alpha\": self.alpha,\n","            \"name\": self.name,\n","            \"warmup_target\": self.warmup_target,\n","            \"warmup_steps\": self.warmup_steps,\n","        }\n"]},{"cell_type":"markdown","metadata":{"id":"Vh1J_bDCYi13"},"source":["# Constants"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.902869Z","iopub.status.busy":"2023-07-18T09:04:43.902415Z","iopub.status.idle":"2023-07-18T09:04:43.928871Z","shell.execute_reply":"2023-07-18T09:04:43.927451Z","shell.execute_reply.started":"2023-07-18T09:04:43.902837Z"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1689879460287,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"5icpG0BeYi13","trusted":true},"outputs":[],"source":["def get_char_dict():\n","    char_dict = {\n","        \" \": 0,\n","        \"!\": 1,\n","        \"#\": 2,\n","        \"$\": 3,\n","        \"%\": 4,\n","        \"&\": 5,\n","        \"'\": 6,\n","        \"(\": 7,\n","        \")\": 8,\n","        \"*\": 9,\n","        \"+\": 10,\n","        \",\": 11,\n","        \"-\": 12,\n","        \".\": 13,\n","        \"/\": 14,\n","        \"0\": 15,\n","        \"1\": 16,\n","        \"2\": 17,\n","        \"3\": 18,\n","        \"4\": 19,\n","        \"5\": 20,\n","        \"6\": 21,\n","        \"7\": 22,\n","        \"8\": 23,\n","        \"9\": 24,\n","        \":\": 25,\n","        \";\": 26,\n","        \"=\": 27,\n","        \"?\": 28,\n","        \"@\": 29,\n","        \"[\": 30,\n","        \"_\": 31,\n","        \"a\": 32,\n","        \"b\": 33,\n","        \"c\": 34,\n","        \"d\": 35,\n","        \"e\": 36,\n","        \"f\": 37,\n","        \"g\": 38,\n","        \"h\": 39,\n","        \"i\": 40,\n","        \"j\": 41,\n","        \"k\": 42,\n","        \"l\": 43,\n","        \"m\": 44,\n","        \"n\": 45,\n","        \"o\": 46,\n","        \"p\": 47,\n","        \"q\": 48,\n","        \"r\": 49,\n","        \"s\": 50,\n","        \"t\": 51,\n","        \"u\": 52,\n","        \"v\": 53,\n","        \"w\": 54,\n","        \"x\": 55,\n","        \"y\": 56,\n","        \"z\": 57,\n","        \"~\": 58,\n","    }\n","    char_dict[\"P\"] = 59\n","    #char_dict[\"SOS\"] = 60\n","    #char_dict[\"EOS\"] = 61\n","    return char_dict\n","\n","\n","class Constants:\n","    ROWS_PER_FRAME = 543\n","    MAX_STRING_LEN = 50\n","    INPUT_PAD = -100.0\n","    char_dict = get_char_dict()\n","    LABEL_PAD = char_dict[\"P\"]\n","    inv_dict = {v: k for k, v in char_dict.items()}\n","    NOSE = [1, 2, 98, 327]\n","\n","    REYE = [33, 7, 163, 144, 145, 153, 154, 155, 133, 246, 161, 160, 159, 158, 157, 173]\n","    LEYE = [263, 249, 390, 373, 374, 380, 381, 382, 362, 466, 388, 387, 386, 385, 384, 398]\n","\n","    LHAND = list(range(468, 489))\n","    RHAND = list(range(522, 543))\n","\n","    LNOSE = [98]\n","    RNOSE = [327]\n","\n","    LLIP = [84, 181, 91, 146, 61, 185, 40, 39, 37, 87, 178, 88, 95, 78, 191, 80, 81, 82]\n","    RLIP = [\n","        314,\n","        405,\n","        321,\n","        375,\n","        291,\n","        409,\n","        270,\n","        269,\n","        267,\n","        317,\n","        402,\n","        318,\n","        324,\n","        308,\n","        415,\n","        310,\n","        311,\n","        312,\n","    ]\n","    POSE = [500, 502, 504, 501, 503, 505, 512, 513]\n","    LPOSE = [513, 505, 503, 501]\n","    RPOSE = [512, 504, 502, 500]\n","\n","    POINT_LANDMARKS_PARTS = [LHAND, RHAND, LLIP, RLIP, LPOSE, RPOSE, NOSE, REYE, LEYE]\n","    # POINT_LANDMARKS_PARTS = [LHAND, RHAND, NOSE]\n","    POINT_LANDMARKS = [item for sublist in POINT_LANDMARKS_PARTS for item in sublist]\n","    parts = {\n","        \"LLIP\": LLIP,\n","        \"RLIP\": RLIP,\n","        \"LHAND\": LHAND,\n","        \"RHAND\": RHAND,\n","        \"LPOSE\": LPOSE,\n","        \"RPOSE\": RPOSE,\n","        \"LNOSE\": LNOSE,\n","        \"RNOSE\": RNOSE,\n","        \"REYE\": REYE,\n","        \"LEYE\": LEYE,\n","    }\n","\n","    LANDMARK_INDICES = {}  # type: ignore\n","    for part in parts:\n","        LANDMARK_INDICES[part] = []\n","        for landmark in parts[part]:\n","            if landmark in POINT_LANDMARKS:\n","                LANDMARK_INDICES[part].append(POINT_LANDMARKS.index(landmark))\n","\n","    CENTER_LANDMARKS = LNOSE + RNOSE\n","    CENTER_INDICES = LANDMARK_INDICES[\"LNOSE\"] + LANDMARK_INDICES[\"RNOSE\"]\n","\n","    NUM_NODES = len(POINT_LANDMARKS)\n","    NUM_INPUT_FEATURES = 2 * NUM_NODES # (x,y)\n","    CHANNELS = 6 * NUM_NODES #(x,y,dx,dy,dx2,dy2)\n"]},{"cell_type":"markdown","metadata":{"id":"MMEA9LflYi14"},"source":["# Utils"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.930812Z","iopub.status.busy":"2023-07-18T09:04:43.93046Z","iopub.status.idle":"2023-07-18T09:04:43.966369Z","shell.execute_reply":"2023-07-18T09:04:43.964929Z","shell.execute_reply.started":"2023-07-18T09:04:43.930782Z"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1689879460288,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"P-kprZ1_Yi14","trusted":true},"outputs":[],"source":["\n","# Seed all random number generators\n","def seed_everything(seed=42):\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    tf.random.set_seed(seed)\n","\n","\n","def selected_columns(file_example):\n","    df = pd.read_parquet(file_example)\n","    selected_x = df.columns[[x + 1 for x in Constants.POINT_LANDMARKS]].tolist()\n","    selected_y = [c.replace(\"x\", \"y\") for c in selected_x]\n","    selected = []\n","    for i in range(Constants.NUM_NODES):\n","        selected.append(selected_x[i])\n","        selected.append(selected_y[i])\n","    return selected  # x1,y1,x2,y2,...\n","\n","\n","\n","def num_to_char_fn(y):\n","    return [Constants.inv_dict.get(x, \"\") for x in y]\n","\n","\n","# A callback class to output a few transcriptions during training\n","class CallbackEval(tf.keras.callbacks.Callback):\n","    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n","\n","    def __init__(self, model, dataset):\n","        super().__init__()\n","        self.dataset = dataset\n","        self.model = model\n","\n","    def on_epoch_end(self, epoch: int, logs=None):\n","        predictions = []\n","        targets = []\n","        for batch in self.dataset:\n","            X, y = batch\n","            batch_predictions = self.model(X)\n","            batch_predictions = decode_batch_predictions(batch_predictions)\n","            predictions.extend(batch_predictions)\n","            for label in y:\n","                label = \"\".join(num_to_char_fn(label.numpy()))\n","                targets.append(label)\n","        print(\"-\" * 100)\n","        # for i in np.random.randint(0, len(predictions), 2):\n","        for i in range(10):\n","            print(f\"Target    : {targets[i]}\")\n","            print(f\"Prediction: {predictions[i]}, len: {len(predictions[i])}\")\n","            print(\"-\" * 100)\n","\n","\n","def decode_phrase(pred):\n","    # decode cts prediction by prunning\n","    # (T,CHAR_NUMS)\n","    x = tf.argmax(pred, axis=1)\n","    paddings = tf.constant(\n","        [\n","            [0, 1],\n","        ]\n","    )\n","    x = tf.pad(x, paddings)\n","    diff = tf.not_equal(x[:-1], x[1:])\n","    adjacent_indices = tf.where(diff)[:, 0]\n","    x = tf.gather(x, adjacent_indices)\n","    mask = x != Constants.LABEL_PAD\n","    x = tf.boolean_mask(x, mask, axis=0)\n","    return x\n","\n","\n","# A utility function to decode the output of the network\n","def decode_batch_predictions(pred):\n","    output_text = []\n","    for result in pred:\n","        result = \"\".join(num_to_char_fn(decode_phrase(result).numpy()))\n","        output_text.append(result)\n","    return output_text\n","\n","\n","\n","\n","def code_to_label(label_code):\n","    label = [Constants.inv_dict[x] for x in label_code if Constants.inv_dict[x] != \"P\"]\n","    label = \"\".join(label)\n","    return label\n","\n","\n","def convert_to_strings(batch_label_code):\n","    output = []\n","    for label_code in batch_label_code:\n","        output.append(code_to_label(label_code))\n","    return output\n","\n","\n","def global_metric(val_ds, model):\n","    global_N, global_D = 0, 0\n","    count = 0\n","    metric = LevDistanceMetric()\n","    for batch in val_ds:\n","        count += 1\n","        print(count)\n","        feature, label = batch\n","        logits = model(feature)\n","        _, _, D = batch_edit_distance(label, logits)\n","        metric.update_state(label, logits)\n","\n","    result = metric.result().numpy()\n","\n","    return result\n","\n","\n","def sparse_from_dense_ignore_value(dense_tensor):\n","    mask = tf.not_equal(dense_tensor, Constants.LABEL_PAD)\n","    indices = tf.where(mask)\n","    values = tf.boolean_mask(dense_tensor, mask)\n","\n","    return tf.SparseTensor(indices, values, tf.shape(dense_tensor, out_type=tf.int64))\n","\n","\n","def batch_edit_distance(y_true, y_logits):\n","    blank = Constants.LABEL_PAD\n","    #y_true=tf.ensure_shape(y_true,(128,Constants.MAX_STRING_LEN))\n","    #y_logits=tf.ensure_shape(y_logits,(128,128,60))\n","    #tf.print(\"edit distance true shape\",tf.shape(y_true))\n","    #tf.print(\"edit distance logits shape\",tf.shape(y_logits))\n","\n","    B = tf.shape(y_logits)[0]\n","    seq_length = tf.shape(y_logits)[1]\n","    to_decode = tf.transpose(y_logits, perm=[1, 0, 2])\n","    sequence_length = tf.fill(dims=[B], value=seq_length)\n","    hypothesis = tf.nn.ctc_greedy_decoder(\n","        tf.cast(to_decode, tf.float32), sequence_length, blank_index=blank\n","    )[0][\n","        0\n","    ]  # full is [B,...]\n","    truth = sparse_from_dense_ignore_value(y_true)  # full is [B,...]\n","    truth = tf.cast(truth, tf.int64)\n","    edit_dist = tf.edit_distance(hypothesis, truth, normalize=False)\n","\n","    non_ignore_mask = tf.not_equal(y_true, blank)\n","    N = tf.reduce_sum(tf.cast(non_ignore_mask, tf.float32))\n","    D = tf.reduce_sum(edit_dist)\n","    result = (N - D) / N\n","    result = tf.clip_by_value(result, 0.0, 1.0)\n","    return result, N, D\n","\n","\n","class LevDistanceMetric(tf.keras.metrics.Metric):\n","    def __init__(self, name=\"Lev\", **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        self.distance = self.add_weight(name=\"dist\", initializer=\"zeros\")\n","        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n","\n","    def update_state(self, y_true, y_logits, sample_weight=None):\n","        # if using with keras compile, make sure the model outputs logits, not softmax probabilities\n","        _, N, D = batch_edit_distance(y_true, y_logits)\n","        self.distance.assign_add(D)\n","        self.count.assign_add(N)\n","\n","    def result(self):\n","        result = (self.count - self.distance) / self.count\n","        result = tf.clip_by_value(result, 0.0, 1.0)\n","        return result\n","\n","    def reset_state(self):\n","        self.count.assign(0.0)\n","        self.distance.assign(0.0)\n","\n","\n","\n","class SWA(tf.keras.callbacks.Callback):\n","    # Stochastic Weight Averaging\n","    def __init__(\n","        self,\n","        save_name,\n","        swa_epochs=[],\n","        strategy=None,\n","        train_ds=None,\n","        valid_ds=None,\n","        train_steps=1000,\n","    ):\n","        super().__init__()\n","        self.swa_epochs = swa_epochs\n","        self.swa_weights = None\n","        self.save_name = save_name\n","        self.train_ds = train_ds\n","        self.valid_ds = valid_ds\n","        self.train_steps = train_steps\n","        self.strategy = strategy\n","\n","    # @tf.function(jit_compile=True)\n","    def train_step(self, iterator):\n","        \"\"\"The step function for one training step.\"\"\"\n","\n","        def step_fn(inputs):\n","            \"\"\"The computation to run on each device.\"\"\"\n","            x, y = inputs\n","            _ = self.model(x, training=True)\n","\n","        for x in iterator:\n","            self.strategy.run(step_fn, args=(x,))\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        if epoch in self.swa_epochs:\n","            if self.swa_weights is None:\n","                self.swa_weights = self.model.get_weights()\n","            else:\n","                w = self.model.get_weights()\n","                for i in range(len(self.swa_weights)):\n","                    self.swa_weights[i] += w[i]\n","\n","    def on_train_end(self, logs=None):\n","        if len(self.swa_epochs):\n","            print(\"applying SWA...\")\n","            for i in range(len(self.swa_weights)):\n","                self.swa_weights[i] = self.swa_weights[i] / len(self.swa_epochs)\n","            self.model.set_weights(self.swa_weights)\n","            if self.train_ds is not None:  # for the re-calculation of running mean and var\n","                self.train_step(self.train_ds.take(self.train_steps))\n","            print(f\"save SWA weights to {self.save_name}-SWA.h5\")\n","            self.model.save_weights(f\"{self.save_name}-SWA.h5\")\n","            if self.valid_ds is not None:\n","                self.model.evaluate(self.valid_ds)\n","\n","\n","class AWP(tf.keras.Model):\n","    # Adversarial Weight Perturbation\n","    def __init__(self, *args, delta=0.1, eps=1e-4, start_step=0, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.delta = delta\n","        self.eps = eps\n","        self.start_step = start_step\n","\n","    def train_step_awp(self, data):\n","        # Unpack the data. Its structure depends on your model and\n","        # on what you pass to `fit()`.\n","        x, y = data\n","\n","        with tf.GradientTape() as tape:\n","            y_pred = self(x, training=True)\n","            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n","        params = self.trainable_variables\n","        params_gradients = tape.gradient(loss, self.trainable_variables)\n","        for i in range(len(params_gradients)):\n","            grad = tf.zeros_like(params[i]) + params_gradients[i]\n","            delta = tf.math.divide_no_nan(\n","                self.delta * grad, tf.math.sqrt(tf.reduce_sum(grad**2)) + self.eps\n","            )\n","            self.trainable_variables[i].assign_add(delta)\n","        with tf.GradientTape() as tape2:\n","            y_pred = self(x, training=True)\n","            new_loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n","            if hasattr(self.optimizer, \"get_scaled_loss\"):\n","                new_loss = self.optimizer.get_scaled_loss(new_loss)\n","\n","        gradients = tape2.gradient(new_loss, self.trainable_variables)\n","        if hasattr(self.optimizer, \"get_unscaled_gradients\"):\n","            gradients = self.optimizer.get_unscaled_gradients(gradients)\n","        for i in range(len(params_gradients)):\n","            grad = tf.zeros_like(params[i]) + params_gradients[i]\n","            delta = tf.math.divide_no_nan(\n","                self.delta * grad, tf.math.sqrt(tf.reduce_sum(grad**2)) + self.eps\n","            )\n","            self.trainable_variables[i].assign_sub(delta)\n","        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","        # self_loss.update_state(loss)\n","        self.compiled_metrics.update_state(y, y_pred)\n","        return {m.name: m.result() for m in self.metrics}\n","\n","    def train_step(self, data):\n","        return tf.cond(\n","            self._train_counter < self.start_step,\n","            lambda: super(AWP, self).train_step(data),\n","            lambda: self.train_step_awp(data),\n","        )\n"]},{"cell_type":"markdown","metadata":{"id":"YQAPz56nYi15"},"source":["# Model"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:43.969375Z","iopub.status.busy":"2023-07-18T09:04:43.968934Z","iopub.status.idle":"2023-07-18T09:04:44.024972Z","shell.execute_reply":"2023-07-18T09:04:44.023605Z","shell.execute_reply.started":"2023-07-18T09:04:43.969338Z"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1689879460289,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"wnoZyLAcYi15","trusted":true},"outputs":[],"source":["\n","class CTCLoss(tf.keras.losses.Loss):\n","    def __init__(self, pad_token_idx,batch_size,max_string_len,output_dim,output_steps):\n","        self.pad_token_idx = pad_token_idx\n","        self.batch_size=batch_size\n","        self.max_string_len=max_string_len\n","        self.output_steps=output_steps\n","        self.output_dim=output_dim\n","        super().__init__()\n","\n","    def call(self, labels, logits):\n","\n","        #logits=tf.ensure_shape(logits,(self.batch_size,self.output_steps,self.output_dim))\n","        #labels=tf.ensure_shape(labels,(self.batch_size,self.max_string_len))\n","        \n","        label_length = tf.reduce_sum(tf.cast(labels != self.pad_token_idx, tf.int32), axis=-1)\n","        logit_length = tf.ones(tf.shape(logits)[0], dtype=tf.int32) * tf.shape(logits)[1]\n","\n","        ctc_loss = tf.nn.ctc_loss(\n","            labels=labels,\n","            logits=logits,\n","            label_length=label_length,\n","            logit_length=logit_length,\n","            blank_index=self.pad_token_idx,\n","            logits_time_major=False,\n","        )\n","\n","        return ctc_loss\n","\n","\n","class ECA(tf.keras.layers.Layer):\n","    # Efficient Channel Attention\n","    def __init__(self, kernel_size=5, **kwargs):\n","        super().__init__(**kwargs)\n","        self.supports_masking = True\n","        self.kernel_size = kernel_size\n","        self.conv = tf.keras.layers.Conv1D(\n","            1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False\n","        )\n","\n","    def call(self, inputs, mask=None):\n","        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n","        nn = tf.expand_dims(nn, -1)\n","        nn = self.conv(nn)\n","        nn = tf.squeeze(nn, -1)\n","        nn = tf.nn.sigmoid(nn)\n","        nn = nn[:, None, :]\n","        return inputs * nn\n","\n","\n","class LateDropout(tf.keras.layers.Layer):\n","    def __init__(self, rate, noise_shape=None, start_step=0, **kwargs):\n","        super().__init__(**kwargs)\n","        self.supports_masking = True\n","        self.rate = rate\n","        self.start_step = start_step\n","        self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n","\n","    def build(self, input_shape):\n","        super().build(input_shape)\n","        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n","        self._train_counter = tf.Variable(0, dtype=\"int64\", aggregation=agg, trainable=False)\n","\n","    def call(self, inputs, training=False):\n","        x = tf.cond(\n","            self._train_counter < self.start_step,\n","            lambda: inputs,\n","            lambda: self.dropout(inputs, training=training),\n","        )\n","        if training:\n","            self._train_counter.assign_add(1)\n","        return x\n","\n","\n","class CausalDWConv1D(tf.keras.layers.Layer):\n","    # Causal Depth Wise Convolution\n","    def __init__(\n","        self,\n","        kernel_size=17,\n","        dilation_rate=1,\n","        use_bias=False,\n","        depthwise_initializer=\"glorot_uniform\",\n","        name=\"\",\n","        **kwargs,\n","    ):\n","        super().__init__(name=name, **kwargs)\n","        self.causal_pad = tf.keras.layers.ZeroPadding1D(\n","            (dilation_rate * (kernel_size - 1), 0), name=name + \"_pad\"\n","        )\n","        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n","            kernel_size,\n","            strides=1,\n","            dilation_rate=dilation_rate,\n","            padding=\"valid\",\n","            use_bias=use_bias,\n","            depthwise_initializer=depthwise_initializer,\n","            name=name + \"_dwconv\",\n","        )\n","        self.supports_masking = True\n","\n","    def call(self, inputs):\n","        x = self.causal_pad(inputs)\n","        x = self.dw_conv(x)\n","        return x\n","\n","\n","def Conv1DBlock(\n","    channel_size,\n","    kernel_size,\n","    dilation_rate=1,\n","    drop_rate=0.0,\n","    expand_ratio=2,\n","    # se_ratio=0.25,\n","    activation=\"swish\",\n","    name=None,\n","):\n","    \"\"\"\n","    efficient conv1d block, @hoyso48\n","    \"\"\"\n","    if name is None:\n","        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n","\n","    # Expansion phase\n","    def apply(inputs):\n","        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n","        channels_expand = channels_in * expand_ratio\n","\n","        skip = inputs\n","\n","        x = tf.keras.layers.Dense(\n","            channels_expand, use_bias=True, activation=activation, name=name + \"_expand_conv\"\n","        )(inputs)\n","\n","        # Depthwise Convolution\n","        x = CausalDWConv1D(\n","            kernel_size, dilation_rate=dilation_rate, use_bias=False, name=name + \"_dwconv\"\n","        )(x)\n","\n","        x = tf.keras.layers.LayerNormalization(name=name + \"_bn\")(x)\n","\n","        x = ECA()(x)  # efficient channel attention\n","\n","        x = tf.keras.layers.Dense(channel_size, use_bias=True, name=name + \"_project_conv\")(x)\n","\n","        if drop_rate > 0:\n","            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + \"_drop\")(x)\n","\n","        if channels_in == channel_size:\n","            x = tf.keras.layers.add([x, skip], name=name + \"_add\")\n","        return x\n","\n","    return apply\n","\n","\n","class MultiHeadSelfAttention(tf.keras.layers.Layer):\n","    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n","        super().__init__(**kwargs)\n","        self.dim = dim\n","        self.scale = self.dim**-0.5\n","        self.num_heads = num_heads\n","        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n","        self.drop1 = tf.keras.layers.Dropout(dropout)\n","        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        qkv = self.qkv(inputs)\n","        qkv = tf.keras.layers.Permute((2, 1, 3))(\n","            tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv)\n","        )\n","        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n","\n","        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n","\n","        if mask is not None:\n","            mask = mask[:, None, None, :]\n","\n","        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n","        attn = self.drop1(attn)\n","\n","        x = attn @ v\n","        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n","        x = self.proj(x)\n","        return x\n","\n","\n","def TransformerBlock(\n","    dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation=\"swish\"\n","):\n","    def apply(inputs):\n","        x = inputs\n","        x = tf.keras.layers.LayerNormalization()(x)\n","        x = MultiHeadSelfAttention(dim=dim, num_heads=num_heads, dropout=attn_dropout)(x)\n","        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1))(x)\n","        x = tf.keras.layers.Add()([inputs, x])\n","        attn_out = x\n","\n","        x = tf.keras.layers.LayerNormalization()(x)\n","        x = tf.keras.layers.Dense(dim * expand, use_bias=False, activation=activation)(x)\n","        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n","        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1))(x)\n","        x = tf.keras.layers.Add()([attn_out, x])\n","        return x\n","\n","    return apply\n","\n","\n","def build_model1(\n","    output_dim,\n","    max_len=64,\n","    dropout_step=0,\n","    dim=192,\n","    input_pad=-100,\n","    with_transformer=False,\n","    drop_rate=0.2,\n","):\n","    inp = tf.keras.Input(shape=(max_len, Constants.CHANNELS), dtype=tf.float32, name=\"inputs\")\n","    x = tf.keras.layers.Masking(mask_value=input_pad, input_shape=(max_len, Constants.CHANNELS))(\n","        inp\n","    )\n","    ksize = 17\n","    x = tf.keras.layers.Dense(dim, use_bias=False, name=\"stem_conv\")(x)\n","    x = tf.keras.layers.LayerNormalization(name=\"stem_bn\")(x)\n","\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    if with_transformer:\n","        x = TransformerBlock(dim, expand=2)(x)\n","\n","    #x = tf.keras.layers.AvgPool1D(2, 2)(x)\n","\n","\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","    if with_transformer:\n","        x = TransformerBlock(dim, expand=2)(x)\n","\n","    x = tf.keras.layers.AvgPool1D(2, 2)(x)\n","    x2=tf.keras.layers.Dense(dim)(x)\n","    lstm2 = tf.keras.layers.LSTM(units=output_dim, return_sequences=True)\n","    x3 = tf.keras.layers.Bidirectional(lstm2)(x2)\n","    x3 = tf.keras.layers.Dense(output_dim)(x3)\n","    soft = tf.keras.activations.softmax(x3)\n","    logsoft=tf.nn.log_softmax(x3,name=\"internal_log_soft\")\n","    x2=tf.keras.layers.Dense(dim)(soft)+x2\n","\n","    if dim == 384:  # for the 4x sized model\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        if with_transformer:\n","            x = TransformerBlock(dim, expand=2)(x)\n","\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n","        if with_transformer:\n","            x = TransformerBlock(dim, expand=2)(x)\n","\n","    lstm = tf.keras.layers.LSTM(units=output_dim, return_sequences=True)\n","    x = tf.keras.layers.Bidirectional(lstm)(x)\n","    x = LateDropout(0.4, start_step=dropout_step)(x)\n","    # x = tf.keras.layers.LayerNormalization()(x)\n","\n","    output = tf.keras.layers.Dense(output_dim, activation=\"log_softmax\",name=\"final_logsoft\")(x)  # logits\n","\n","    model = tf.keras.Model(inp, outputs=[output,logsoft])\n","    #model = tf.keras.Model(inp, outputs=output)\n","    return model\n","\n","\n","def get_model(output_dim, max_len, dim, input_pad,dropout_step,drop_rate):\n","\n","    model = build_model1(output_dim, max_len=max_len, input_pad=input_pad, dim=dim,  dropout_step=dropout_step,drop_rate=drop_rate)\n","                         \n","    return model\n"]},{"cell_type":"markdown","metadata":{"id":"piKWNcVHYi15"},"source":["# Configuration"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.026971Z","iopub.status.busy":"2023-07-18T09:04:44.026546Z","iopub.status.idle":"2023-07-18T09:04:44.044853Z","shell.execute_reply":"2023-07-18T09:04:44.043616Z","shell.execute_reply.started":"2023-07-18T09:04:44.026941Z"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1689879460290,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"7e1LGyFWYi16","trusted":true},"outputs":[],"source":["\n","def get_strategy():\n","    logical_devices = tf.config.list_logical_devices()\n","    # Check if TPU is available\n","\n","    gpu_available = any(\"GPU\" in device.name for device in logical_devices)\n","    strategy = None\n","    is_tpu = False\n","    try:\n","        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","        print(\"Running on TPU \", tpu.master())\n","        is_tpu = True\n","    except ValueError:\n","        is_tpu = False\n","\n","    if is_tpu:\n","        tf.config.experimental_connect_to_cluster(tpu)\n","        tf.tpu.experimental.initialize_tpu_system(tpu)\n","\n","        print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n","\n","        strategy = tf.distribute.TPUStrategy(tpu)\n","        #disable_eager_execution()  # LSTM layer can't use bfloat16 unless we do this.\n","\n","    else:\n","        if gpu_available:\n","\n","            ngpu = len(gpus)\n","            print(\"Num GPUs Available: \", ngpu)\n","            if ngpu > 1:\n","                strategy = tf.distribute.MirroredStrategy()\n","            else:\n","                strategy = tf.distribute.get_strategy()\n","\n","        else:\n","            print(\"Runing on CPU\")\n","            strategy = tf.distribute.get_strategy()\n","    replicas = strategy.num_replicas_in_sync\n","\n","    print(f\"get strategy replicas: {replicas}\")\n","\n","    return strategy, replicas, is_tpu\n","\n","\n","class CFG:\n","    # These 3 variables are update dynamically later by calling update_config_with_strategy.\n","    strategy = None  # type: ignore\n","    replicas = 1\n","    is_tpu = False\n","\n","    save_output = True\n","    log_path = \"/kaggle/working/\"\n","    input_path = \"/kaggle/input/asl-fingerspelling/\"\n","    output_path = \"/kaggle/working/\"\n","\n","    seed = 42\n","    verbose = 1  # 0) silent 1) progress bar 2) one line per epoch\n","\n","    # max number of frames\n","    #max_len = 256\n","    max_len = 256\n","    replicas = 1\n","\n","    lr = 5e-4   # 5e-4\n","    weight_decay = 1e-4  # 4e-4\n","    epochs = 200\n","    batch_size=128\n","    snapshot_epochs = []  # type: ignore\n","    swa_epochs = list(range(epochs//2,epochs+1))\n","\n","    fp16=True\n","\n","    awp = False\n","    awp_lambda = 0.02 #  aka delta\n","    awp_start_epoch = 15\n","    dropout_start_epoch = 15\n","    resume = 0\n","\n","    dim = 384\n","\n","    comment = f\"model-{dim}-seed{seed}\"\n","    output_dim = 60\n","    num_eval = 6\n","\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.046817Z","iopub.status.busy":"2023-07-18T09:04:44.046403Z","iopub.status.idle":"2023-07-18T09:04:44.063046Z","shell.execute_reply":"2023-07-18T09:04:44.062007Z","shell.execute_reply.started":"2023-07-18T09:04:44.046786Z"},"executionInfo":{"elapsed":226,"status":"ok","timestamp":1689879460503,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"HCKh-AxTYi16","trusted":true},"outputs":[],"source":["\n","def update_config_with_strategy(config):\n","    # cfg is configuration instance\n","    strategy, replicas, is_tpu = get_strategy()\n","    config.strategy = strategy\n","    config.replicas = replicas\n","    config.is_tpu = is_tpu\n","    config.lr = config.lr * replicas\n","    config.batch_size = config.batch_size * replicas\n","    return config"]},{"cell_type":"markdown","metadata":{"id":"5msRCmt0Yi16"},"source":["# Training"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.065242Z","iopub.status.busy":"2023-07-18T09:04:44.064883Z","iopub.status.idle":"2023-07-18T09:04:44.114564Z","shell.execute_reply":"2023-07-18T09:04:44.112776Z","shell.execute_reply.started":"2023-07-18T09:04:44.065211Z"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1689879460504,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"j_tHWSs8Yi16","trusted":true},"outputs":[],"source":["\n","def count_data_items(dataset):\n","    dataset_size = 0\n","    for _ in dataset:\n","        dataset_size += 1\n","    return dataset_size\n","\n","\n","def interp1d_(x, target_len):\n","    target_len = tf.maximum(1, target_len)\n","    x = tf.image.resize(x, (target_len, tf.shape(x)[1]))\n","    return x\n","\n","\n","def tf_nan_mean(x, axis=0, keepdims=False):\n","    return tf.reduce_sum(\n","        tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims\n","    ) / tf.reduce_sum(\n","        tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims\n","    )\n","\n","\n","def tf_nan_std(x, center=None, axis=0, keepdims=False):\n","    if center is None:\n","        center = tf_nan_mean(x, axis=axis, keepdims=True)\n","    d = x - center\n","    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n","\n","\n","def flip_lr(x):\n","    if x.shape[1] == Constants.ROWS_PER_FRAME:\n","        LHAND = Constants.LHAND\n","        RHAND = Constants.RHAND\n","        LLIP = Constants.LLIP\n","        RLIP = Constants.RLIP\n","        LEYE = Constants.LEYE\n","        REYE = Constants.REYE\n","        LNOSE = Constants.LNOSE\n","        RNOSE = Constants.RNOSE\n","        LPOSE = Constants.LPOSE\n","        RPOSE = Constants.RPOSE\n","    else:\n","        LHAND = Constants.LANDMARK_INDICES[\"LHAND\"]\n","        RHAND = Constants.LANDMARK_INDICES[\"RHAND\"]\n","        LLIP = Constants.LANDMARK_INDICES[\"LLIP\"]\n","        RLIP = Constants.LANDMARK_INDICES[\"RLIP\"]\n","        LEYE = Constants.LANDMARK_INDICES[\"LEYE\"]\n","        REYE = Constants.LANDMARK_INDICES[\"REYE\"]\n","        LNOSE = Constants.LANDMARK_INDICES[\"LNOSE\"]\n","        RNOSE = Constants.LANDMARK_INDICES[\"RNOSE\"]\n","        LPOSE = Constants.LANDMARK_INDICES[\"LPOSE\"]\n","        RPOSE = Constants.LANDMARK_INDICES[\"RPOSE\"]\n","\n","    x, y = tf.unstack(x, axis=-1)\n","    x = 1 - x\n","    new_x = tf.stack([x, y], -1)\n","    new_x = tf.transpose(new_x, [1, 0, 2])\n","    lhand = tf.gather(new_x, LHAND, axis=0)\n","    rhand = tf.gather(new_x, RHAND, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LHAND)[..., None], rhand)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RHAND)[..., None], lhand)\n","    llip = tf.gather(new_x, LLIP, axis=0)\n","    rlip = tf.gather(new_x, RLIP, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LLIP)[..., None], rlip)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RLIP)[..., None], llip)\n","    lpose = tf.gather(new_x, LPOSE, axis=0)\n","    rpose = tf.gather(new_x, RPOSE, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LPOSE)[..., None], rpose)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RPOSE)[..., None], lpose)\n","    leye = tf.gather(new_x, LEYE, axis=0)\n","    reye = tf.gather(new_x, REYE, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LEYE)[..., None], reye)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(REYE)[..., None], leye)\n","    lnose = tf.gather(new_x, LNOSE, axis=0)\n","    rnose = tf.gather(new_x, RNOSE, axis=0)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LNOSE)[..., None], rnose)\n","    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RNOSE)[..., None], lnose)\n","    new_x = tf.transpose(new_x, [1, 0, 2])\n","    return new_x\n","\n","\n","def resample(x, rate=(0.8, 1.2)):\n","    rate = tf.random.uniform((), rate[0], rate[1])\n","    length = tf.shape(x)[0]\n","    new_size = tf.cast(rate * tf.cast(length, tf.float32), tf.int32)\n","    new_x = interp1d_(x, new_size)\n","    return new_x\n","\n","\n","def spatial_random_affine(\n","    xyz,\n","    scale=(0.8, 1.2),\n","    shear=(-0.1, 0.1),\n","    shift=(-0.1, 0.1),\n","    degree=(-20, 20),\n","):\n","    center = tf.constant([0.5, 0.5])\n","    if degree is not None:\n","        xy = xyz[..., :2]\n","        z = xyz[..., 2:]\n","        xy -= center\n","        degree = tf.random.uniform((), *degree)\n","        radian = degree / 180 * np.pi\n","        c = tf.math.cos(radian)\n","        s = tf.math.sin(radian)\n","        rotate_mat = tf.identity(\n","            [\n","                [c, s],\n","                [-s, c],\n","            ]\n","        )\n","        xy = xy @ rotate_mat\n","        xy = xy + center\n","        xyz = tf.concat([xy, z], axis=-1)\n","\n","    if scale is not None:\n","        scale = tf.random.uniform((), *scale)\n","        xyz = scale * xyz\n","\n","    if shear is not None:\n","        xy = xyz[..., :2]\n","        z = xyz[..., 2:]\n","        shear_x = shear_y = tf.random.uniform((), *shear)\n","        if tf.random.uniform(()) < 0.5:\n","            shear_x = 0.0\n","        else:\n","            shear_y = 0.0\n","        shear_mat = tf.identity([[1.0, shear_x], [shear_y, 1.0]])\n","        xy = xy @ shear_mat\n","        xyz = tf.concat([xy, z], axis=-1)\n","\n","    if shift is not None:\n","        shift = tf.random.uniform((), *shift)\n","        xyz = xyz + shift\n","\n","    return xyz\n","\n","\n","def temporal_mask(x, size=[1, 15], mask_value=float(\"nan\")):\n","    l0 = tf.shape(x)[0]\n","    if size[1] > l0 // 8:\n","        size[1] = l0 // 8\n","        if size[1] <= 1:\n","            size[1] = 2\n","    mask_size = tf.random.uniform((), *size, dtype=tf.int32)\n","    mask_offset = tf.random.uniform((), 0, tf.clip_by_value(l0 - mask_size, 1, l0), dtype=tf.int32)\n","    x = tf.tensor_scatter_nd_update(\n","        x,\n","        tf.range(mask_offset, mask_offset + mask_size)[..., None],\n","        tf.fill([mask_size, tf.shape(x)[1], 2], mask_value),\n","    )\n","    return x\n","\n","\n","def spatial_mask(x, size=(0.05, 0.2), mask_value=float(\"nan\")):\n","    mask_offset_y = tf.random.uniform(())\n","    mask_offset_x = tf.random.uniform(())\n","    mask_size = tf.random.uniform((), *size)\n","    mask_x = (mask_offset_x < x[..., 0]) & (x[..., 0] < mask_offset_x + mask_size)\n","    mask_y = (mask_offset_y < x[..., 1]) & (x[..., 1] < mask_offset_y + mask_size)\n","    mask = mask_x & mask_y\n","    x = tf.where(mask[..., None], mask_value, x)\n","    return x\n","\n","\n","\n","def augment_fn(x):\n","    # shape (T,F)\n","    x = tf.reshape(x, (tf.shape(x)[0], -1, 2))\n","    if tf.random.uniform(()) < 0.6:\n","        x = resample(x, (0.5, 1.5))\n","    if tf.random.uniform(()) < 0.6:\n","        x = flip_lr(x)\n","    if tf.random.uniform(()) < 0.6:\n","        x = spatial_random_affine(x)\n","    if tf.random.uniform(()) < 0.4:\n","        x = temporal_mask(x)\n","    if tf.random.uniform(()) < 0.4:\n","        x = spatial_mask(x)\n","    x = tf.reshape(x, (tf.shape(x)[0], -1))\n","    return x\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.117655Z","iopub.status.busy":"2023-07-18T09:04:44.117104Z","iopub.status.idle":"2023-07-18T09:04:44.142302Z","shell.execute_reply":"2023-07-18T09:04:44.140662Z","shell.execute_reply.started":"2023-07-18T09:04:44.117609Z"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1689879460504,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"jYUGdwEDYi17","trusted":true},"outputs":[],"source":["\n","class Preprocess(tf.keras.layers.Layer):\n","    def __init__(self, max_len, normalize=False, **kwargs):\n","        super().__init__(**kwargs)\n","        self.max_len = max_len\n","        self.center = Constants.CENTER_INDICES\n","        self.normalize = normalize\n","\n","    # preprocess a batch of data\n","    def call(self, x):\n","        # rank is 3: [B,T,F]\n","        # if your input is just [T,F], extend its dimesnion before calling.\n","\n","        x = tf.reshape(x, (tf.shape(x)[0], tf.shape(x)[1], Constants.NUM_NODES, 2))\n","        # dimensions now are [B,T,F//2,2]\n","\n","        x_selected = x\n","        if self.normalize:\n","            mean = tf_nan_mean(tf.gather(x, self.center, axis=2), axis=[1, 2], keepdims=True)\n","            mean = tf.where(tf.math.is_nan(mean), tf.constant(0.5, x.dtype), mean)\n","            std = tf_nan_std(x_selected, center=mean, axis=[1, 2], keepdims=True)\n","            x = (x_selected - mean) / std\n","        else:\n","            x = x_selected\n","\n","        dx = tf.cond(\n","            tf.shape(x)[1] > 1,\n","            lambda: tf.pad(x[:, 1:] - x[:, :-1], [[0, 0], [0, 1], [0, 0], [0, 0]]),\n","            lambda: tf.zeros_like(x),\n","        )\n","\n","        dx2 = tf.cond(\n","            tf.shape(x)[1] > 2,\n","            lambda: tf.pad(x[:, 2:] - x[:, :-2], [[0, 0], [0, 2], [0, 0], [0, 0]]),\n","            lambda: tf.zeros_like(x),\n","        )\n","        length = tf.shape(x)[1]\n","\n","        x = tf.concat(\n","            [\n","                tf.reshape(x, (-1, length, 2 * Constants.NUM_NODES)),  # x1,y1,x2,y2,...\n","                tf.reshape(dx, (-1, length, 2 * Constants.NUM_NODES)),\n","                tf.reshape(dx2, (-1, length, 2 * Constants.NUM_NODES)),\n","            ],\n","            axis=-1,\n","        )\n","\n","        # x1,y1,x2,y2,...dx1,dy1,dx2,dy2,...\n","        x = tf.where(tf.math.is_nan(x), tf.constant(0.0, x.dtype), x)\n","        return x\n","\n","\n","def pad_if_short(x, max_len):\n","    # shape (T,F)\n","    pad_len = max_len - tf.shape(x)[0]\n","    padding = tf.ones((pad_len, tf.shape(x)[1]), dtype=x.dtype) * Constants.INPUT_PAD\n","    x = tf.concat([x, padding], axis=0)\n","    return x\n","\n","\n","def shrink_if_long(x, max_len):\n","    # shape is [T,F]\n","    if tf.shape(x)[0] > max_len:\n","        # we need to extend the dimension to [T,F,channels]  for tf.image.resize\n","        x = tf.image.resize(x[..., None], (max_len, tf.shape(x)[1]))\n","        x = tf.squeeze(x, axis=2)\n","\n","    return x\n","\n","def preprocess(x, max_len, do_pad=True):\n","    # shape (T,F)\n","    x = shrink_if_long(x, max_len=max_len)\n","    # Preprocess expects a batch, so we extend the dimension to (None,T,F), then reduce the output back to (T,F).\n","    x = tf.cast(Preprocess(max_len=max_len)(x[None, ...])[0], tf.float32)\n","\n","    if do_pad:  # we can avoid this step if there is batch padding\n","        x = pad_if_short(x, max_len=max_len)\n","        #x=tf.ensure_shape(x,(max_len,Constants.CHANNELS))\n","    else:\n","        #x=tf.ensure_shape(x,(None,Constants.CHANNELS))\n","        pass\n","    return x"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.144369Z","iopub.status.busy":"2023-07-18T09:04:44.143941Z","iopub.status.idle":"2023-07-18T09:04:44.160043Z","shell.execute_reply":"2023-07-18T09:04:44.159041Z","shell.execute_reply.started":"2023-07-18T09:04:44.144335Z"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1689879460504,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"FdtpVVSbYi17","trusted":true},"outputs":[],"source":["\n","def decode_tfrec(record_bytes):\n","    features = tf.io.parse_single_example(\n","        record_bytes,\n","        {\n","            \"coordinates\": tf.io.VarLenFeature(tf.float32),\n","            \"label\": tf.io.VarLenFeature(tf.int64),\n","        },\n","    )\n","    coords = tf.sparse.to_dense(features[\"coordinates\"])\n","    coords = tf.reshape(coords, (-1, Constants.NUM_INPUT_FEATURES))\n","    label = tf.sparse.to_dense(features[\"label\"])\n","\n","    coords=tf.ensure_shape(coords,(None,Constants.NUM_INPUT_FEATURES))\n","    label=tf.ensure_shape(label,(None,))\n","\n","\n","    return (coords, label)\n","\n","def ensure_shapes(x,y,batch_size,max_len):\n","  x=tf.ensure_shape(x,(batch_size,max_len,Constants.CHANNELS))\n","  y=tf.ensure_shape(y,(batch_size,Constants.MAX_STRING_LEN))\n","  return x,y"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.167121Z","iopub.status.busy":"2023-07-18T09:04:44.166717Z","iopub.status.idle":"2023-07-18T09:04:44.179273Z","shell.execute_reply":"2023-07-18T09:04:44.17748Z","shell.execute_reply.started":"2023-07-18T09:04:44.167092Z"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1689879460505,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"ImAgehPoYi18","trusted":true},"outputs":[],"source":["\n","def get_dataset(\n","    filenames,\n","    input_path,\n","    max_len,\n","    batch_size=64,\n","    drop_remainder=False,\n","    augment=False,\n","    shuffle_buffer=None,\n","    repeat=False,\n","    use_tfrecords=True,\n","):\n","    ignore_order = tf.data.Options()\n","    ignore_order.experimental_deterministic = False\n","\n","\n","    ds = tf.data.TFRecordDataset(\n","        filenames, num_parallel_reads=tf.data.AUTOTUNE, compression_type=\"GZIP\"\n","    )\n","    ds.with_options(ignore_order)\n","    ds = ds.map(decode_tfrec, tf.data.AUTOTUNE)\n","\n","    if augment:\n","        ds = ds.map(lambda x, y: (augment_fn(x), y), tf.data.AUTOTUNE)\n","\n","    ds = ds.map(lambda x, y: (preprocess(x, max_len=max_len, do_pad=False), y), tf.data.AUTOTUNE)\n","    #if repeat:\n","    #    ds = ds.repeat()\n","\n","    if shuffle_buffer is not None:\n","        ds = ds.shuffle(shuffle_buffer)\n","\n","    ds = ds.padded_batch(\n","        batch_size,\n","        padding_values=(\n","            tf.constant(Constants.INPUT_PAD, dtype=tf.float32),\n","            tf.constant(Constants.LABEL_PAD, dtype=tf.int64),\n","        ),\n","        padded_shapes=([max_len, Constants.CHANNELS], [Constants.MAX_STRING_LEN]),\n","        drop_remainder=drop_remainder,\n","    )\n","\n","    #tf.data.experimental.assert_cardinality(len(labels) // BATCH_SIZE)\n","    ds.map(lambda x,y: ensure_shapes(x,y,batch_size,max_len),tf.data.AUTOTUNE)\n","    ds = ds.prefetch(tf.data.AUTOTUNE)\n","\n","    return ds\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.182143Z","iopub.status.busy":"2023-07-18T09:04:44.181727Z","iopub.status.idle":"2023-07-18T09:04:44.208936Z","shell.execute_reply":"2023-07-18T09:04:44.207184Z","shell.execute_reply.started":"2023-07-18T09:04:44.182112Z"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1689879460505,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"4HDYkwwgYi18","trusted":true},"outputs":[],"source":["\n","def train_run(train_files, valid_files, config, num_train, experiment_id=0, use_tfrecords=True,summary=False):\n","    gc.collect()\n","    tf.keras.backend.clear_session()\n","    # tf.config.optimizer.set_jit(\"autoclustering\")\n","\n","    if config.fp16:\n","        if config.is_tpu:\n","            policy = \"mixed_bfloat16\"\n","        else:\n","            policy = \"mixed_float16\"\n","    else:\n","        policy = \"float32\"\n","    tf.keras.mixed_precision.set_global_policy(policy)\n","\n","    augment_train= True\n","    repeat_train = True\n","\n","    shuffle_buffer = 4096\n","    train_ds = get_dataset(\n","        train_files,\n","        input_path=config.input_path,\n","        max_len=config.max_len,\n","        batch_size=config.batch_size,\n","        drop_remainder=True,\n","        augment=augment_train,\n","        repeat=repeat_train,\n","        shuffle_buffer=shuffle_buffer,\n","        use_tfrecords=True,\n","    )\n","    if valid_files is not None:\n","        valid_ds = get_dataset(\n","            valid_files,\n","            input_path=config.input_path,\n","            max_len=config.max_len,\n","            batch_size=config.batch_size,\n","            use_tfrecords=True,\n","            drop_remainder=True\n","        )\n","    else:\n","        valid_ds = None\n","        valid_files = []\n","\n","    # num_train = count_data_items(train_ds)\n","    # num_valid = count_data_items(valid_ds)\n","    # print(num_train, num_valid, config.batch_size)\n","    # exit()\n","\n","    steps_per_epoch = num_train // config.batch_size\n","    dropout_step = config.dropout_start_epoch * steps_per_epoch\n","    strategy = config.strategy\n","    with strategy.scope():\n","        model = get_model(\n","            max_len=config.max_len,\n","            output_dim=config.output_dim,\n","            input_pad=Constants.INPUT_PAD,\n","            dim=config.dim,\n","            dropout_step=dropout_step,\n","            drop_rate=0.2\n","        )\n","\n","        base_lr = config.lr\n","        lr_schedule = CosineDecay(\n","            initial_learning_rate=base_lr / 10,\n","            decay_steps=int(0.95 * steps_per_epoch * config.epochs),\n","            alpha=0.005,\n","            name=None,\n","            warmup_target=base_lr,\n","            warmup_steps=int(0.05 * steps_per_epoch * config.epochs),\n","        )\n","\n","        opt = tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=config.weight_decay)\n","        awp_step = config.awp_start_epoch * steps_per_epoch\n","        if config.awp:\n","            model = AWP(model.input, model.output, delta=config.awp_lambda, eps=0., start_step=awp_step)\n","            print(\"Using AWP\")\n","\n","        ctc_loss1 = CTCLoss(pad_token_idx=Constants.LABEL_PAD,batch_size=config.batch_size,\n","                           max_string_len=Constants.MAX_STRING_LEN,\n","                           output_dim=config.output_dim,\n","                           output_steps=config.max_len//2)\n","        ctc_loss2 = CTCLoss(pad_token_idx=Constants.LABEL_PAD,batch_size=config.batch_size,\n","                           max_string_len=Constants.MAX_STRING_LEN,\n","                           output_dim=config.output_dim,\n","                           output_steps=config.max_len//2)\n","\n","\n","        model.compile(\n","            optimizer=opt,\n","            loss=[ctc_loss1,ctc_loss2],\n","            loss_weights=[0.5,0.5],\n","            metrics=[\n","                LevDistanceMetric(),\n","            ],\n","            #jit_compile= not config.is_tpu # Should be False on TPU!!\n","        )\n","\n","    if summary:\n","        print()\n","        model.summary()\n","        print()\n","        print(train_ds, valid_ds)\n","        print()\n","    print(f\"---------experiment {experiment_id}---------\")\n","    print(f\"train:{num_train} \")\n","    print()\n","\n","    if config.resume:\n","        print(f\"resume from epoch{config.resume}\")\n","        model.load_weights(f\"{config.log_path}/{config.comment}-exp{experiment_id}-last.h5\")\n","        if train_ds is not None:\n","            model.evaluate(train_ds.take(steps_per_epoch))\n","        if valid_ds is not None:\n","            model.evaluate(valid_ds)\n","\n","    tb_logger = tf.keras.callbacks.TensorBoard(\n","        log_dir=\"config.log_path\", histogram_freq=0, write_graph=True, write_images=True\n","    )\n","    sv_loss = tf.keras.callbacks.ModelCheckpoint(\n","        f\"{config.log_path}/{config.comment}-exp{experiment_id}-best.h5\",\n","        monitor=\"val_loss\",\n","        verbose=1,\n","        save_best_only=True,\n","        save_weights_only=True,\n","        mode=\"min\",\n","        save_freq=\"epoch\",\n","    )\n","\n","    # Callback function to check transcription on the val set.\n","    # validation_callback = CallbackEval(model, valid_ds)\n","    memory_usage = MemoryUsageCallbackExtended()\n","    swa = SWA(\n","        f\"{config.log_path}/{config.comment}-exp{experiment_id}\",\n","        config.swa_epochs,\n","        strategy=strategy,\n","        train_ds=train_ds,\n","        valid_ds=valid_ds,\n","    )\n","    callbacks = []\n","    if config.save_output:\n","        callbacks.append(tb_logger)\n","        callbacks.append(swa)\n","        callbacks.append(sv_loss)\n","    callbacks.append(memory_usage)\n","    callbacks.append(tf.keras.callbacks.TerminateOnNaN())\n","    # callbacks.append(validation_callback)\n","\n","    history = model.fit(\n","        train_ds,\n","        epochs=config.epochs - config.resume,\n","        #steps_per_epoch=steps_per_epoch,\n","        callbacks=callbacks,\n","        validation_data=valid_ds,\n","        verbose=config.verbose,\n","        # validation_steps=None,\n","    )\n","\n","    if config.save_output:  # reload the saved best weights checkpoint\n","        saved_based_model = f\"{config.log_path}/{config.comment}-exp{experiment_id}-best.h5\"\n","        if os.path.exists(saved_based_model):\n","            model.load_weights(saved_based_model)\n","        else:\n","            print(f\"Warning: could not find {saved_based_model}\")\n","    if valid_ds is not None:\n","        cv = model.evaluate(valid_ds, verbose=config.verbose)\n","    else:\n","        cv = None\n","    return model, cv, history\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.210911Z","iopub.status.busy":"2023-07-18T09:04:44.21051Z","iopub.status.idle":"2023-07-18T09:04:44.227683Z","shell.execute_reply":"2023-07-18T09:04:44.226169Z","shell.execute_reply.started":"2023-07-18T09:04:44.21088Z"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1689879460505,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"YpxT9BTFYi18","trusted":true},"outputs":[],"source":["\n","def train(config, experiment_id=0, use_supplemental=True):\n","    tf.keras.backend.clear_session()\n","    if config.strategy is None:\n","      update_config_with_strategy(config)\n","    print(f\"using {config.replicas} replicas\")\n","    print(f\"batch size {config.batch_size}\")\n","    print(f\"fp16={config.fp16}\")\n","    seed_everything(config.seed)\n","\n","    #all_filenames = sorted(glob.glob(\"/kaggle/input/asl-preprocessing/records/*.tfrecord\"))\n","\n","    all_filenames = sorted(tf.io.gfile.glob(\"/kaggle/input/asl-preprocessing/records/*.tfrecord\"))\n","    regular = [x for x in all_filenames if \"supp\" not in x]\n","    supp = [x for x in all_filenames if \"supp\" in x]\n","\n","    data_filenames = regular\n","    if use_supplemental:\n","        data_filenames += supp\n","    print(\"Using TFRECORDS\")\n","\n","\n","    valid_files = data_filenames[: config.num_eval]  # first part in list\n","    train_files = data_filenames[config.num_eval :]\n","    random.shuffle(train_files)\n","\n","\n","    df1 = pd.read_csv(config.input_path + \"train.csv\")\n","    df2 = pd.read_csv(config.input_path + \"supplemental_metadata.csv\")\n","    df_info = pd.concat([df1, df2])\n","\n","    #ds = get_dataset(train_files, CFG.input_path,max_len=CFG.max_len, augment=False, batch_size=64)\n","    #print(ds)\n","    #for x,y in ds:\n","    #    print(x,y)\n","    #raise\n","\n","    if use_supplemental:\n","        num_train = 3567 * 32  # with supplemental\n","    else:\n","        num_train = 1912 * 32  # without supplemental\n","\n","    train_run(\n","        train_files,\n","        valid_files,\n","        config,\n","        num_train,\n","        summary=False,\n","        experiment_id=experiment_id,\n","        use_tfrecords=True,\n","    )\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.231071Z","iopub.status.busy":"2023-07-18T09:04:44.229724Z","iopub.status.idle":"2023-07-18T09:04:44.497144Z","shell.execute_reply":"2023-07-18T09:04:44.495773Z","shell.execute_reply.started":"2023-07-18T09:04:44.231015Z"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1689879460842,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"lfC8z_wLYi19","trusted":true},"outputs":[],"source":["gc.collect()\n","tf.keras.backend.clear_session()"]},{"cell_type":"markdown","metadata":{"id":"BVIhC2jeYi19"},"source":["# Train It!"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":680},"execution":{"iopub.execute_input":"2023-07-18T09:04:44.499483Z","iopub.status.busy":"2023-07-18T09:04:44.499005Z","iopub.status.idle":"2023-07-18T09:04:44.505927Z","shell.execute_reply":"2023-07-18T09:04:44.504277Z","shell.execute_reply.started":"2023-07-18T09:04:44.49944Z"},"executionInfo":{"elapsed":439647,"status":"error","timestamp":1689879900487,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"5UdMh9V7Yi19","outputId":"c9123f2b-12ea-4fcb-e05d-01cbfa11aadd","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  1\n","get strategy replicas: 1\n","using 1 replicas\n","batch size 128\n","fp16=True\n","Using TFRECORDS\n","INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n","Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3060 Ti, compute capability 8.6\n"]},{"name":"stderr","output_type":"stream","text":["2023-07-21 06:50:50.274663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-21 06:50:50.274829: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-21 06:50:50.274920: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-21 06:50:51.188836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-21 06:50:51.188983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-21 06:50:51.189080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-21 06:50:51.189458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1831] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5949 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n","2023-07-21 06:50:51.360831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-07-21 06:50:52.611458: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"]},{"name":"stdout","output_type":"stream","text":["---------experiment 0---------\n","train:114144 \n","\n","Epoch 1/200\n","WARNING:tensorflow:From /home/sronen/code/.venv/lib/python3.10/site-packages/tensorflow/python/ops/ctc_ops.py:1514: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\n","WARNING:tensorflow:From /home/sronen/code/.venv/lib/python3.10/site-packages/tensorflow/python/ops/ctc_ops.py:1497: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\n"]},{"name":"stderr","output_type":"stream","text":["2023-07-21 06:51:15.886305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:440] Loaded cuDNN version 8901\n","2023-07-21 06:51:16.250660: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"]},{"name":"stdout","output_type":"stream","text":["      4/Unknown - 24s 487ms/step - loss: 411.4375 - final_logsoft_loss: 406.7500 - tf.nn.log_softmax_loss: 416.1250 - final_logsoft_Lev: 0.0000e+00 - tf.nn.log_softmax_Lev: 0.0000e+00"]},{"name":"stderr","output_type":"stream","text":["2023-07-21 06:51:20.728498: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe39017c8b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2023-07-21 06:51:20.728515: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Ti, Compute Capability 8.6\n","2023-07-21 06:51:20.736348: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","2023-07-21 06:51:20.824792: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"]},{"name":"stdout","output_type":"stream","text":["    891/Unknown - 477s 511ms/step - loss: 87.0386 - final_logsoft_loss: 85.5749 - tf.nn.log_softmax_loss: 88.5025 - final_logsoft_Lev: 0.0000e+00 - tf.nn.log_softmax_Lev: 2.9538e-05"]},{"name":"stderr","output_type":"stream","text":["2023-07-21 06:58:53.138870: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14407816593210733544\n","2023-07-21 06:58:53.138917: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14806397973763176505\n","2023-07-21 06:58:53.138926: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12886818814031774251\n","2023-07-21 06:58:53.138935: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3203736070686874429\n","2023-07-21 06:58:53.138942: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4372580701003288331\n","2023-07-21 06:59:09.416081: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5009386464961438454\n","2023-07-21 06:59:09.416119: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17380943087610856358\n","2023-07-21 06:59:09.416128: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5521428811400530445\n","2023-07-21 06:59:09.416136: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14053907474552685417\n","2023-07-21 06:59:09.416143: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10799721537814050273\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 1: val_loss improved from inf to 66.98981, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.51 GB\n","891/891 [==============================] - 494s 529ms/step - loss: 87.0386 - final_logsoft_loss: 85.5749 - tf.nn.log_softmax_loss: 88.5025 - final_logsoft_Lev: 0.0000e+00 - tf.nn.log_softmax_Lev: 2.9538e-05 - val_loss: 66.9898 - val_final_logsoft_loss: 66.9491 - val_tf.nn.log_softmax_loss: 67.0326 - val_final_logsoft_Lev: 0.0000e+00 - val_tf.nn.log_softmax_Lev: 0.0000e+00\n","Epoch 2/200\n","891/891 [==============================] - ETA: 0s - loss: 78.0619 - final_logsoft_loss: 77.9824 - tf.nn.log_softmax_loss: 78.1440 - final_logsoft_Lev: 0.0012 - tf.nn.log_softmax_Lev: 0.0014\n","Epoch 2: val_loss improved from 66.98981 to 66.39130, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.21 GB\n","891/891 [==============================] - 471s 526ms/step - loss: 78.0619 - final_logsoft_loss: 77.9824 - tf.nn.log_softmax_loss: 78.1440 - final_logsoft_Lev: 0.0012 - tf.nn.log_softmax_Lev: 0.0014 - val_loss: 66.3913 - val_final_logsoft_loss: 66.2609 - val_tf.nn.log_softmax_loss: 66.5245 - val_final_logsoft_Lev: 0.0000e+00 - val_tf.nn.log_softmax_Lev: 0.0000e+00\n","Epoch 3/200\n","891/891 [==============================] - ETA: 0s - loss: 77.3165 - final_logsoft_loss: 77.1348 - tf.nn.log_softmax_loss: 77.4985 - final_logsoft_Lev: 0.0234 - tf.nn.log_softmax_Lev: 0.0044\n","Epoch 3: val_loss improved from 66.39130 to 63.41644, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 4.78 GB\n","891/891 [==============================] - 501s 561ms/step - loss: 77.3165 - final_logsoft_loss: 77.1348 - tf.nn.log_softmax_loss: 77.4985 - final_logsoft_Lev: 0.0234 - tf.nn.log_softmax_Lev: 0.0044 - val_loss: 63.4164 - val_final_logsoft_loss: 63.1433 - val_tf.nn.log_softmax_loss: 63.6963 - val_final_logsoft_Lev: 0.0248 - val_tf.nn.log_softmax_Lev: 7.5573e-04\n","Epoch 4/200\n","891/891 [==============================] - ETA: 0s - loss: 73.6123 - final_logsoft_loss: 72.8494 - tf.nn.log_softmax_loss: 74.3754 - final_logsoft_Lev: 0.0767 - tf.nn.log_softmax_Lev: 0.0127\n","Epoch 4: val_loss improved from 63.41644 to 59.33899, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.03 GB\n","891/891 [==============================] - 513s 573ms/step - loss: 73.6123 - final_logsoft_loss: 72.8494 - tf.nn.log_softmax_loss: 74.3754 - final_logsoft_Lev: 0.0767 - tf.nn.log_softmax_Lev: 0.0127 - val_loss: 59.3390 - val_final_logsoft_loss: 58.8288 - val_tf.nn.log_softmax_loss: 59.8458 - val_final_logsoft_Lev: 0.0889 - val_tf.nn.log_softmax_Lev: 0.0122\n","Epoch 5/200\n","891/891 [==============================] - ETA: 0s - loss: 68.7112 - final_logsoft_loss: 68.0536 - tf.nn.log_softmax_loss: 69.3691 - final_logsoft_Lev: 0.1300 - tf.nn.log_softmax_Lev: 0.0395\n","Epoch 5: val_loss improved from 59.33899 to 52.33152, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 4.19 GB\n","891/891 [==============================] - 510s 570ms/step - loss: 68.7112 - final_logsoft_loss: 68.0536 - tf.nn.log_softmax_loss: 69.3691 - final_logsoft_Lev: 0.1300 - tf.nn.log_softmax_Lev: 0.0395 - val_loss: 52.3315 - val_final_logsoft_loss: 52.6372 - val_tf.nn.log_softmax_loss: 52.0238 - val_final_logsoft_Lev: 0.1369 - val_tf.nn.log_softmax_Lev: 0.0630\n","Epoch 6/200\n","891/891 [==============================] - ETA: 0s - loss: 52.7456 - final_logsoft_loss: 52.4396 - tf.nn.log_softmax_loss: 53.0529 - final_logsoft_Lev: 0.3088 - tf.nn.log_softmax_Lev: 0.2692\n","Epoch 6: val_loss improved from 52.33152 to 33.04925, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.72 GB\n","891/891 [==============================] - 508s 568ms/step - loss: 52.7456 - final_logsoft_loss: 52.4396 - tf.nn.log_softmax_loss: 53.0529 - final_logsoft_Lev: 0.3088 - tf.nn.log_softmax_Lev: 0.2692 - val_loss: 33.0493 - val_final_logsoft_loss: 32.2959 - val_tf.nn.log_softmax_loss: 33.8033 - val_final_logsoft_Lev: 0.5270 - val_tf.nn.log_softmax_Lev: 0.4900\n","Epoch 7/200\n","891/891 [==============================] - ETA: 0s - loss: 41.0901 - final_logsoft_loss: 39.8745 - tf.nn.log_softmax_loss: 42.3055 - final_logsoft_Lev: 0.5000 - tf.nn.log_softmax_Lev: 0.4604\n","Epoch 7: val_loss improved from 33.04925 to 28.56012, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 4.29 GB\n","891/891 [==============================] - 510s 570ms/step - loss: 41.0901 - final_logsoft_loss: 39.8745 - tf.nn.log_softmax_loss: 42.3055 - final_logsoft_Lev: 0.5000 - tf.nn.log_softmax_Lev: 0.4604 - val_loss: 28.5601 - val_final_logsoft_loss: 27.6671 - val_tf.nn.log_softmax_loss: 29.4518 - val_final_logsoft_Lev: 0.5935 - val_tf.nn.log_softmax_Lev: 0.5597\n","Epoch 8/200\n","891/891 [==============================] - ETA: 0s - loss: 37.8826 - final_logsoft_loss: 36.5562 - tf.nn.log_softmax_loss: 39.2085 - final_logsoft_Lev: 0.5438 - tf.nn.log_softmax_Lev: 0.5023\n","Epoch 8: val_loss improved from 28.56012 to 27.03465, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.66 GB\n","891/891 [==============================] - 501s 560ms/step - loss: 37.8826 - final_logsoft_loss: 36.5562 - tf.nn.log_softmax_loss: 39.2085 - final_logsoft_Lev: 0.5438 - tf.nn.log_softmax_Lev: 0.5023 - val_loss: 27.0346 - val_final_logsoft_loss: 26.0873 - val_tf.nn.log_softmax_loss: 27.9813 - val_final_logsoft_Lev: 0.6163 - val_tf.nn.log_softmax_Lev: 0.5819\n","Epoch 9/200\n","891/891 [==============================] - ETA: 0s - loss: 36.1227 - final_logsoft_loss: 34.7139 - tf.nn.log_softmax_loss: 37.5314 - final_logsoft_Lev: 0.5685 - tf.nn.log_softmax_Lev: 0.5245\n","Epoch 9: val_loss improved from 27.03465 to 25.81182, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 4.15 GB\n","891/891 [==============================] - 510s 570ms/step - loss: 36.1227 - final_logsoft_loss: 34.7139 - tf.nn.log_softmax_loss: 37.5314 - final_logsoft_Lev: 0.5685 - tf.nn.log_softmax_Lev: 0.5245 - val_loss: 25.8118 - val_final_logsoft_loss: 24.9599 - val_tf.nn.log_softmax_loss: 26.6620 - val_final_logsoft_Lev: 0.6335 - val_tf.nn.log_softmax_Lev: 0.5976\n","Epoch 10/200\n","891/891 [==============================] - ETA: 0s - loss: 34.9015 - final_logsoft_loss: 33.4417 - tf.nn.log_softmax_loss: 36.3610 - final_logsoft_Lev: 0.5850 - tf.nn.log_softmax_Lev: 0.5396\n","Epoch 10: val_loss improved from 25.81182 to 25.53091, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.79 GB\n","891/891 [==============================] - 512s 572ms/step - loss: 34.9015 - final_logsoft_loss: 33.4417 - tf.nn.log_softmax_loss: 36.3610 - final_logsoft_Lev: 0.5850 - tf.nn.log_softmax_Lev: 0.5396 - val_loss: 25.5309 - val_final_logsoft_loss: 24.5034 - val_tf.nn.log_softmax_loss: 26.5567 - val_final_logsoft_Lev: 0.6390 - val_tf.nn.log_softmax_Lev: 0.6063\n","Epoch 11/200\n","891/891 [==============================] - ETA: 0s - loss: 33.8230 - final_logsoft_loss: 32.3187 - tf.nn.log_softmax_loss: 35.3261 - final_logsoft_Lev: 0.5993 - tf.nn.log_softmax_Lev: 0.5531\n","Epoch 11: val_loss improved from 25.53091 to 24.00985, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 4.49 GB\n","891/891 [==============================] - 511s 571ms/step - loss: 33.8230 - final_logsoft_loss: 32.3187 - tf.nn.log_softmax_loss: 35.3261 - final_logsoft_Lev: 0.5993 - tf.nn.log_softmax_Lev: 0.5531 - val_loss: 24.0099 - val_final_logsoft_loss: 23.0890 - val_tf.nn.log_softmax_loss: 24.9307 - val_final_logsoft_Lev: 0.6491 - val_tf.nn.log_softmax_Lev: 0.6116\n","Epoch 12/200\n","891/891 [==============================] - ETA: 0s - loss: 32.9972 - final_logsoft_loss: 31.4416 - tf.nn.log_softmax_loss: 34.5530 - final_logsoft_Lev: 0.6107 - tf.nn.log_softmax_Lev: 0.5630\n","Epoch 12: val_loss improved from 24.00985 to 23.76834, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.71 GB\n","891/891 [==============================] - 489s 547ms/step - loss: 32.9972 - final_logsoft_loss: 31.4416 - tf.nn.log_softmax_loss: 34.5530 - final_logsoft_Lev: 0.6107 - tf.nn.log_softmax_Lev: 0.5630 - val_loss: 23.7683 - val_final_logsoft_loss: 22.7279 - val_tf.nn.log_softmax_loss: 24.8081 - val_final_logsoft_Lev: 0.6604 - val_tf.nn.log_softmax_Lev: 0.6254\n","Epoch 13/200\n","891/891 [==============================] - ETA: 0s - loss: 32.0940 - final_logsoft_loss: 30.5046 - tf.nn.log_softmax_loss: 33.6834 - final_logsoft_Lev: 0.6221 - tf.nn.log_softmax_Lev: 0.5739\n","Epoch 13: val_loss improved from 23.76834 to 22.96773, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 4.77 GB\n","891/891 [==============================] - 485s 542ms/step - loss: 32.0940 - final_logsoft_loss: 30.5046 - tf.nn.log_softmax_loss: 33.6834 - final_logsoft_Lev: 0.6221 - tf.nn.log_softmax_Lev: 0.5739 - val_loss: 22.9677 - val_final_logsoft_loss: 21.9348 - val_tf.nn.log_softmax_loss: 24.0014 - val_final_logsoft_Lev: 0.6687 - val_tf.nn.log_softmax_Lev: 0.6309\n","Epoch 14/200\n","891/891 [==============================] - ETA: 0s - loss: 31.4468 - final_logsoft_loss: 29.8176 - tf.nn.log_softmax_loss: 33.0760 - final_logsoft_Lev: 0.6306 - tf.nn.log_softmax_Lev: 0.5823\n","Epoch 14: val_loss improved from 22.96773 to 22.64198, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.57 GB\n","891/891 [==============================] - 493s 551ms/step - loss: 31.4468 - final_logsoft_loss: 29.8176 - tf.nn.log_softmax_loss: 33.0760 - final_logsoft_Lev: 0.6306 - tf.nn.log_softmax_Lev: 0.5823 - val_loss: 22.6420 - val_final_logsoft_loss: 21.6101 - val_tf.nn.log_softmax_loss: 23.6736 - val_final_logsoft_Lev: 0.6791 - val_tf.nn.log_softmax_Lev: 0.6473\n","Epoch 15/200\n","891/891 [==============================] - ETA: 0s - loss: 30.8138 - final_logsoft_loss: 29.1560 - tf.nn.log_softmax_loss: 32.4704 - final_logsoft_Lev: 0.6394 - tf.nn.log_softmax_Lev: 0.5902\n","Epoch 15: val_loss improved from 22.64198 to 21.90999, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 4.69 GB\n","891/891 [==============================] - 489s 547ms/step - loss: 30.8138 - final_logsoft_loss: 29.1560 - tf.nn.log_softmax_loss: 32.4704 - final_logsoft_Lev: 0.6394 - tf.nn.log_softmax_Lev: 0.5902 - val_loss: 21.9100 - val_final_logsoft_loss: 20.9392 - val_tf.nn.log_softmax_loss: 22.8791 - val_final_logsoft_Lev: 0.6829 - val_tf.nn.log_softmax_Lev: 0.6471\n","Epoch 16/200\n","891/891 [==============================] - ETA: 0s - loss: 31.5270 - final_logsoft_loss: 31.0258 - tf.nn.log_softmax_loss: 32.0287 - final_logsoft_Lev: 0.6294 - tf.nn.log_softmax_Lev: 0.5957\n","Epoch 16: val_loss improved from 21.90999 to 21.90353, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.61 GB\n","891/891 [==============================] - 496s 554ms/step - loss: 31.5270 - final_logsoft_loss: 31.0258 - tf.nn.log_softmax_loss: 32.0287 - final_logsoft_Lev: 0.6294 - tf.nn.log_softmax_Lev: 0.5957 - val_loss: 21.9035 - val_final_logsoft_loss: 21.1624 - val_tf.nn.log_softmax_loss: 22.6444 - val_final_logsoft_Lev: 0.6829 - val_tf.nn.log_softmax_Lev: 0.6518\n","Epoch 17/200\n","891/891 [==============================] - ETA: 0s - loss: 30.7726 - final_logsoft_loss: 30.0168 - tf.nn.log_softmax_loss: 31.5288 - final_logsoft_Lev: 0.6359 - tf.nn.log_softmax_Lev: 0.6028\n","Epoch 17: val_loss improved from 21.90353 to 21.88485, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 4.98 GB\n","891/891 [==============================] - 486s 543ms/step - loss: 30.7726 - final_logsoft_loss: 30.0168 - tf.nn.log_softmax_loss: 31.5288 - final_logsoft_Lev: 0.6359 - tf.nn.log_softmax_Lev: 0.6028 - val_loss: 21.8849 - val_final_logsoft_loss: 21.1080 - val_tf.nn.log_softmax_loss: 22.6600 - val_final_logsoft_Lev: 0.6799 - val_tf.nn.log_softmax_Lev: 0.6529\n","Epoch 18/200\n","891/891 [==============================] - ETA: 0s - loss: 30.2331 - final_logsoft_loss: 29.3687 - tf.nn.log_softmax_loss: 31.0981 - final_logsoft_Lev: 0.6423 - tf.nn.log_softmax_Lev: 0.6087\n","Epoch 18: val_loss improved from 21.88485 to 21.13689, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.67 GB\n","891/891 [==============================] - 489s 547ms/step - loss: 30.2331 - final_logsoft_loss: 29.3687 - tf.nn.log_softmax_loss: 31.0981 - final_logsoft_Lev: 0.6423 - tf.nn.log_softmax_Lev: 0.6087 - val_loss: 21.1369 - val_final_logsoft_loss: 20.3939 - val_tf.nn.log_softmax_loss: 21.8794 - val_final_logsoft_Lev: 0.6924 - val_tf.nn.log_softmax_Lev: 0.6675\n","Epoch 19/200\n","891/891 [==============================] - ETA: 0s - loss: 29.7428 - final_logsoft_loss: 28.8171 - tf.nn.log_softmax_loss: 30.6687 - final_logsoft_Lev: 0.6475 - tf.nn.log_softmax_Lev: 0.6144\n","Epoch 19: val_loss improved from 21.13689 to 20.79144, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 4.96 GB\n","891/891 [==============================] - 492s 550ms/step - loss: 29.7428 - final_logsoft_loss: 28.8171 - tf.nn.log_softmax_loss: 30.6687 - final_logsoft_Lev: 0.6475 - tf.nn.log_softmax_Lev: 0.6144 - val_loss: 20.7914 - val_final_logsoft_loss: 20.1090 - val_tf.nn.log_softmax_loss: 21.4718 - val_final_logsoft_Lev: 0.6950 - val_tf.nn.log_softmax_Lev: 0.6706\n","Epoch 20/200\n","891/891 [==============================] - ETA: 0s - loss: 29.3313 - final_logsoft_loss: 28.3519 - tf.nn.log_softmax_loss: 30.3110 - final_logsoft_Lev: 0.6525 - tf.nn.log_softmax_Lev: 0.6196\n","Epoch 20: val_loss did not improve from 20.79144\n","Memory usage on epoch end: 5.42 GB\n","891/891 [==============================] - 479s 535ms/step - loss: 29.3313 - final_logsoft_loss: 28.3519 - tf.nn.log_softmax_loss: 30.3110 - final_logsoft_Lev: 0.6525 - tf.nn.log_softmax_Lev: 0.6196 - val_loss: 20.9986 - val_final_logsoft_loss: 20.1919 - val_tf.nn.log_softmax_loss: 21.8050 - val_final_logsoft_Lev: 0.6975 - val_tf.nn.log_softmax_Lev: 0.6734\n","Epoch 21/200\n","891/891 [==============================] - ETA: 0s - loss: 28.9421 - final_logsoft_loss: 27.9165 - tf.nn.log_softmax_loss: 29.9677 - final_logsoft_Lev: 0.6577 - tf.nn.log_softmax_Lev: 0.6244\n","Epoch 21: val_loss improved from 20.79144 to 20.57269, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 4.93 GB\n","891/891 [==============================] - 488s 545ms/step - loss: 28.9421 - final_logsoft_loss: 27.9165 - tf.nn.log_softmax_loss: 29.9677 - final_logsoft_Lev: 0.6577 - tf.nn.log_softmax_Lev: 0.6244 - val_loss: 20.5727 - val_final_logsoft_loss: 19.6928 - val_tf.nn.log_softmax_loss: 21.4545 - val_final_logsoft_Lev: 0.7047 - val_tf.nn.log_softmax_Lev: 0.6824\n","Epoch 22/200\n","891/891 [==============================] - ETA: 0s - loss: 28.5446 - final_logsoft_loss: 27.4815 - tf.nn.log_softmax_loss: 29.6079 - final_logsoft_Lev: 0.6627 - tf.nn.log_softmax_Lev: 0.6296\n","Epoch 22: val_loss improved from 20.57269 to 20.27955, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.43 GB\n","891/891 [==============================] - 478s 534ms/step - loss: 28.5446 - final_logsoft_loss: 27.4815 - tf.nn.log_softmax_loss: 29.6079 - final_logsoft_Lev: 0.6627 - tf.nn.log_softmax_Lev: 0.6296 - val_loss: 20.2796 - val_final_logsoft_loss: 19.5769 - val_tf.nn.log_softmax_loss: 20.9803 - val_final_logsoft_Lev: 0.7060 - val_tf.nn.log_softmax_Lev: 0.6816\n","Epoch 23/200\n","891/891 [==============================] - ETA: 0s - loss: 28.2404 - final_logsoft_loss: 27.1395 - tf.nn.log_softmax_loss: 29.3415 - final_logsoft_Lev: 0.6668 - tf.nn.log_softmax_Lev: 0.6336\n","Epoch 23: val_loss improved from 20.27955 to 20.06454, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 4.69 GB\n","891/891 [==============================] - 474s 530ms/step - loss: 28.2404 - final_logsoft_loss: 27.1395 - tf.nn.log_softmax_loss: 29.3415 - final_logsoft_Lev: 0.6668 - tf.nn.log_softmax_Lev: 0.6336 - val_loss: 20.0645 - val_final_logsoft_loss: 19.3127 - val_tf.nn.log_softmax_loss: 20.8149 - val_final_logsoft_Lev: 0.7100 - val_tf.nn.log_softmax_Lev: 0.6828\n","Epoch 24/200\n","891/891 [==============================] - ETA: 0s - loss: 27.9343 - final_logsoft_loss: 26.7948 - tf.nn.log_softmax_loss: 29.0738 - final_logsoft_Lev: 0.6701 - tf.nn.log_softmax_Lev: 0.6373\n","Epoch 24: val_loss did not improve from 20.06454\n","Memory usage on epoch end: 5.81 GB\n","891/891 [==============================] - 474s 530ms/step - loss: 27.9343 - final_logsoft_loss: 26.7948 - tf.nn.log_softmax_loss: 29.0738 - final_logsoft_Lev: 0.6701 - tf.nn.log_softmax_Lev: 0.6373 - val_loss: 20.2918 - val_final_logsoft_loss: 19.3480 - val_tf.nn.log_softmax_loss: 21.2364 - val_final_logsoft_Lev: 0.7103 - val_tf.nn.log_softmax_Lev: 0.6883\n","Epoch 25/200\n","891/891 [==============================] - ETA: 0s - loss: 27.6092 - final_logsoft_loss: 26.4524 - tf.nn.log_softmax_loss: 28.7660 - final_logsoft_Lev: 0.6743 - tf.nn.log_softmax_Lev: 0.6413\n","Epoch 25: val_loss improved from 20.06454 to 19.71654, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.07 GB\n","891/891 [==============================] - 473s 529ms/step - loss: 27.6092 - final_logsoft_loss: 26.4524 - tf.nn.log_softmax_loss: 28.7660 - final_logsoft_Lev: 0.6743 - tf.nn.log_softmax_Lev: 0.6413 - val_loss: 19.7165 - val_final_logsoft_loss: 18.9312 - val_tf.nn.log_softmax_loss: 20.5065 - val_final_logsoft_Lev: 0.7115 - val_tf.nn.log_softmax_Lev: 0.6894\n","Epoch 26/200\n","891/891 [==============================] - ETA: 0s - loss: 27.3304 - final_logsoft_loss: 26.1389 - tf.nn.log_softmax_loss: 28.5223 - final_logsoft_Lev: 0.6781 - tf.nn.log_softmax_Lev: 0.6448\n","Epoch 26: val_loss did not improve from 19.71654\n","Memory usage on epoch end: 5.54 GB\n","891/891 [==============================] - 471s 527ms/step - loss: 27.3304 - final_logsoft_loss: 26.1389 - tf.nn.log_softmax_loss: 28.5223 - final_logsoft_Lev: 0.6781 - tf.nn.log_softmax_Lev: 0.6448 - val_loss: 19.8976 - val_final_logsoft_loss: 19.1216 - val_tf.nn.log_softmax_loss: 20.6732 - val_final_logsoft_Lev: 0.7157 - val_tf.nn.log_softmax_Lev: 0.6934\n","Epoch 27/200\n","891/891 [==============================] - ETA: 0s - loss: 27.0281 - final_logsoft_loss: 25.8091 - tf.nn.log_softmax_loss: 28.2474 - final_logsoft_Lev: 0.6823 - tf.nn.log_softmax_Lev: 0.6485\n","Epoch 27: val_loss improved from 19.71654 to 19.50509, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.00 GB\n","891/891 [==============================] - 495s 553ms/step - loss: 27.0281 - final_logsoft_loss: 25.8091 - tf.nn.log_softmax_loss: 28.2474 - final_logsoft_Lev: 0.6823 - tf.nn.log_softmax_Lev: 0.6485 - val_loss: 19.5051 - val_final_logsoft_loss: 18.6151 - val_tf.nn.log_softmax_loss: 20.3930 - val_final_logsoft_Lev: 0.7163 - val_tf.nn.log_softmax_Lev: 0.6962\n","Epoch 28/200\n","891/891 [==============================] - ETA: 0s - loss: 26.8169 - final_logsoft_loss: 25.5724 - tf.nn.log_softmax_loss: 28.0620 - final_logsoft_Lev: 0.6850 - tf.nn.log_softmax_Lev: 0.6511\n","Epoch 28: val_loss improved from 19.50509 to 19.44718, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.56 GB\n","891/891 [==============================] - 496s 555ms/step - loss: 26.8169 - final_logsoft_loss: 25.5724 - tf.nn.log_softmax_loss: 28.0620 - final_logsoft_Lev: 0.6850 - tf.nn.log_softmax_Lev: 0.6511 - val_loss: 19.4472 - val_final_logsoft_loss: 18.7221 - val_tf.nn.log_softmax_loss: 20.1709 - val_final_logsoft_Lev: 0.7218 - val_tf.nn.log_softmax_Lev: 0.6979\n","Epoch 29/200\n","891/891 [==============================] - ETA: 0s - loss: 26.5694 - final_logsoft_loss: 25.3038 - tf.nn.log_softmax_loss: 27.8348 - final_logsoft_Lev: 0.6883 - tf.nn.log_softmax_Lev: 0.6545\n","Epoch 29: val_loss improved from 19.44718 to 19.19293, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.22 GB\n","891/891 [==============================] - 499s 557ms/step - loss: 26.5694 - final_logsoft_loss: 25.3038 - tf.nn.log_softmax_loss: 27.8348 - final_logsoft_Lev: 0.6883 - tf.nn.log_softmax_Lev: 0.6545 - val_loss: 19.1929 - val_final_logsoft_loss: 18.4166 - val_tf.nn.log_softmax_loss: 19.9704 - val_final_logsoft_Lev: 0.7219 - val_tf.nn.log_softmax_Lev: 0.6976\n","Epoch 30/200\n","891/891 [==============================] - ETA: 0s - loss: 26.2983 - final_logsoft_loss: 24.9910 - tf.nn.log_softmax_loss: 27.6060 - final_logsoft_Lev: 0.6918 - tf.nn.log_softmax_Lev: 0.6576\n","Epoch 30: val_loss improved from 19.19293 to 19.17561, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.42 GB\n","891/891 [==============================] - 499s 557ms/step - loss: 26.2983 - final_logsoft_loss: 24.9910 - tf.nn.log_softmax_loss: 27.6060 - final_logsoft_Lev: 0.6918 - tf.nn.log_softmax_Lev: 0.6576 - val_loss: 19.1756 - val_final_logsoft_loss: 18.3939 - val_tf.nn.log_softmax_loss: 19.9572 - val_final_logsoft_Lev: 0.7243 - val_tf.nn.log_softmax_Lev: 0.7035\n","Epoch 31/200\n","891/891 [==============================] - ETA: 0s - loss: 26.0949 - final_logsoft_loss: 24.7806 - tf.nn.log_softmax_loss: 27.4091 - final_logsoft_Lev: 0.6941 - tf.nn.log_softmax_Lev: 0.6604\n","Epoch 31: val_loss improved from 19.17561 to 18.97792, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.30 GB\n","891/891 [==============================] - 495s 554ms/step - loss: 26.0949 - final_logsoft_loss: 24.7806 - tf.nn.log_softmax_loss: 27.4091 - final_logsoft_Lev: 0.6941 - tf.nn.log_softmax_Lev: 0.6604 - val_loss: 18.9779 - val_final_logsoft_loss: 18.2700 - val_tf.nn.log_softmax_loss: 19.6844 - val_final_logsoft_Lev: 0.7283 - val_tf.nn.log_softmax_Lev: 0.7061\n","Epoch 32/200\n","891/891 [==============================] - ETA: 0s - loss: 25.8904 - final_logsoft_loss: 24.5500 - tf.nn.log_softmax_loss: 27.2303 - final_logsoft_Lev: 0.6966 - tf.nn.log_softmax_Lev: 0.6626\n","Epoch 32: val_loss improved from 18.97792 to 18.85088, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.60 GB\n","891/891 [==============================] - 496s 554ms/step - loss: 25.8904 - final_logsoft_loss: 24.5500 - tf.nn.log_softmax_loss: 27.2303 - final_logsoft_Lev: 0.6966 - tf.nn.log_softmax_Lev: 0.6626 - val_loss: 18.8509 - val_final_logsoft_loss: 18.0598 - val_tf.nn.log_softmax_loss: 19.6439 - val_final_logsoft_Lev: 0.7318 - val_tf.nn.log_softmax_Lev: 0.7103\n","Epoch 33/200\n","891/891 [==============================] - ETA: 0s - loss: 25.6825 - final_logsoft_loss: 24.3273 - tf.nn.log_softmax_loss: 27.0385 - final_logsoft_Lev: 0.6994 - tf.nn.log_softmax_Lev: 0.6652\n","Epoch 33: val_loss did not improve from 18.85088\n","Memory usage on epoch end: 5.15 GB\n","891/891 [==============================] - 478s 534ms/step - loss: 25.6825 - final_logsoft_loss: 24.3273 - tf.nn.log_softmax_loss: 27.0385 - final_logsoft_Lev: 0.6994 - tf.nn.log_softmax_Lev: 0.6652 - val_loss: 18.8618 - val_final_logsoft_loss: 18.0484 - val_tf.nn.log_softmax_loss: 19.6741 - val_final_logsoft_Lev: 0.7303 - val_tf.nn.log_softmax_Lev: 0.7098\n","Epoch 34/200\n","891/891 [==============================] - ETA: 0s - loss: 25.4986 - final_logsoft_loss: 24.1186 - tf.nn.log_softmax_loss: 26.8789 - final_logsoft_Lev: 0.7019 - tf.nn.log_softmax_Lev: 0.6675\n","Epoch 34: val_loss improved from 18.85088 to 18.56080, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.32 GB\n","891/891 [==============================] - 464s 518ms/step - loss: 25.4986 - final_logsoft_loss: 24.1186 - tf.nn.log_softmax_loss: 26.8789 - final_logsoft_Lev: 0.7019 - tf.nn.log_softmax_Lev: 0.6675 - val_loss: 18.5608 - val_final_logsoft_loss: 17.8545 - val_tf.nn.log_softmax_loss: 19.2668 - val_final_logsoft_Lev: 0.7346 - val_tf.nn.log_softmax_Lev: 0.7138\n","Epoch 35/200\n","891/891 [==============================] - ETA: 0s - loss: 25.2406 - final_logsoft_loss: 23.8463 - tf.nn.log_softmax_loss: 26.6342 - final_logsoft_Lev: 0.7052 - tf.nn.log_softmax_Lev: 0.6706\n","Epoch 35: val_loss did not improve from 18.56080\n","Memory usage on epoch end: 5.41 GB\n","891/891 [==============================] - 480s 537ms/step - loss: 25.2406 - final_logsoft_loss: 23.8463 - tf.nn.log_softmax_loss: 26.6342 - final_logsoft_Lev: 0.7052 - tf.nn.log_softmax_Lev: 0.6706 - val_loss: 18.6836 - val_final_logsoft_loss: 18.0187 - val_tf.nn.log_softmax_loss: 19.3485 - val_final_logsoft_Lev: 0.7339 - val_tf.nn.log_softmax_Lev: 0.7140\n","Epoch 36/200\n","891/891 [==============================] - ETA: 0s - loss: 25.1636 - final_logsoft_loss: 23.7431 - tf.nn.log_softmax_loss: 26.5841 - final_logsoft_Lev: 0.7061 - tf.nn.log_softmax_Lev: 0.6707\n","Epoch 36: val_loss did not improve from 18.56080\n","Memory usage on epoch end: 5.74 GB\n","891/891 [==============================] - 489s 547ms/step - loss: 25.1636 - final_logsoft_loss: 23.7431 - tf.nn.log_softmax_loss: 26.5841 - final_logsoft_Lev: 0.7061 - tf.nn.log_softmax_Lev: 0.6707 - val_loss: 18.7064 - val_final_logsoft_loss: 17.9110 - val_tf.nn.log_softmax_loss: 19.5008 - val_final_logsoft_Lev: 0.7333 - val_tf.nn.log_softmax_Lev: 0.7124\n","Epoch 37/200\n","891/891 [==============================] - ETA: 0s - loss: 24.9012 - final_logsoft_loss: 23.4665 - tf.nn.log_softmax_loss: 26.3363 - final_logsoft_Lev: 0.7093 - tf.nn.log_softmax_Lev: 0.6742\n","Epoch 37: val_loss improved from 18.56080 to 18.38757, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.23 GB\n","891/891 [==============================] - 497s 555ms/step - loss: 24.9012 - final_logsoft_loss: 23.4665 - tf.nn.log_softmax_loss: 26.3363 - final_logsoft_Lev: 0.7093 - tf.nn.log_softmax_Lev: 0.6742 - val_loss: 18.3876 - val_final_logsoft_loss: 17.7298 - val_tf.nn.log_softmax_loss: 19.0465 - val_final_logsoft_Lev: 0.7329 - val_tf.nn.log_softmax_Lev: 0.7100\n","Epoch 38/200\n","891/891 [==============================] - ETA: 0s - loss: 24.8120 - final_logsoft_loss: 23.3638 - tf.nn.log_softmax_loss: 26.2604 - final_logsoft_Lev: 0.7109 - tf.nn.log_softmax_Lev: 0.6753\n","Epoch 38: val_loss improved from 18.38757 to 18.26444, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.46 GB\n","891/891 [==============================] - 495s 554ms/step - loss: 24.8120 - final_logsoft_loss: 23.3638 - tf.nn.log_softmax_loss: 26.2604 - final_logsoft_Lev: 0.7109 - tf.nn.log_softmax_Lev: 0.6753 - val_loss: 18.2644 - val_final_logsoft_loss: 17.5620 - val_tf.nn.log_softmax_loss: 18.9694 - val_final_logsoft_Lev: 0.7380 - val_tf.nn.log_softmax_Lev: 0.7164\n","Epoch 39/200\n","891/891 [==============================] - ETA: 0s - loss: 24.6208 - final_logsoft_loss: 23.1444 - tf.nn.log_softmax_loss: 26.0971 - final_logsoft_Lev: 0.7133 - tf.nn.log_softmax_Lev: 0.6778\n","Epoch 39: val_loss improved from 18.26444 to 18.23064, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.01 GB\n","891/891 [==============================] - 498s 557ms/step - loss: 24.6208 - final_logsoft_loss: 23.1444 - tf.nn.log_softmax_loss: 26.0971 - final_logsoft_Lev: 0.7133 - tf.nn.log_softmax_Lev: 0.6778 - val_loss: 18.2306 - val_final_logsoft_loss: 17.4912 - val_tf.nn.log_softmax_loss: 18.9715 - val_final_logsoft_Lev: 0.7398 - val_tf.nn.log_softmax_Lev: 0.7190\n","Epoch 40/200\n","891/891 [==============================] - ETA: 0s - loss: 24.4155 - final_logsoft_loss: 22.9269 - tf.nn.log_softmax_loss: 25.9041 - final_logsoft_Lev: 0.7152 - tf.nn.log_softmax_Lev: 0.6797\n","Epoch 40: val_loss did not improve from 18.23064\n","Memory usage on epoch end: 5.21 GB\n","891/891 [==============================] - 494s 553ms/step - loss: 24.4155 - final_logsoft_loss: 22.9269 - tf.nn.log_softmax_loss: 25.9041 - final_logsoft_Lev: 0.7152 - tf.nn.log_softmax_Lev: 0.6797 - val_loss: 18.2498 - val_final_logsoft_loss: 17.5853 - val_tf.nn.log_softmax_loss: 18.9130 - val_final_logsoft_Lev: 0.7395 - val_tf.nn.log_softmax_Lev: 0.7193\n","Epoch 41/200\n","891/891 [==============================] - ETA: 0s - loss: 24.3205 - final_logsoft_loss: 22.8082 - tf.nn.log_softmax_loss: 25.8325 - final_logsoft_Lev: 0.7178 - tf.nn.log_softmax_Lev: 0.6809\n","Epoch 41: val_loss improved from 18.23064 to 18.15285, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.12 GB\n","891/891 [==============================] - 495s 553ms/step - loss: 24.3205 - final_logsoft_loss: 22.8082 - tf.nn.log_softmax_loss: 25.8325 - final_logsoft_Lev: 0.7178 - tf.nn.log_softmax_Lev: 0.6809 - val_loss: 18.1529 - val_final_logsoft_loss: 17.4757 - val_tf.nn.log_softmax_loss: 18.8297 - val_final_logsoft_Lev: 0.7421 - val_tf.nn.log_softmax_Lev: 0.7208\n","Epoch 42/200\n","891/891 [==============================] - ETA: 0s - loss: 24.2116 - final_logsoft_loss: 22.6804 - tf.nn.log_softmax_loss: 25.7428 - final_logsoft_Lev: 0.7183 - tf.nn.log_softmax_Lev: 0.6821\n","Epoch 42: val_loss improved from 18.15285 to 17.95245, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.37 GB\n","891/891 [==============================] - 496s 555ms/step - loss: 24.2116 - final_logsoft_loss: 22.6804 - tf.nn.log_softmax_loss: 25.7428 - final_logsoft_Lev: 0.7183 - tf.nn.log_softmax_Lev: 0.6821 - val_loss: 17.9524 - val_final_logsoft_loss: 17.2237 - val_tf.nn.log_softmax_loss: 18.6827 - val_final_logsoft_Lev: 0.7414 - val_tf.nn.log_softmax_Lev: 0.7231\n","Epoch 43/200\n","891/891 [==============================] - ETA: 0s - loss: 24.0439 - final_logsoft_loss: 22.5132 - tf.nn.log_softmax_loss: 25.5744 - final_logsoft_Lev: 0.7206 - tf.nn.log_softmax_Lev: 0.6840\n","Epoch 43: val_loss did not improve from 17.95245\n","Memory usage on epoch end: 5.10 GB\n","891/891 [==============================] - 498s 557ms/step - loss: 24.0439 - final_logsoft_loss: 22.5132 - tf.nn.log_softmax_loss: 25.5744 - final_logsoft_Lev: 0.7206 - tf.nn.log_softmax_Lev: 0.6840 - val_loss: 18.0746 - val_final_logsoft_loss: 17.3901 - val_tf.nn.log_softmax_loss: 18.7607 - val_final_logsoft_Lev: 0.7428 - val_tf.nn.log_softmax_Lev: 0.7230\n","Epoch 44/200\n","891/891 [==============================] - ETA: 0s - loss: 23.8551 - final_logsoft_loss: 22.3068 - tf.nn.log_softmax_loss: 25.4034 - final_logsoft_Lev: 0.7231 - tf.nn.log_softmax_Lev: 0.6862\n","Epoch 44: val_loss did not improve from 17.95245\n","Memory usage on epoch end: 5.36 GB\n","891/891 [==============================] - 495s 553ms/step - loss: 23.8551 - final_logsoft_loss: 22.3068 - tf.nn.log_softmax_loss: 25.4034 - final_logsoft_Lev: 0.7231 - tf.nn.log_softmax_Lev: 0.6862 - val_loss: 17.9706 - val_final_logsoft_loss: 17.3037 - val_tf.nn.log_softmax_loss: 18.6359 - val_final_logsoft_Lev: 0.7462 - val_tf.nn.log_softmax_Lev: 0.7258\n","Epoch 45/200\n","891/891 [==============================] - ETA: 0s - loss: 23.7678 - final_logsoft_loss: 22.2226 - tf.nn.log_softmax_loss: 25.3134 - final_logsoft_Lev: 0.7237 - tf.nn.log_softmax_Lev: 0.6875\n","Epoch 45: val_loss did not improve from 17.95245\n","Memory usage on epoch end: 5.21 GB\n","891/891 [==============================] - 497s 556ms/step - loss: 23.7678 - final_logsoft_loss: 22.2226 - tf.nn.log_softmax_loss: 25.3134 - final_logsoft_Lev: 0.7237 - tf.nn.log_softmax_Lev: 0.6875 - val_loss: 17.9851 - val_final_logsoft_loss: 17.2303 - val_tf.nn.log_softmax_loss: 18.7396 - val_final_logsoft_Lev: 0.7441 - val_tf.nn.log_softmax_Lev: 0.7235\n","Epoch 46/200\n","891/891 [==============================] - ETA: 0s - loss: 23.6552 - final_logsoft_loss: 22.0838 - tf.nn.log_softmax_loss: 25.2274 - final_logsoft_Lev: 0.7256 - tf.nn.log_softmax_Lev: 0.6888\n","Epoch 46: val_loss improved from 17.95245 to 17.64980, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.36 GB\n","891/891 [==============================] - 497s 555ms/step - loss: 23.6552 - final_logsoft_loss: 22.0838 - tf.nn.log_softmax_loss: 25.2274 - final_logsoft_Lev: 0.7256 - tf.nn.log_softmax_Lev: 0.6888 - val_loss: 17.6498 - val_final_logsoft_loss: 16.9830 - val_tf.nn.log_softmax_loss: 18.3164 - val_final_logsoft_Lev: 0.7451 - val_tf.nn.log_softmax_Lev: 0.7265\n","Epoch 47/200\n","891/891 [==============================] - ETA: 0s - loss: 23.4952 - final_logsoft_loss: 21.9162 - tf.nn.log_softmax_loss: 25.0742 - final_logsoft_Lev: 0.7273 - tf.nn.log_softmax_Lev: 0.6905\n","Epoch 47: val_loss did not improve from 17.64980\n","Memory usage on epoch end: 5.13 GB\n","891/891 [==============================] - 492s 550ms/step - loss: 23.4952 - final_logsoft_loss: 21.9162 - tf.nn.log_softmax_loss: 25.0742 - final_logsoft_Lev: 0.7273 - tf.nn.log_softmax_Lev: 0.6905 - val_loss: 17.7903 - val_final_logsoft_loss: 17.0859 - val_tf.nn.log_softmax_loss: 18.4959 - val_final_logsoft_Lev: 0.7484 - val_tf.nn.log_softmax_Lev: 0.7296\n","Epoch 48/200\n","891/891 [==============================] - ETA: 0s - loss: 23.3666 - final_logsoft_loss: 21.7632 - tf.nn.log_softmax_loss: 24.9701 - final_logsoft_Lev: 0.7290 - tf.nn.log_softmax_Lev: 0.6920\n","Epoch 48: val_loss did not improve from 17.64980\n","Memory usage on epoch end: 5.59 GB\n","891/891 [==============================] - 495s 553ms/step - loss: 23.3666 - final_logsoft_loss: 21.7632 - tf.nn.log_softmax_loss: 24.9701 - final_logsoft_Lev: 0.7290 - tf.nn.log_softmax_Lev: 0.6920 - val_loss: 17.8509 - val_final_logsoft_loss: 17.2573 - val_tf.nn.log_softmax_loss: 18.4440 - val_final_logsoft_Lev: 0.7491 - val_tf.nn.log_softmax_Lev: 0.7277\n","Epoch 49/200\n","891/891 [==============================] - ETA: 0s - loss: 23.2133 - final_logsoft_loss: 21.6060 - tf.nn.log_softmax_loss: 24.8204 - final_logsoft_Lev: 0.7309 - tf.nn.log_softmax_Lev: 0.6938\n","Epoch 49: val_loss did not improve from 17.64980\n","Memory usage on epoch end: 5.26 GB\n","891/891 [==============================] - 496s 555ms/step - loss: 23.2133 - final_logsoft_loss: 21.6060 - tf.nn.log_softmax_loss: 24.8204 - final_logsoft_Lev: 0.7309 - tf.nn.log_softmax_Lev: 0.6938 - val_loss: 17.7889 - val_final_logsoft_loss: 17.1758 - val_tf.nn.log_softmax_loss: 18.3995 - val_final_logsoft_Lev: 0.7506 - val_tf.nn.log_softmax_Lev: 0.7316\n","Epoch 50/200\n","891/891 [==============================] - ETA: 0s - loss: 23.1205 - final_logsoft_loss: 21.4914 - tf.nn.log_softmax_loss: 24.7496 - final_logsoft_Lev: 0.7327 - tf.nn.log_softmax_Lev: 0.6944\n","Epoch 50: val_loss did not improve from 17.64980\n","Memory usage on epoch end: 5.38 GB\n","891/891 [==============================] - 495s 553ms/step - loss: 23.1205 - final_logsoft_loss: 21.4914 - tf.nn.log_softmax_loss: 24.7496 - final_logsoft_Lev: 0.7327 - tf.nn.log_softmax_Lev: 0.6944 - val_loss: 17.6515 - val_final_logsoft_loss: 16.9136 - val_tf.nn.log_softmax_loss: 18.3894 - val_final_logsoft_Lev: 0.7492 - val_tf.nn.log_softmax_Lev: 0.7291\n","Epoch 51/200\n","891/891 [==============================] - ETA: 0s - loss: 23.0375 - final_logsoft_loss: 21.4057 - tf.nn.log_softmax_loss: 24.6695 - final_logsoft_Lev: 0.7335 - tf.nn.log_softmax_Lev: 0.6954\n","Epoch 51: val_loss improved from 17.64980 to 17.47028, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.35 GB\n","891/891 [==============================] - 499s 556ms/step - loss: 23.0375 - final_logsoft_loss: 21.4057 - tf.nn.log_softmax_loss: 24.6695 - final_logsoft_Lev: 0.7335 - tf.nn.log_softmax_Lev: 0.6954 - val_loss: 17.4703 - val_final_logsoft_loss: 16.8641 - val_tf.nn.log_softmax_loss: 18.0763 - val_final_logsoft_Lev: 0.7513 - val_tf.nn.log_softmax_Lev: 0.7309\n","Epoch 52/200\n","891/891 [==============================] - ETA: 0s - loss: 22.8675 - final_logsoft_loss: 21.2126 - tf.nn.log_softmax_loss: 24.5229 - final_logsoft_Lev: 0.7359 - tf.nn.log_softmax_Lev: 0.6974\n","Epoch 52: val_loss did not improve from 17.47028\n","Memory usage on epoch end: 5.43 GB\n","891/891 [==============================] - 496s 555ms/step - loss: 22.8675 - final_logsoft_loss: 21.2126 - tf.nn.log_softmax_loss: 24.5229 - final_logsoft_Lev: 0.7359 - tf.nn.log_softmax_Lev: 0.6974 - val_loss: 17.6797 - val_final_logsoft_loss: 17.1578 - val_tf.nn.log_softmax_loss: 18.2028 - val_final_logsoft_Lev: 0.7527 - val_tf.nn.log_softmax_Lev: 0.7314\n","Epoch 53/200\n","891/891 [==============================] - ETA: 0s - loss: 22.8023 - final_logsoft_loss: 21.1561 - tf.nn.log_softmax_loss: 24.4486 - final_logsoft_Lev: 0.7364 - tf.nn.log_softmax_Lev: 0.6986\n","Epoch 53: val_loss did not improve from 17.47028\n","Memory usage on epoch end: 5.72 GB\n","891/891 [==============================] - 511s 572ms/step - loss: 22.8023 - final_logsoft_loss: 21.1561 - tf.nn.log_softmax_loss: 24.4486 - final_logsoft_Lev: 0.7364 - tf.nn.log_softmax_Lev: 0.6986 - val_loss: 17.7869 - val_final_logsoft_loss: 17.1863 - val_tf.nn.log_softmax_loss: 18.3876 - val_final_logsoft_Lev: 0.7527 - val_tf.nn.log_softmax_Lev: 0.7307\n","Epoch 54/200\n","891/891 [==============================] - ETA: 0s - loss: 22.7189 - final_logsoft_loss: 21.0389 - tf.nn.log_softmax_loss: 24.3988 - final_logsoft_Lev: 0.7379 - tf.nn.log_softmax_Lev: 0.6990\n","Epoch 54: val_loss did not improve from 17.47028\n","Memory usage on epoch end: 5.43 GB\n","891/891 [==============================] - 513s 572ms/step - loss: 22.7189 - final_logsoft_loss: 21.0389 - tf.nn.log_softmax_loss: 24.3988 - final_logsoft_Lev: 0.7379 - tf.nn.log_softmax_Lev: 0.6990 - val_loss: 17.5065 - val_final_logsoft_loss: 16.8701 - val_tf.nn.log_softmax_loss: 18.1439 - val_final_logsoft_Lev: 0.7550 - val_tf.nn.log_softmax_Lev: 0.7338\n","Epoch 55/200\n","891/891 [==============================] - ETA: 0s - loss: 22.5695 - final_logsoft_loss: 20.8796 - tf.nn.log_softmax_loss: 24.2589 - final_logsoft_Lev: 0.7394 - tf.nn.log_softmax_Lev: 0.7006\n","Epoch 55: val_loss did not improve from 17.47028\n","Memory usage on epoch end: 5.68 GB\n","891/891 [==============================] - 511s 571ms/step - loss: 22.5695 - final_logsoft_loss: 20.8796 - tf.nn.log_softmax_loss: 24.2589 - final_logsoft_Lev: 0.7394 - tf.nn.log_softmax_Lev: 0.7006 - val_loss: 17.4747 - val_final_logsoft_loss: 16.9348 - val_tf.nn.log_softmax_loss: 18.0149 - val_final_logsoft_Lev: 0.7562 - val_tf.nn.log_softmax_Lev: 0.7371\n","Epoch 56/200\n","891/891 [==============================] - ETA: 0s - loss: 22.4745 - final_logsoft_loss: 20.7805 - tf.nn.log_softmax_loss: 24.1677 - final_logsoft_Lev: 0.7406 - tf.nn.log_softmax_Lev: 0.7017\n","Epoch 56: val_loss improved from 17.47028 to 17.17238, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.55 GB\n","891/891 [==============================] - 512s 572ms/step - loss: 22.4745 - final_logsoft_loss: 20.7805 - tf.nn.log_softmax_loss: 24.1677 - final_logsoft_Lev: 0.7406 - tf.nn.log_softmax_Lev: 0.7017 - val_loss: 17.1724 - val_final_logsoft_loss: 16.5503 - val_tf.nn.log_softmax_loss: 17.7926 - val_final_logsoft_Lev: 0.7549 - val_tf.nn.log_softmax_Lev: 0.7362\n","Epoch 57/200\n","891/891 [==============================] - ETA: 0s - loss: 22.3421 - final_logsoft_loss: 20.6195 - tf.nn.log_softmax_loss: 24.0648 - final_logsoft_Lev: 0.7427 - tf.nn.log_softmax_Lev: 0.7029\n","Epoch 57: val_loss did not improve from 17.17238\n","Memory usage on epoch end: 5.64 GB\n","891/891 [==============================] - 515s 575ms/step - loss: 22.3421 - final_logsoft_loss: 20.6195 - tf.nn.log_softmax_loss: 24.0648 - final_logsoft_Lev: 0.7427 - tf.nn.log_softmax_Lev: 0.7029 - val_loss: 17.2476 - val_final_logsoft_loss: 16.5151 - val_tf.nn.log_softmax_loss: 17.9803 - val_final_logsoft_Lev: 0.7549 - val_tf.nn.log_softmax_Lev: 0.7353\n","Epoch 58/200\n","891/891 [==============================] - ETA: 0s - loss: 22.3087 - final_logsoft_loss: 20.5872 - tf.nn.log_softmax_loss: 24.0300 - final_logsoft_Lev: 0.7432 - tf.nn.log_softmax_Lev: 0.7036\n","Epoch 58: val_loss did not improve from 17.17238\n","Memory usage on epoch end: 5.39 GB\n","891/891 [==============================] - 515s 575ms/step - loss: 22.3087 - final_logsoft_loss: 20.5872 - tf.nn.log_softmax_loss: 24.0300 - final_logsoft_Lev: 0.7432 - tf.nn.log_softmax_Lev: 0.7036 - val_loss: 17.3011 - val_final_logsoft_loss: 16.6590 - val_tf.nn.log_softmax_loss: 17.9394 - val_final_logsoft_Lev: 0.7564 - val_tf.nn.log_softmax_Lev: 0.7358\n","Epoch 59/200\n","891/891 [==============================] - ETA: 0s - loss: 22.1802 - final_logsoft_loss: 20.4513 - tf.nn.log_softmax_loss: 23.9087 - final_logsoft_Lev: 0.7448 - tf.nn.log_softmax_Lev: 0.7048\n","Epoch 59: val_loss did not improve from 17.17238\n","Memory usage on epoch end: 5.83 GB\n","891/891 [==============================] - 515s 576ms/step - loss: 22.1802 - final_logsoft_loss: 20.4513 - tf.nn.log_softmax_loss: 23.9087 - final_logsoft_Lev: 0.7448 - tf.nn.log_softmax_Lev: 0.7048 - val_loss: 17.7301 - val_final_logsoft_loss: 17.1418 - val_tf.nn.log_softmax_loss: 18.3176 - val_final_logsoft_Lev: 0.7549 - val_tf.nn.log_softmax_Lev: 0.7330\n","Epoch 60/200\n","891/891 [==============================] - ETA: 0s - loss: 22.1195 - final_logsoft_loss: 20.3823 - tf.nn.log_softmax_loss: 23.8570 - final_logsoft_Lev: 0.7454 - tf.nn.log_softmax_Lev: 0.7052\n","Epoch 60: val_loss did not improve from 17.17238\n","Memory usage on epoch end: 5.50 GB\n","891/891 [==============================] - 516s 576ms/step - loss: 22.1195 - final_logsoft_loss: 20.3823 - tf.nn.log_softmax_loss: 23.8570 - final_logsoft_Lev: 0.7454 - tf.nn.log_softmax_Lev: 0.7052 - val_loss: 17.3173 - val_final_logsoft_loss: 16.7184 - val_tf.nn.log_softmax_loss: 17.9175 - val_final_logsoft_Lev: 0.7571 - val_tf.nn.log_softmax_Lev: 0.7369\n","Epoch 61/200\n","891/891 [==============================] - ETA: 0s - loss: 22.0110 - final_logsoft_loss: 20.2661 - tf.nn.log_softmax_loss: 23.7561 - final_logsoft_Lev: 0.7469 - tf.nn.log_softmax_Lev: 0.7068\n","Epoch 61: val_loss improved from 17.17238 to 17.12075, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.48 GB\n","891/891 [==============================] - 517s 578ms/step - loss: 22.0110 - final_logsoft_loss: 20.2661 - tf.nn.log_softmax_loss: 23.7561 - final_logsoft_Lev: 0.7469 - tf.nn.log_softmax_Lev: 0.7068 - val_loss: 17.1208 - val_final_logsoft_loss: 16.6065 - val_tf.nn.log_softmax_loss: 17.6342 - val_final_logsoft_Lev: 0.7599 - val_tf.nn.log_softmax_Lev: 0.7405\n","Epoch 62/200\n","891/891 [==============================] - ETA: 0s - loss: 21.8663 - final_logsoft_loss: 20.1164 - tf.nn.log_softmax_loss: 23.6162 - final_logsoft_Lev: 0.7487 - tf.nn.log_softmax_Lev: 0.7084\n","Epoch 62: val_loss did not improve from 17.12075\n","Memory usage on epoch end: 5.40 GB\n","891/891 [==============================] - 509s 569ms/step - loss: 21.8663 - final_logsoft_loss: 20.1164 - tf.nn.log_softmax_loss: 23.6162 - final_logsoft_Lev: 0.7487 - tf.nn.log_softmax_Lev: 0.7084 - val_loss: 17.2322 - val_final_logsoft_loss: 16.7060 - val_tf.nn.log_softmax_loss: 17.7576 - val_final_logsoft_Lev: 0.7579 - val_tf.nn.log_softmax_Lev: 0.7379\n","Epoch 63/200\n","891/891 [==============================] - ETA: 0s - loss: 21.8009 - final_logsoft_loss: 20.0214 - tf.nn.log_softmax_loss: 23.5799 - final_logsoft_Lev: 0.7496 - tf.nn.log_softmax_Lev: 0.7089\n","Epoch 63: val_loss did not improve from 17.12075\n","Memory usage on epoch end: 5.65 GB\n","891/891 [==============================] - 500s 559ms/step - loss: 21.8009 - final_logsoft_loss: 20.0214 - tf.nn.log_softmax_loss: 23.5799 - final_logsoft_Lev: 0.7496 - tf.nn.log_softmax_Lev: 0.7089 - val_loss: 17.1724 - val_final_logsoft_loss: 16.5600 - val_tf.nn.log_softmax_loss: 17.7833 - val_final_logsoft_Lev: 0.7580 - val_tf.nn.log_softmax_Lev: 0.7398\n","Epoch 64/200\n","891/891 [==============================] - ETA: 0s - loss: 21.7243 - final_logsoft_loss: 19.9524 - tf.nn.log_softmax_loss: 23.4963 - final_logsoft_Lev: 0.7506 - tf.nn.log_softmax_Lev: 0.7098\n","Epoch 64: val_loss did not improve from 17.12075\n","Memory usage on epoch end: 5.39 GB\n","891/891 [==============================] - 502s 561ms/step - loss: 21.7243 - final_logsoft_loss: 19.9524 - tf.nn.log_softmax_loss: 23.4963 - final_logsoft_Lev: 0.7506 - tf.nn.log_softmax_Lev: 0.7098 - val_loss: 17.1625 - val_final_logsoft_loss: 16.5075 - val_tf.nn.log_softmax_loss: 17.8184 - val_final_logsoft_Lev: 0.7600 - val_tf.nn.log_softmax_Lev: 0.7389\n","Epoch 65/200\n","891/891 [==============================] - ETA: 0s - loss: 21.6312 - final_logsoft_loss: 19.8463 - tf.nn.log_softmax_loss: 23.4159 - final_logsoft_Lev: 0.7518 - tf.nn.log_softmax_Lev: 0.7108\n","Epoch 65: val_loss did not improve from 17.12075\n","Memory usage on epoch end: 5.72 GB\n","891/891 [==============================] - 502s 561ms/step - loss: 21.6312 - final_logsoft_loss: 19.8463 - tf.nn.log_softmax_loss: 23.4159 - final_logsoft_Lev: 0.7518 - tf.nn.log_softmax_Lev: 0.7108 - val_loss: 17.1484 - val_final_logsoft_loss: 16.5963 - val_tf.nn.log_softmax_loss: 17.7002 - val_final_logsoft_Lev: 0.7609 - val_tf.nn.log_softmax_Lev: 0.7413\n","Epoch 66/200\n","891/891 [==============================] - ETA: 0s - loss: 21.5039 - final_logsoft_loss: 19.7045 - tf.nn.log_softmax_loss: 23.3042 - final_logsoft_Lev: 0.7531 - tf.nn.log_softmax_Lev: 0.7121\n","Epoch 66: val_loss improved from 17.12075 to 17.11549, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.39 GB\n","891/891 [==============================] - 503s 562ms/step - loss: 21.5039 - final_logsoft_loss: 19.7045 - tf.nn.log_softmax_loss: 23.3042 - final_logsoft_Lev: 0.7531 - tf.nn.log_softmax_Lev: 0.7121 - val_loss: 17.1155 - val_final_logsoft_loss: 16.5377 - val_tf.nn.log_softmax_loss: 17.6938 - val_final_logsoft_Lev: 0.7617 - val_tf.nn.log_softmax_Lev: 0.7435\n","Epoch 67/200\n","891/891 [==============================] - ETA: 0s - loss: 21.4680 - final_logsoft_loss: 19.6693 - tf.nn.log_softmax_loss: 23.2663 - final_logsoft_Lev: 0.7539 - tf.nn.log_softmax_Lev: 0.7128\n","Epoch 67: val_loss improved from 17.11549 to 17.02768, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.56 GB\n","891/891 [==============================] - 501s 559ms/step - loss: 21.4680 - final_logsoft_loss: 19.6693 - tf.nn.log_softmax_loss: 23.2663 - final_logsoft_Lev: 0.7539 - tf.nn.log_softmax_Lev: 0.7128 - val_loss: 17.0277 - val_final_logsoft_loss: 16.3906 - val_tf.nn.log_softmax_loss: 17.6622 - val_final_logsoft_Lev: 0.7613 - val_tf.nn.log_softmax_Lev: 0.7434\n","Epoch 68/200\n","891/891 [==============================] - ETA: 0s - loss: 21.3776 - final_logsoft_loss: 19.5714 - tf.nn.log_softmax_loss: 23.1835 - final_logsoft_Lev: 0.7553 - tf.nn.log_softmax_Lev: 0.7135\n","Epoch 68: val_loss improved from 17.02768 to 16.96586, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.51 GB\n","891/891 [==============================] - 503s 562ms/step - loss: 21.3776 - final_logsoft_loss: 19.5714 - tf.nn.log_softmax_loss: 23.1835 - final_logsoft_Lev: 0.7553 - tf.nn.log_softmax_Lev: 0.7135 - val_loss: 16.9659 - val_final_logsoft_loss: 16.4737 - val_tf.nn.log_softmax_loss: 17.4570 - val_final_logsoft_Lev: 0.7615 - val_tf.nn.log_softmax_Lev: 0.7408\n","Epoch 69/200\n","891/891 [==============================] - ETA: 0s - loss: 21.2694 - final_logsoft_loss: 19.4531 - tf.nn.log_softmax_loss: 23.0856 - final_logsoft_Lev: 0.7561 - tf.nn.log_softmax_Lev: 0.7150\n","Epoch 69: val_loss improved from 16.96586 to 16.94803, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.47 GB\n","891/891 [==============================] - 502s 561ms/step - loss: 21.2694 - final_logsoft_loss: 19.4531 - tf.nn.log_softmax_loss: 23.0856 - final_logsoft_Lev: 0.7561 - tf.nn.log_softmax_Lev: 0.7150 - val_loss: 16.9480 - val_final_logsoft_loss: 16.3482 - val_tf.nn.log_softmax_loss: 17.5484 - val_final_logsoft_Lev: 0.7631 - val_tf.nn.log_softmax_Lev: 0.7436\n","Epoch 70/200\n","891/891 [==============================] - ETA: 0s - loss: 21.1985 - final_logsoft_loss: 19.3834 - tf.nn.log_softmax_loss: 23.0137 - final_logsoft_Lev: 0.7573 - tf.nn.log_softmax_Lev: 0.7156\n","Epoch 70: val_loss did not improve from 16.94803\n","Memory usage on epoch end: 5.56 GB\n","891/891 [==============================] - 498s 557ms/step - loss: 21.1985 - final_logsoft_loss: 19.3834 - tf.nn.log_softmax_loss: 23.0137 - final_logsoft_Lev: 0.7573 - tf.nn.log_softmax_Lev: 0.7156 - val_loss: 16.9876 - val_final_logsoft_loss: 16.5309 - val_tf.nn.log_softmax_loss: 17.4441 - val_final_logsoft_Lev: 0.7620 - val_tf.nn.log_softmax_Lev: 0.7407\n","Epoch 71/200\n","891/891 [==============================] - ETA: 0s - loss: 21.1183 - final_logsoft_loss: 19.2855 - tf.nn.log_softmax_loss: 22.9507 - final_logsoft_Lev: 0.7583 - tf.nn.log_softmax_Lev: 0.7166\n","Epoch 71: val_loss improved from 16.94803 to 16.80401, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.80 GB\n","891/891 [==============================] - 500s 559ms/step - loss: 21.1183 - final_logsoft_loss: 19.2855 - tf.nn.log_softmax_loss: 22.9507 - final_logsoft_Lev: 0.7583 - tf.nn.log_softmax_Lev: 0.7166 - val_loss: 16.8040 - val_final_logsoft_loss: 16.3013 - val_tf.nn.log_softmax_loss: 17.3088 - val_final_logsoft_Lev: 0.7629 - val_tf.nn.log_softmax_Lev: 0.7450\n","Epoch 72/200\n","891/891 [==============================] - ETA: 0s - loss: 21.0296 - final_logsoft_loss: 19.2021 - tf.nn.log_softmax_loss: 22.8573 - final_logsoft_Lev: 0.7592 - tf.nn.log_softmax_Lev: 0.7176\n","Epoch 72: val_loss did not improve from 16.80401\n","Memory usage on epoch end: 5.77 GB\n","891/891 [==============================] - 499s 558ms/step - loss: 21.0296 - final_logsoft_loss: 19.2021 - tf.nn.log_softmax_loss: 22.8573 - final_logsoft_Lev: 0.7592 - tf.nn.log_softmax_Lev: 0.7176 - val_loss: 16.9490 - val_final_logsoft_loss: 16.3860 - val_tf.nn.log_softmax_loss: 17.5124 - val_final_logsoft_Lev: 0.7658 - val_tf.nn.log_softmax_Lev: 0.7453\n","Epoch 73/200\n","891/891 [==============================] - ETA: 0s - loss: 20.9483 - final_logsoft_loss: 19.1052 - tf.nn.log_softmax_loss: 22.7918 - final_logsoft_Lev: 0.7610 - tf.nn.log_softmax_Lev: 0.7186\n","Epoch 73: val_loss did not improve from 16.80401\n","Memory usage on epoch end: 5.97 GB\n","891/891 [==============================] - 500s 559ms/step - loss: 20.9483 - final_logsoft_loss: 19.1052 - tf.nn.log_softmax_loss: 22.7918 - final_logsoft_Lev: 0.7610 - tf.nn.log_softmax_Lev: 0.7186 - val_loss: 16.8147 - val_final_logsoft_loss: 16.3337 - val_tf.nn.log_softmax_loss: 17.2965 - val_final_logsoft_Lev: 0.7641 - val_tf.nn.log_softmax_Lev: 0.7448\n","Epoch 74/200\n","891/891 [==============================] - ETA: 0s - loss: 20.8868 - final_logsoft_loss: 19.0282 - tf.nn.log_softmax_loss: 22.7453 - final_logsoft_Lev: 0.7613 - tf.nn.log_softmax_Lev: 0.7189\n","Epoch 74: val_loss did not improve from 16.80401\n","Memory usage on epoch end: 5.72 GB\n","891/891 [==============================] - 502s 561ms/step - loss: 20.8868 - final_logsoft_loss: 19.0282 - tf.nn.log_softmax_loss: 22.7453 - final_logsoft_Lev: 0.7613 - tf.nn.log_softmax_Lev: 0.7189 - val_loss: 16.8653 - val_final_logsoft_loss: 16.3159 - val_tf.nn.log_softmax_loss: 17.4149 - val_final_logsoft_Lev: 0.7641 - val_tf.nn.log_softmax_Lev: 0.7463\n","Epoch 75/200\n","891/891 [==============================] - ETA: 0s - loss: 20.7788 - final_logsoft_loss: 18.9095 - tf.nn.log_softmax_loss: 22.6484 - final_logsoft_Lev: 0.7625 - tf.nn.log_softmax_Lev: 0.7197\n","Epoch 75: val_loss improved from 16.80401 to 16.70941, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.41 GB\n","891/891 [==============================] - 501s 560ms/step - loss: 20.7788 - final_logsoft_loss: 18.9095 - tf.nn.log_softmax_loss: 22.6484 - final_logsoft_Lev: 0.7625 - tf.nn.log_softmax_Lev: 0.7197 - val_loss: 16.7094 - val_final_logsoft_loss: 16.1810 - val_tf.nn.log_softmax_loss: 17.2381 - val_final_logsoft_Lev: 0.7667 - val_tf.nn.log_softmax_Lev: 0.7467\n","Epoch 76/200\n","891/891 [==============================] - ETA: 0s - loss: 20.7137 - final_logsoft_loss: 18.8430 - tf.nn.log_softmax_loss: 22.5837 - final_logsoft_Lev: 0.7639 - tf.nn.log_softmax_Lev: 0.7209\n","Epoch 76: val_loss did not improve from 16.70941\n","Memory usage on epoch end: 5.93 GB\n","891/891 [==============================] - 516s 577ms/step - loss: 20.7137 - final_logsoft_loss: 18.8430 - tf.nn.log_softmax_loss: 22.5837 - final_logsoft_Lev: 0.7639 - tf.nn.log_softmax_Lev: 0.7209 - val_loss: 16.8015 - val_final_logsoft_loss: 16.3149 - val_tf.nn.log_softmax_loss: 17.2908 - val_final_logsoft_Lev: 0.7647 - val_tf.nn.log_softmax_Lev: 0.7469\n","Epoch 77/200\n","891/891 [==============================] - ETA: 0s - loss: 20.6927 - final_logsoft_loss: 18.8243 - tf.nn.log_softmax_loss: 22.5609 - final_logsoft_Lev: 0.7638 - tf.nn.log_softmax_Lev: 0.7211\n","Epoch 77: val_loss did not improve from 16.70941\n","Memory usage on epoch end: 5.87 GB\n","891/891 [==============================] - 517s 578ms/step - loss: 20.6927 - final_logsoft_loss: 18.8243 - tf.nn.log_softmax_loss: 22.5609 - final_logsoft_Lev: 0.7638 - tf.nn.log_softmax_Lev: 0.7211 - val_loss: 16.8317 - val_final_logsoft_loss: 16.2952 - val_tf.nn.log_softmax_loss: 17.3711 - val_final_logsoft_Lev: 0.7654 - val_tf.nn.log_softmax_Lev: 0.7481\n","Epoch 78/200\n","891/891 [==============================] - ETA: 0s - loss: 20.5593 - final_logsoft_loss: 18.6653 - tf.nn.log_softmax_loss: 22.4535 - final_logsoft_Lev: 0.7657 - tf.nn.log_softmax_Lev: 0.7226\n","Epoch 78: val_loss did not improve from 16.70941\n","Memory usage on epoch end: 5.83 GB\n","891/891 [==============================] - 514s 575ms/step - loss: 20.5593 - final_logsoft_loss: 18.6653 - tf.nn.log_softmax_loss: 22.4535 - final_logsoft_Lev: 0.7657 - tf.nn.log_softmax_Lev: 0.7226 - val_loss: 16.9474 - val_final_logsoft_loss: 16.3750 - val_tf.nn.log_softmax_loss: 17.5197 - val_final_logsoft_Lev: 0.7667 - val_tf.nn.log_softmax_Lev: 0.7480\n","Epoch 79/200\n","891/891 [==============================] - ETA: 0s - loss: 20.5218 - final_logsoft_loss: 18.6382 - tf.nn.log_softmax_loss: 22.4050 - final_logsoft_Lev: 0.7659 - tf.nn.log_softmax_Lev: 0.7230\n","Epoch 79: val_loss did not improve from 16.70941\n","Memory usage on epoch end: 5.80 GB\n","891/891 [==============================] - 475s 530ms/step - loss: 20.5218 - final_logsoft_loss: 18.6382 - tf.nn.log_softmax_loss: 22.4050 - final_logsoft_Lev: 0.7659 - tf.nn.log_softmax_Lev: 0.7230 - val_loss: 16.7174 - val_final_logsoft_loss: 16.2113 - val_tf.nn.log_softmax_loss: 17.2242 - val_final_logsoft_Lev: 0.7677 - val_tf.nn.log_softmax_Lev: 0.7470\n","Epoch 80/200\n","891/891 [==============================] - ETA: 0s - loss: 20.4706 - final_logsoft_loss: 18.5717 - tf.nn.log_softmax_loss: 22.3699 - final_logsoft_Lev: 0.7670 - tf.nn.log_softmax_Lev: 0.7233\n","Epoch 80: val_loss improved from 16.70941 to 16.68733, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.96 GB\n","891/891 [==============================] - 465s 520ms/step - loss: 20.4706 - final_logsoft_loss: 18.5717 - tf.nn.log_softmax_loss: 22.3699 - final_logsoft_Lev: 0.7670 - tf.nn.log_softmax_Lev: 0.7233 - val_loss: 16.6873 - val_final_logsoft_loss: 16.1262 - val_tf.nn.log_softmax_loss: 17.2468 - val_final_logsoft_Lev: 0.7661 - val_tf.nn.log_softmax_Lev: 0.7467\n","Epoch 81/200\n","891/891 [==============================] - ETA: 0s - loss: 20.3570 - final_logsoft_loss: 18.4443 - tf.nn.log_softmax_loss: 22.2700 - final_logsoft_Lev: 0.7682 - tf.nn.log_softmax_Lev: 0.7244\n","Epoch 81: val_loss improved from 16.68733 to 16.53074, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.62 GB\n","891/891 [==============================] - 465s 519ms/step - loss: 20.3570 - final_logsoft_loss: 18.4443 - tf.nn.log_softmax_loss: 22.2700 - final_logsoft_Lev: 0.7682 - tf.nn.log_softmax_Lev: 0.7244 - val_loss: 16.5307 - val_final_logsoft_loss: 16.0143 - val_tf.nn.log_softmax_loss: 17.0479 - val_final_logsoft_Lev: 0.7686 - val_tf.nn.log_softmax_Lev: 0.7482\n","Epoch 82/200\n","891/891 [==============================] - ETA: 0s - loss: 20.2839 - final_logsoft_loss: 18.3769 - tf.nn.log_softmax_loss: 22.1910 - final_logsoft_Lev: 0.7694 - tf.nn.log_softmax_Lev: 0.7258\n","Epoch 82: val_loss did not improve from 16.53074\n","Memory usage on epoch end: 5.95 GB\n","891/891 [==============================] - 464s 519ms/step - loss: 20.2839 - final_logsoft_loss: 18.3769 - tf.nn.log_softmax_loss: 22.1910 - final_logsoft_Lev: 0.7694 - tf.nn.log_softmax_Lev: 0.7258 - val_loss: 16.6709 - val_final_logsoft_loss: 16.1004 - val_tf.nn.log_softmax_loss: 17.2417 - val_final_logsoft_Lev: 0.7661 - val_tf.nn.log_softmax_Lev: 0.7501\n","Epoch 83/200\n","891/891 [==============================] - ETA: 0s - loss: 20.2256 - final_logsoft_loss: 18.2940 - tf.nn.log_softmax_loss: 22.1567 - final_logsoft_Lev: 0.7702 - tf.nn.log_softmax_Lev: 0.7262\n","Epoch 83: val_loss improved from 16.53074 to 16.50934, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.52 GB\n","891/891 [==============================] - 465s 520ms/step - loss: 20.2256 - final_logsoft_loss: 18.2940 - tf.nn.log_softmax_loss: 22.1567 - final_logsoft_Lev: 0.7702 - tf.nn.log_softmax_Lev: 0.7262 - val_loss: 16.5093 - val_final_logsoft_loss: 16.0309 - val_tf.nn.log_softmax_loss: 16.9886 - val_final_logsoft_Lev: 0.7662 - val_tf.nn.log_softmax_Lev: 0.7469\n","Epoch 84/200\n","891/891 [==============================] - ETA: 0s - loss: 20.1706 - final_logsoft_loss: 18.2467 - tf.nn.log_softmax_loss: 22.0950 - final_logsoft_Lev: 0.7710 - tf.nn.log_softmax_Lev: 0.7267\n","Epoch 84: val_loss did not improve from 16.50934\n","Memory usage on epoch end: 6.07 GB\n","891/891 [==============================] - 464s 518ms/step - loss: 20.1706 - final_logsoft_loss: 18.2467 - tf.nn.log_softmax_loss: 22.0950 - final_logsoft_Lev: 0.7710 - tf.nn.log_softmax_Lev: 0.7267 - val_loss: 16.7023 - val_final_logsoft_loss: 16.1817 - val_tf.nn.log_softmax_loss: 17.2223 - val_final_logsoft_Lev: 0.7691 - val_tf.nn.log_softmax_Lev: 0.7498\n","Epoch 85/200\n","891/891 [==============================] - ETA: 0s - loss: 20.1480 - final_logsoft_loss: 18.2719 - tf.nn.log_softmax_loss: 22.0242 - final_logsoft_Lev: 0.7715 - tf.nn.log_softmax_Lev: 0.7277\n","Epoch 85: val_loss did not improve from 16.50934\n","Memory usage on epoch end: 5.34 GB\n","891/891 [==============================] - 464s 519ms/step - loss: 20.1480 - final_logsoft_loss: 18.2719 - tf.nn.log_softmax_loss: 22.0242 - final_logsoft_Lev: 0.7715 - tf.nn.log_softmax_Lev: 0.7277 - val_loss: 16.7446 - val_final_logsoft_loss: 16.2485 - val_tf.nn.log_softmax_loss: 17.2415 - val_final_logsoft_Lev: 0.7681 - val_tf.nn.log_softmax_Lev: 0.7500\n","Epoch 86/200\n","891/891 [==============================] - ETA: 0s - loss: 20.0355 - final_logsoft_loss: 18.1067 - tf.nn.log_softmax_loss: 21.9644 - final_logsoft_Lev: 0.7727 - tf.nn.log_softmax_Lev: 0.7284\n","Epoch 86: val_loss did not improve from 16.50934\n","Memory usage on epoch end: 5.93 GB\n","891/891 [==============================] - 461s 516ms/step - loss: 20.0355 - final_logsoft_loss: 18.1067 - tf.nn.log_softmax_loss: 21.9644 - final_logsoft_Lev: 0.7727 - tf.nn.log_softmax_Lev: 0.7284 - val_loss: 16.7420 - val_final_logsoft_loss: 16.2756 - val_tf.nn.log_softmax_loss: 17.2089 - val_final_logsoft_Lev: 0.7702 - val_tf.nn.log_softmax_Lev: 0.7512\n","Epoch 87/200\n","891/891 [==============================] - ETA: 0s - loss: 19.9585 - final_logsoft_loss: 18.0213 - tf.nn.log_softmax_loss: 21.8952 - final_logsoft_Lev: 0.7735 - tf.nn.log_softmax_Lev: 0.7290\n","Epoch 87: val_loss did not improve from 16.50934\n","Memory usage on epoch end: 5.16 GB\n","891/891 [==============================] - 463s 517ms/step - loss: 19.9585 - final_logsoft_loss: 18.0213 - tf.nn.log_softmax_loss: 21.8952 - final_logsoft_Lev: 0.7735 - tf.nn.log_softmax_Lev: 0.7290 - val_loss: 16.6422 - val_final_logsoft_loss: 16.1848 - val_tf.nn.log_softmax_loss: 17.0977 - val_final_logsoft_Lev: 0.7714 - val_tf.nn.log_softmax_Lev: 0.7503\n","Epoch 88/200\n","891/891 [==============================] - ETA: 0s - loss: 19.9008 - final_logsoft_loss: 17.9413 - tf.nn.log_softmax_loss: 21.8611 - final_logsoft_Lev: 0.7743 - tf.nn.log_softmax_Lev: 0.7294\n","Epoch 88: val_loss did not improve from 16.50934\n","Memory usage on epoch end: 5.91 GB\n","891/891 [==============================] - 459s 513ms/step - loss: 19.9008 - final_logsoft_loss: 17.9413 - tf.nn.log_softmax_loss: 21.8611 - final_logsoft_Lev: 0.7743 - tf.nn.log_softmax_Lev: 0.7294 - val_loss: 16.6579 - val_final_logsoft_loss: 16.1760 - val_tf.nn.log_softmax_loss: 17.1405 - val_final_logsoft_Lev: 0.7709 - val_tf.nn.log_softmax_Lev: 0.7516\n","Epoch 89/200\n","891/891 [==============================] - ETA: 0s - loss: 19.8638 - final_logsoft_loss: 17.9016 - tf.nn.log_softmax_loss: 21.8259 - final_logsoft_Lev: 0.7752 - tf.nn.log_softmax_Lev: 0.7299\n","Epoch 89: val_loss did not improve from 16.50934\n","Memory usage on epoch end: 5.05 GB\n","891/891 [==============================] - 460s 514ms/step - loss: 19.8638 - final_logsoft_loss: 17.9016 - tf.nn.log_softmax_loss: 21.8259 - final_logsoft_Lev: 0.7752 - tf.nn.log_softmax_Lev: 0.7299 - val_loss: 16.5357 - val_final_logsoft_loss: 16.0778 - val_tf.nn.log_softmax_loss: 16.9932 - val_final_logsoft_Lev: 0.7693 - val_tf.nn.log_softmax_Lev: 0.7507\n","Epoch 90/200\n","891/891 [==============================] - ETA: 0s - loss: 19.7512 - final_logsoft_loss: 17.8058 - tf.nn.log_softmax_loss: 21.6970 - final_logsoft_Lev: 0.7762 - tf.nn.log_softmax_Lev: 0.7313\n","Epoch 90: val_loss did not improve from 16.50934\n","Memory usage on epoch end: 5.85 GB\n","891/891 [==============================] - 460s 514ms/step - loss: 19.7512 - final_logsoft_loss: 17.8058 - tf.nn.log_softmax_loss: 21.6970 - final_logsoft_Lev: 0.7762 - tf.nn.log_softmax_Lev: 0.7313 - val_loss: 16.6316 - val_final_logsoft_loss: 16.1413 - val_tf.nn.log_softmax_loss: 17.1236 - val_final_logsoft_Lev: 0.7720 - val_tf.nn.log_softmax_Lev: 0.7533\n","Epoch 91/200\n","891/891 [==============================] - ETA: 0s - loss: 19.7043 - final_logsoft_loss: 17.7423 - tf.nn.log_softmax_loss: 21.6670 - final_logsoft_Lev: 0.7768 - tf.nn.log_softmax_Lev: 0.7316\n","Epoch 91: val_loss did not improve from 16.50934\n","Memory usage on epoch end: 5.11 GB\n","891/891 [==============================] - 460s 514ms/step - loss: 19.7043 - final_logsoft_loss: 17.7423 - tf.nn.log_softmax_loss: 21.6670 - final_logsoft_Lev: 0.7768 - tf.nn.log_softmax_Lev: 0.7316 - val_loss: 16.5921 - val_final_logsoft_loss: 16.0428 - val_tf.nn.log_softmax_loss: 17.1393 - val_final_logsoft_Lev: 0.7719 - val_tf.nn.log_softmax_Lev: 0.7526\n","Epoch 92/200\n","891/891 [==============================] - ETA: 0s - loss: 19.6347 - final_logsoft_loss: 17.6705 - tf.nn.log_softmax_loss: 21.5992 - final_logsoft_Lev: 0.7777 - tf.nn.log_softmax_Lev: 0.7325\n","Epoch 92: val_loss did not improve from 16.50934\n","Memory usage on epoch end: 5.88 GB\n","891/891 [==============================] - 460s 514ms/step - loss: 19.6347 - final_logsoft_loss: 17.6705 - tf.nn.log_softmax_loss: 21.5992 - final_logsoft_Lev: 0.7777 - tf.nn.log_softmax_Lev: 0.7325 - val_loss: 16.5528 - val_final_logsoft_loss: 16.0764 - val_tf.nn.log_softmax_loss: 17.0321 - val_final_logsoft_Lev: 0.7731 - val_tf.nn.log_softmax_Lev: 0.7520\n","Epoch 93/200\n","891/891 [==============================] - ETA: 0s - loss: 19.5566 - final_logsoft_loss: 17.5887 - tf.nn.log_softmax_loss: 21.5245 - final_logsoft_Lev: 0.7785 - tf.nn.log_softmax_Lev: 0.7333\n","Epoch 93: val_loss did not improve from 16.50934\n","Memory usage on epoch end: 5.10 GB\n","891/891 [==============================] - 461s 516ms/step - loss: 19.5566 - final_logsoft_loss: 17.5887 - tf.nn.log_softmax_loss: 21.5245 - final_logsoft_Lev: 0.7785 - tf.nn.log_softmax_Lev: 0.7333 - val_loss: 16.5598 - val_final_logsoft_loss: 16.1320 - val_tf.nn.log_softmax_loss: 16.9878 - val_final_logsoft_Lev: 0.7739 - val_tf.nn.log_softmax_Lev: 0.7526\n","Epoch 94/200\n","891/891 [==============================] - ETA: 0s - loss: 19.4994 - final_logsoft_loss: 17.5155 - tf.nn.log_softmax_loss: 21.4834 - final_logsoft_Lev: 0.7794 - tf.nn.log_softmax_Lev: 0.7337\n","Epoch 94: val_loss did not improve from 16.50934\n","Memory usage on epoch end: 6.05 GB\n","891/891 [==============================] - 460s 514ms/step - loss: 19.4994 - final_logsoft_loss: 17.5155 - tf.nn.log_softmax_loss: 21.4834 - final_logsoft_Lev: 0.7794 - tf.nn.log_softmax_Lev: 0.7337 - val_loss: 16.6170 - val_final_logsoft_loss: 16.0960 - val_tf.nn.log_softmax_loss: 17.1359 - val_final_logsoft_Lev: 0.7727 - val_tf.nn.log_softmax_Lev: 0.7537\n","Epoch 95/200\n","891/891 [==============================] - ETA: 0s - loss: 19.4279 - final_logsoft_loss: 17.4419 - tf.nn.log_softmax_loss: 21.4134 - final_logsoft_Lev: 0.7801 - tf.nn.log_softmax_Lev: 0.7347\n","Epoch 95: val_loss did not improve from 16.50934\n","Memory usage on epoch end: 5.24 GB\n","891/891 [==============================] - 461s 516ms/step - loss: 19.4279 - final_logsoft_loss: 17.4419 - tf.nn.log_softmax_loss: 21.4134 - final_logsoft_Lev: 0.7801 - tf.nn.log_softmax_Lev: 0.7347 - val_loss: 16.6262 - val_final_logsoft_loss: 16.1479 - val_tf.nn.log_softmax_loss: 17.1043 - val_final_logsoft_Lev: 0.7727 - val_tf.nn.log_softmax_Lev: 0.7537\n","Epoch 96/200\n","891/891 [==============================] - ETA: 0s - loss: 19.3689 - final_logsoft_loss: 17.3796 - tf.nn.log_softmax_loss: 21.3580 - final_logsoft_Lev: 0.7813 - tf.nn.log_softmax_Lev: 0.7352\n","Epoch 96: val_loss did not improve from 16.50934\n","Memory usage on epoch end: 5.93 GB\n","891/891 [==============================] - 460s 514ms/step - loss: 19.3689 - final_logsoft_loss: 17.3796 - tf.nn.log_softmax_loss: 21.3580 - final_logsoft_Lev: 0.7813 - tf.nn.log_softmax_Lev: 0.7352 - val_loss: 16.5194 - val_final_logsoft_loss: 16.1527 - val_tf.nn.log_softmax_loss: 16.8865 - val_final_logsoft_Lev: 0.7726 - val_tf.nn.log_softmax_Lev: 0.7542\n","Epoch 97/200\n","891/891 [==============================] - ETA: 0s - loss: 19.4092 - final_logsoft_loss: 17.3908 - tf.nn.log_softmax_loss: 21.4273 - final_logsoft_Lev: 0.7809 - tf.nn.log_softmax_Lev: 0.7342\n","Epoch 97: val_loss improved from 16.50934 to 16.43852, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.39 GB\n","891/891 [==============================] - 460s 515ms/step - loss: 19.4092 - final_logsoft_loss: 17.3908 - tf.nn.log_softmax_loss: 21.4273 - final_logsoft_Lev: 0.7809 - tf.nn.log_softmax_Lev: 0.7342 - val_loss: 16.4385 - val_final_logsoft_loss: 15.9859 - val_tf.nn.log_softmax_loss: 16.8908 - val_final_logsoft_Lev: 0.7751 - val_tf.nn.log_softmax_Lev: 0.7558\n","Epoch 98/200\n","891/891 [==============================] - ETA: 0s - loss: 19.2704 - final_logsoft_loss: 17.2761 - tf.nn.log_softmax_loss: 21.2649 - final_logsoft_Lev: 0.7824 - tf.nn.log_softmax_Lev: 0.7364\n","Epoch 98: val_loss improved from 16.43852 to 16.38298, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.67 GB\n","891/891 [==============================] - 459s 513ms/step - loss: 19.2704 - final_logsoft_loss: 17.2761 - tf.nn.log_softmax_loss: 21.2649 - final_logsoft_Lev: 0.7824 - tf.nn.log_softmax_Lev: 0.7364 - val_loss: 16.3830 - val_final_logsoft_loss: 15.9385 - val_tf.nn.log_softmax_loss: 16.8286 - val_final_logsoft_Lev: 0.7752 - val_tf.nn.log_softmax_Lev: 0.7549\n","Epoch 99/200\n","891/891 [==============================] - ETA: 0s - loss: 19.2143 - final_logsoft_loss: 17.2199 - tf.nn.log_softmax_loss: 21.2089 - final_logsoft_Lev: 0.7830 - tf.nn.log_softmax_Lev: 0.7371\n","Epoch 99: val_loss did not improve from 16.38298\n","Memory usage on epoch end: 5.74 GB\n","891/891 [==============================] - 460s 514ms/step - loss: 19.2143 - final_logsoft_loss: 17.2199 - tf.nn.log_softmax_loss: 21.2089 - final_logsoft_Lev: 0.7830 - tf.nn.log_softmax_Lev: 0.7371 - val_loss: 16.6126 - val_final_logsoft_loss: 16.2133 - val_tf.nn.log_softmax_loss: 17.0112 - val_final_logsoft_Lev: 0.7719 - val_tf.nn.log_softmax_Lev: 0.7509\n","Epoch 100/200\n","891/891 [==============================] - ETA: 0s - loss: 19.1208 - final_logsoft_loss: 17.1030 - tf.nn.log_softmax_loss: 21.1392 - final_logsoft_Lev: 0.7845 - tf.nn.log_softmax_Lev: 0.7378\n","Epoch 100: val_loss improved from 16.38298 to 16.35071, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 6.01 GB\n","891/891 [==============================] - 461s 515ms/step - loss: 19.1208 - final_logsoft_loss: 17.1030 - tf.nn.log_softmax_loss: 21.1392 - final_logsoft_Lev: 0.7845 - tf.nn.log_softmax_Lev: 0.7378 - val_loss: 16.3507 - val_final_logsoft_loss: 15.9321 - val_tf.nn.log_softmax_loss: 16.7700 - val_final_logsoft_Lev: 0.7745 - val_tf.nn.log_softmax_Lev: 0.7551\n","Epoch 101/200\n","891/891 [==============================] - ETA: 0s - loss: 19.1007 - final_logsoft_loss: 17.0897 - tf.nn.log_softmax_loss: 21.1121 - final_logsoft_Lev: 0.7846 - tf.nn.log_softmax_Lev: 0.7381\n","Epoch 101: val_loss did not improve from 16.35071\n","Memory usage on epoch end: 5.89 GB\n","891/891 [==============================] - 461s 516ms/step - loss: 19.1007 - final_logsoft_loss: 17.0897 - tf.nn.log_softmax_loss: 21.1121 - final_logsoft_Lev: 0.7846 - tf.nn.log_softmax_Lev: 0.7381 - val_loss: 16.4051 - val_final_logsoft_loss: 15.9268 - val_tf.nn.log_softmax_loss: 16.8825 - val_final_logsoft_Lev: 0.7754 - val_tf.nn.log_softmax_Lev: 0.7551\n","Epoch 102/200\n","891/891 [==============================] - ETA: 0s - loss: 18.9949 - final_logsoft_loss: 16.9757 - tf.nn.log_softmax_loss: 21.0142 - final_logsoft_Lev: 0.7862 - tf.nn.log_softmax_Lev: 0.7392\n","Epoch 102: val_loss improved from 16.35071 to 16.32150, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.93 GB\n","891/891 [==============================] - 460s 514ms/step - loss: 18.9949 - final_logsoft_loss: 16.9757 - tf.nn.log_softmax_loss: 21.0142 - final_logsoft_Lev: 0.7862 - tf.nn.log_softmax_Lev: 0.7392 - val_loss: 16.3215 - val_final_logsoft_loss: 15.8239 - val_tf.nn.log_softmax_loss: 16.8220 - val_final_logsoft_Lev: 0.7772 - val_tf.nn.log_softmax_Lev: 0.7574\n","Epoch 103/200\n","891/891 [==============================] - ETA: 0s - loss: 18.9568 - final_logsoft_loss: 16.9269 - tf.nn.log_softmax_loss: 20.9869 - final_logsoft_Lev: 0.7865 - tf.nn.log_softmax_Lev: 0.7397\n","Epoch 103: val_loss did not improve from 16.32150\n","Memory usage on epoch end: 5.96 GB\n","891/891 [==============================] - 461s 515ms/step - loss: 18.9568 - final_logsoft_loss: 16.9269 - tf.nn.log_softmax_loss: 20.9869 - final_logsoft_Lev: 0.7865 - tf.nn.log_softmax_Lev: 0.7397 - val_loss: 16.6133 - val_final_logsoft_loss: 16.2396 - val_tf.nn.log_softmax_loss: 16.9871 - val_final_logsoft_Lev: 0.7753 - val_tf.nn.log_softmax_Lev: 0.7557\n","Epoch 104/200\n","891/891 [==============================] - ETA: 0s - loss: 18.9048 - final_logsoft_loss: 16.8811 - tf.nn.log_softmax_loss: 20.9282 - final_logsoft_Lev: 0.7874 - tf.nn.log_softmax_Lev: 0.7404\n","Epoch 104: val_loss did not improve from 16.32150\n","Memory usage on epoch end: 5.92 GB\n","891/891 [==============================] - 461s 516ms/step - loss: 18.9048 - final_logsoft_loss: 16.8811 - tf.nn.log_softmax_loss: 20.9282 - final_logsoft_Lev: 0.7874 - tf.nn.log_softmax_Lev: 0.7404 - val_loss: 16.4035 - val_final_logsoft_loss: 15.9913 - val_tf.nn.log_softmax_loss: 16.8132 - val_final_logsoft_Lev: 0.7774 - val_tf.nn.log_softmax_Lev: 0.7570\n","Epoch 105/200\n","891/891 [==============================] - ETA: 0s - loss: 18.8708 - final_logsoft_loss: 16.8486 - tf.nn.log_softmax_loss: 20.8934 - final_logsoft_Lev: 0.7876 - tf.nn.log_softmax_Lev: 0.7409\n","Epoch 105: val_loss did not improve from 16.32150\n","Memory usage on epoch end: 5.60 GB\n","891/891 [==============================] - 462s 517ms/step - loss: 18.8708 - final_logsoft_loss: 16.8486 - tf.nn.log_softmax_loss: 20.8934 - final_logsoft_Lev: 0.7876 - tf.nn.log_softmax_Lev: 0.7409 - val_loss: 16.3685 - val_final_logsoft_loss: 15.9828 - val_tf.nn.log_softmax_loss: 16.7558 - val_final_logsoft_Lev: 0.7778 - val_tf.nn.log_softmax_Lev: 0.7567\n","Epoch 106/200\n","891/891 [==============================] - ETA: 0s - loss: 18.8184 - final_logsoft_loss: 16.7885 - tf.nn.log_softmax_loss: 20.8479 - final_logsoft_Lev: 0.7885 - tf.nn.log_softmax_Lev: 0.7415\n","Epoch 106: val_loss did not improve from 16.32150\n","Memory usage on epoch end: 5.76 GB\n","891/891 [==============================] - 461s 516ms/step - loss: 18.8184 - final_logsoft_loss: 16.7885 - tf.nn.log_softmax_loss: 20.8479 - final_logsoft_Lev: 0.7885 - tf.nn.log_softmax_Lev: 0.7415 - val_loss: 16.3932 - val_final_logsoft_loss: 15.9326 - val_tf.nn.log_softmax_loss: 16.8538 - val_final_logsoft_Lev: 0.7766 - val_tf.nn.log_softmax_Lev: 0.7575\n","Epoch 107/200\n","891/891 [==============================] - ETA: 0s - loss: 18.7355 - final_logsoft_loss: 16.6949 - tf.nn.log_softmax_loss: 20.7764 - final_logsoft_Lev: 0.7896 - tf.nn.log_softmax_Lev: 0.7423\n","Epoch 107: val_loss did not improve from 16.32150\n","Memory usage on epoch end: 5.70 GB\n","891/891 [==============================] - 478s 534ms/step - loss: 18.7355 - final_logsoft_loss: 16.6949 - tf.nn.log_softmax_loss: 20.7764 - final_logsoft_Lev: 0.7896 - tf.nn.log_softmax_Lev: 0.7423 - val_loss: 16.3765 - val_final_logsoft_loss: 15.9952 - val_tf.nn.log_softmax_loss: 16.7573 - val_final_logsoft_Lev: 0.7775 - val_tf.nn.log_softmax_Lev: 0.7568\n","Epoch 108/200\n","891/891 [==============================] - ETA: 0s - loss: 18.6686 - final_logsoft_loss: 16.6215 - tf.nn.log_softmax_loss: 20.7153 - final_logsoft_Lev: 0.7904 - tf.nn.log_softmax_Lev: 0.7427\n","Epoch 108: val_loss did not improve from 16.32150\n","Memory usage on epoch end: 6.07 GB\n","891/891 [==============================] - 515s 576ms/step - loss: 18.6686 - final_logsoft_loss: 16.6215 - tf.nn.log_softmax_loss: 20.7153 - final_logsoft_Lev: 0.7904 - tf.nn.log_softmax_Lev: 0.7427 - val_loss: 16.3660 - val_final_logsoft_loss: 15.9370 - val_tf.nn.log_softmax_loss: 16.7945 - val_final_logsoft_Lev: 0.7792 - val_tf.nn.log_softmax_Lev: 0.7588\n","Epoch 109/200\n","891/891 [==============================] - ETA: 0s - loss: 18.6861 - final_logsoft_loss: 16.6521 - tf.nn.log_softmax_loss: 20.7201 - final_logsoft_Lev: 0.7900 - tf.nn.log_softmax_Lev: 0.7426\n","Epoch 109: val_loss did not improve from 16.32150\n","Memory usage on epoch end: 5.37 GB\n","891/891 [==============================] - 517s 578ms/step - loss: 18.6861 - final_logsoft_loss: 16.6521 - tf.nn.log_softmax_loss: 20.7201 - final_logsoft_Lev: 0.7900 - tf.nn.log_softmax_Lev: 0.7426 - val_loss: 16.4402 - val_final_logsoft_loss: 16.0532 - val_tf.nn.log_softmax_loss: 16.8271 - val_final_logsoft_Lev: 0.7780 - val_tf.nn.log_softmax_Lev: 0.7593\n","Epoch 110/200\n","891/891 [==============================] - ETA: 0s - loss: 18.5476 - final_logsoft_loss: 16.5042 - tf.nn.log_softmax_loss: 20.5910 - final_logsoft_Lev: 0.7917 - tf.nn.log_softmax_Lev: 0.7443\n","Epoch 110: val_loss did not improve from 16.32150\n","Memory usage on epoch end: 5.98 GB\n","891/891 [==============================] - 515s 575ms/step - loss: 18.5476 - final_logsoft_loss: 16.5042 - tf.nn.log_softmax_loss: 20.5910 - final_logsoft_Lev: 0.7917 - tf.nn.log_softmax_Lev: 0.7443 - val_loss: 16.4193 - val_final_logsoft_loss: 16.0683 - val_tf.nn.log_softmax_loss: 16.7729 - val_final_logsoft_Lev: 0.7782 - val_tf.nn.log_softmax_Lev: 0.7582\n","Epoch 111/200\n","891/891 [==============================] - ETA: 0s - loss: 18.5163 - final_logsoft_loss: 16.4514 - tf.nn.log_softmax_loss: 20.5813 - final_logsoft_Lev: 0.7926 - tf.nn.log_softmax_Lev: 0.7447\n","Epoch 111: val_loss did not improve from 16.32150\n","Memory usage on epoch end: 5.45 GB\n","891/891 [==============================] - 514s 574ms/step - loss: 18.5163 - final_logsoft_loss: 16.4514 - tf.nn.log_softmax_loss: 20.5813 - final_logsoft_Lev: 0.7926 - tf.nn.log_softmax_Lev: 0.7447 - val_loss: 16.4102 - val_final_logsoft_loss: 16.0219 - val_tf.nn.log_softmax_loss: 16.7986 - val_final_logsoft_Lev: 0.7773 - val_tf.nn.log_softmax_Lev: 0.7585\n","Epoch 112/200\n","891/891 [==============================] - ETA: 0s - loss: 18.4595 - final_logsoft_loss: 16.3849 - tf.nn.log_softmax_loss: 20.5344 - final_logsoft_Lev: 0.7933 - tf.nn.log_softmax_Lev: 0.7450\n","Epoch 112: val_loss did not improve from 16.32150\n","Memory usage on epoch end: 6.09 GB\n","891/891 [==============================] - 507s 566ms/step - loss: 18.4595 - final_logsoft_loss: 16.3849 - tf.nn.log_softmax_loss: 20.5344 - final_logsoft_Lev: 0.7933 - tf.nn.log_softmax_Lev: 0.7450 - val_loss: 16.4232 - val_final_logsoft_loss: 16.1138 - val_tf.nn.log_softmax_loss: 16.7334 - val_final_logsoft_Lev: 0.7788 - val_tf.nn.log_softmax_Lev: 0.7577\n","Epoch 113/200\n","891/891 [==============================] - ETA: 0s - loss: 18.4002 - final_logsoft_loss: 16.3329 - tf.nn.log_softmax_loss: 20.4672 - final_logsoft_Lev: 0.7938 - tf.nn.log_softmax_Lev: 0.7456\n","Epoch 113: val_loss did not improve from 16.32150\n","Memory usage on epoch end: 5.54 GB\n","891/891 [==============================] - 503s 562ms/step - loss: 18.4002 - final_logsoft_loss: 16.3329 - tf.nn.log_softmax_loss: 20.4672 - final_logsoft_Lev: 0.7938 - tf.nn.log_softmax_Lev: 0.7456 - val_loss: 16.3246 - val_final_logsoft_loss: 15.9720 - val_tf.nn.log_softmax_loss: 16.6788 - val_final_logsoft_Lev: 0.7799 - val_tf.nn.log_softmax_Lev: 0.7604\n","Epoch 114/200\n","891/891 [==============================] - ETA: 0s - loss: 18.3496 - final_logsoft_loss: 16.2933 - tf.nn.log_softmax_loss: 20.4055 - final_logsoft_Lev: 0.7945 - tf.nn.log_softmax_Lev: 0.7463\n","Epoch 114: val_loss did not improve from 16.32150\n","Memory usage on epoch end: 6.11 GB\n","891/891 [==============================] - 502s 561ms/step - loss: 18.3496 - final_logsoft_loss: 16.2933 - tf.nn.log_softmax_loss: 20.4055 - final_logsoft_Lev: 0.7945 - tf.nn.log_softmax_Lev: 0.7463 - val_loss: 16.3575 - val_final_logsoft_loss: 16.0205 - val_tf.nn.log_softmax_loss: 16.6945 - val_final_logsoft_Lev: 0.7806 - val_tf.nn.log_softmax_Lev: 0.7605\n","Epoch 115/200\n","891/891 [==============================] - ETA: 0s - loss: 18.3414 - final_logsoft_loss: 16.2788 - tf.nn.log_softmax_loss: 20.4042 - final_logsoft_Lev: 0.7948 - tf.nn.log_softmax_Lev: 0.7468\n","Epoch 115: val_loss did not improve from 16.32150\n","Memory usage on epoch end: 5.66 GB\n","891/891 [==============================] - 501s 559ms/step - loss: 18.3414 - final_logsoft_loss: 16.2788 - tf.nn.log_softmax_loss: 20.4042 - final_logsoft_Lev: 0.7948 - tf.nn.log_softmax_Lev: 0.7468 - val_loss: 16.3993 - val_final_logsoft_loss: 16.1381 - val_tf.nn.log_softmax_loss: 16.6608 - val_final_logsoft_Lev: 0.7793 - val_tf.nn.log_softmax_Lev: 0.7596\n","Epoch 116/200\n","891/891 [==============================] - ETA: 0s - loss: 18.2484 - final_logsoft_loss: 16.1879 - tf.nn.log_softmax_loss: 20.3089 - final_logsoft_Lev: 0.7956 - tf.nn.log_softmax_Lev: 0.7474\n","Epoch 116: val_loss did not improve from 16.32150\n","Memory usage on epoch end: 6.10 GB\n","891/891 [==============================] - 503s 563ms/step - loss: 18.2484 - final_logsoft_loss: 16.1879 - tf.nn.log_softmax_loss: 20.3089 - final_logsoft_Lev: 0.7956 - tf.nn.log_softmax_Lev: 0.7474 - val_loss: 16.3857 - val_final_logsoft_loss: 16.0299 - val_tf.nn.log_softmax_loss: 16.7405 - val_final_logsoft_Lev: 0.7790 - val_tf.nn.log_softmax_Lev: 0.7595\n","Epoch 117/200\n","891/891 [==============================] - ETA: 0s - loss: 18.1937 - final_logsoft_loss: 16.1184 - tf.nn.log_softmax_loss: 20.2690 - final_logsoft_Lev: 0.7963 - tf.nn.log_softmax_Lev: 0.7478\n","Epoch 117: val_loss improved from 16.32150 to 16.29399, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.43 GB\n","891/891 [==============================] - 501s 560ms/step - loss: 18.1937 - final_logsoft_loss: 16.1184 - tf.nn.log_softmax_loss: 20.2690 - final_logsoft_Lev: 0.7963 - tf.nn.log_softmax_Lev: 0.7478 - val_loss: 16.2940 - val_final_logsoft_loss: 15.8635 - val_tf.nn.log_softmax_loss: 16.7247 - val_final_logsoft_Lev: 0.7785 - val_tf.nn.log_softmax_Lev: 0.7584\n","Epoch 118/200\n","891/891 [==============================] - ETA: 0s - loss: 18.1612 - final_logsoft_loss: 16.0913 - tf.nn.log_softmax_loss: 20.2306 - final_logsoft_Lev: 0.7967 - tf.nn.log_softmax_Lev: 0.7485\n","Epoch 118: val_loss did not improve from 16.29399\n","Memory usage on epoch end: 6.09 GB\n","891/891 [==============================] - 500s 559ms/step - loss: 18.1612 - final_logsoft_loss: 16.0913 - tf.nn.log_softmax_loss: 20.2306 - final_logsoft_Lev: 0.7967 - tf.nn.log_softmax_Lev: 0.7485 - val_loss: 16.3162 - val_final_logsoft_loss: 15.8601 - val_tf.nn.log_softmax_loss: 16.7716 - val_final_logsoft_Lev: 0.7792 - val_tf.nn.log_softmax_Lev: 0.7603\n","Epoch 119/200\n","891/891 [==============================] - ETA: 0s - loss: 18.1136 - final_logsoft_loss: 16.0293 - tf.nn.log_softmax_loss: 20.1982 - final_logsoft_Lev: 0.7975 - tf.nn.log_softmax_Lev: 0.7489\n","Epoch 119: val_loss improved from 16.29399 to 16.18750, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.43 GB\n","891/891 [==============================] - 498s 557ms/step - loss: 18.1136 - final_logsoft_loss: 16.0293 - tf.nn.log_softmax_loss: 20.1982 - final_logsoft_Lev: 0.7975 - tf.nn.log_softmax_Lev: 0.7489 - val_loss: 16.1875 - val_final_logsoft_loss: 15.8407 - val_tf.nn.log_softmax_loss: 16.5353 - val_final_logsoft_Lev: 0.7812 - val_tf.nn.log_softmax_Lev: 0.7613\n","Epoch 120/200\n","891/891 [==============================] - ETA: 0s - loss: 18.0565 - final_logsoft_loss: 15.9652 - tf.nn.log_softmax_loss: 20.1474 - final_logsoft_Lev: 0.7985 - tf.nn.log_softmax_Lev: 0.7495\n","Epoch 120: val_loss did not improve from 16.18750\n","Memory usage on epoch end: 6.01 GB\n","891/891 [==============================] - 498s 557ms/step - loss: 18.0565 - final_logsoft_loss: 15.9652 - tf.nn.log_softmax_loss: 20.1474 - final_logsoft_Lev: 0.7985 - tf.nn.log_softmax_Lev: 0.7495 - val_loss: 16.2311 - val_final_logsoft_loss: 15.8164 - val_tf.nn.log_softmax_loss: 16.6481 - val_final_logsoft_Lev: 0.7803 - val_tf.nn.log_softmax_Lev: 0.7600\n","Epoch 121/200\n","891/891 [==============================] - ETA: 0s - loss: 18.0309 - final_logsoft_loss: 15.9569 - tf.nn.log_softmax_loss: 20.1052 - final_logsoft_Lev: 0.7987 - tf.nn.log_softmax_Lev: 0.7501\n","Epoch 121: val_loss did not improve from 16.18750\n","Memory usage on epoch end: 5.40 GB\n","891/891 [==============================] - 505s 564ms/step - loss: 18.0309 - final_logsoft_loss: 15.9569 - tf.nn.log_softmax_loss: 20.1052 - final_logsoft_Lev: 0.7987 - tf.nn.log_softmax_Lev: 0.7501 - val_loss: 16.2023 - val_final_logsoft_loss: 15.8781 - val_tf.nn.log_softmax_loss: 16.5260 - val_final_logsoft_Lev: 0.7825 - val_tf.nn.log_softmax_Lev: 0.7618\n","Epoch 122/200\n","891/891 [==============================] - ETA: 0s - loss: 17.9716 - final_logsoft_loss: 15.8941 - tf.nn.log_softmax_loss: 20.0494 - final_logsoft_Lev: 0.7991 - tf.nn.log_softmax_Lev: 0.7505\n","Epoch 122: val_loss did not improve from 16.18750\n","Memory usage on epoch end: 5.77 GB\n","891/891 [==============================] - 499s 558ms/step - loss: 17.9716 - final_logsoft_loss: 15.8941 - tf.nn.log_softmax_loss: 20.0494 - final_logsoft_Lev: 0.7991 - tf.nn.log_softmax_Lev: 0.7505 - val_loss: 16.2216 - val_final_logsoft_loss: 15.8899 - val_tf.nn.log_softmax_loss: 16.5528 - val_final_logsoft_Lev: 0.7812 - val_tf.nn.log_softmax_Lev: 0.7610\n","Epoch 123/200\n","891/891 [==============================] - ETA: 0s - loss: 17.8945 - final_logsoft_loss: 15.7903 - tf.nn.log_softmax_loss: 19.9986 - final_logsoft_Lev: 0.8003 - tf.nn.log_softmax_Lev: 0.7509\n","Epoch 123: val_loss did not improve from 16.18750\n","Memory usage on epoch end: 4.98 GB\n","891/891 [==============================] - 499s 557ms/step - loss: 17.8945 - final_logsoft_loss: 15.7903 - tf.nn.log_softmax_loss: 19.9986 - final_logsoft_Lev: 0.8003 - tf.nn.log_softmax_Lev: 0.7509 - val_loss: 16.2525 - val_final_logsoft_loss: 15.9234 - val_tf.nn.log_softmax_loss: 16.5814 - val_final_logsoft_Lev: 0.7806 - val_tf.nn.log_softmax_Lev: 0.7611\n","Epoch 124/200\n","891/891 [==============================] - ETA: 0s - loss: 17.8265 - final_logsoft_loss: 15.7162 - tf.nn.log_softmax_loss: 19.9367 - final_logsoft_Lev: 0.8013 - tf.nn.log_softmax_Lev: 0.7518\n","Epoch 124: val_loss did not improve from 16.18750\n","Memory usage on epoch end: 5.75 GB\n","891/891 [==============================] - 504s 563ms/step - loss: 17.8265 - final_logsoft_loss: 15.7162 - tf.nn.log_softmax_loss: 19.9367 - final_logsoft_Lev: 0.8013 - tf.nn.log_softmax_Lev: 0.7518 - val_loss: 16.3358 - val_final_logsoft_loss: 16.0226 - val_tf.nn.log_softmax_loss: 16.6483 - val_final_logsoft_Lev: 0.7823 - val_tf.nn.log_softmax_Lev: 0.7621\n","Epoch 125/200\n","891/891 [==============================] - ETA: 0s - loss: 17.8102 - final_logsoft_loss: 15.7189 - tf.nn.log_softmax_loss: 19.9018 - final_logsoft_Lev: 0.8013 - tf.nn.log_softmax_Lev: 0.7522\n","Epoch 125: val_loss did not improve from 16.18750\n","Memory usage on epoch end: 5.03 GB\n","891/891 [==============================] - 505s 562ms/step - loss: 17.8102 - final_logsoft_loss: 15.7189 - tf.nn.log_softmax_loss: 19.9018 - final_logsoft_Lev: 0.8013 - tf.nn.log_softmax_Lev: 0.7522 - val_loss: 16.2148 - val_final_logsoft_loss: 15.8813 - val_tf.nn.log_softmax_loss: 16.5508 - val_final_logsoft_Lev: 0.7819 - val_tf.nn.log_softmax_Lev: 0.7626\n","Epoch 126/200\n","891/891 [==============================] - ETA: 0s - loss: 17.7614 - final_logsoft_loss: 15.6580 - tf.nn.log_softmax_loss: 19.8646 - final_logsoft_Lev: 0.8021 - tf.nn.log_softmax_Lev: 0.7523\n","Epoch 126: val_loss did not improve from 16.18750\n","Memory usage on epoch end: 5.73 GB\n","891/891 [==============================] - 502s 561ms/step - loss: 17.7614 - final_logsoft_loss: 15.6580 - tf.nn.log_softmax_loss: 19.8646 - final_logsoft_Lev: 0.8021 - tf.nn.log_softmax_Lev: 0.7523 - val_loss: 16.2906 - val_final_logsoft_loss: 16.0093 - val_tf.nn.log_softmax_loss: 16.5717 - val_final_logsoft_Lev: 0.7828 - val_tf.nn.log_softmax_Lev: 0.7633\n","Epoch 127/200\n","891/891 [==============================] - ETA: 0s - loss: 17.7054 - final_logsoft_loss: 15.6123 - tf.nn.log_softmax_loss: 19.7986 - final_logsoft_Lev: 0.8027 - tf.nn.log_softmax_Lev: 0.7533\n","Epoch 127: val_loss improved from 16.18750 to 16.17069, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 5.07 GB\n","891/891 [==============================] - 504s 563ms/step - loss: 17.7054 - final_logsoft_loss: 15.6123 - tf.nn.log_softmax_loss: 19.7986 - final_logsoft_Lev: 0.8027 - tf.nn.log_softmax_Lev: 0.7533 - val_loss: 16.1707 - val_final_logsoft_loss: 15.8877 - val_tf.nn.log_softmax_loss: 16.4552 - val_final_logsoft_Lev: 0.7815 - val_tf.nn.log_softmax_Lev: 0.7628\n","Epoch 128/200\n","891/891 [==============================] - ETA: 0s - loss: 17.6548 - final_logsoft_loss: 15.5433 - tf.nn.log_softmax_loss: 19.7663 - final_logsoft_Lev: 0.8032 - tf.nn.log_softmax_Lev: 0.7536\n","Epoch 128: val_loss did not improve from 16.17069\n","Memory usage on epoch end: 5.55 GB\n","891/891 [==============================] - 504s 563ms/step - loss: 17.6548 - final_logsoft_loss: 15.5433 - tf.nn.log_softmax_loss: 19.7663 - final_logsoft_Lev: 0.8032 - tf.nn.log_softmax_Lev: 0.7536 - val_loss: 16.2094 - val_final_logsoft_loss: 15.9631 - val_tf.nn.log_softmax_loss: 16.4555 - val_final_logsoft_Lev: 0.7821 - val_tf.nn.log_softmax_Lev: 0.7620\n","Epoch 129/200\n","891/891 [==============================] - ETA: 0s - loss: 17.6310 - final_logsoft_loss: 15.5242 - tf.nn.log_softmax_loss: 19.7377 - final_logsoft_Lev: 0.8034 - tf.nn.log_softmax_Lev: 0.7540\n","Epoch 129: val_loss improved from 16.17069 to 16.09035, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 4.44 GB\n","891/891 [==============================] - 501s 560ms/step - loss: 17.6310 - final_logsoft_loss: 15.5242 - tf.nn.log_softmax_loss: 19.7377 - final_logsoft_Lev: 0.8034 - tf.nn.log_softmax_Lev: 0.7540 - val_loss: 16.0904 - val_final_logsoft_loss: 15.7111 - val_tf.nn.log_softmax_loss: 16.4713 - val_final_logsoft_Lev: 0.7816 - val_tf.nn.log_softmax_Lev: 0.7625\n","Epoch 130/200\n","891/891 [==============================] - ETA: 0s - loss: 17.6072 - final_logsoft_loss: 15.4824 - tf.nn.log_softmax_loss: 19.7326 - final_logsoft_Lev: 0.8038 - tf.nn.log_softmax_Lev: 0.7541\n","Epoch 130: val_loss did not improve from 16.09035\n","Memory usage on epoch end: 5.13 GB\n","891/891 [==============================] - 503s 562ms/step - loss: 17.6072 - final_logsoft_loss: 15.4824 - tf.nn.log_softmax_loss: 19.7326 - final_logsoft_Lev: 0.8038 - tf.nn.log_softmax_Lev: 0.7541 - val_loss: 16.1228 - val_final_logsoft_loss: 15.7418 - val_tf.nn.log_softmax_loss: 16.5025 - val_final_logsoft_Lev: 0.7834 - val_tf.nn.log_softmax_Lev: 0.7638\n","Epoch 131/200\n","891/891 [==============================] - ETA: 0s - loss: 17.5525 - final_logsoft_loss: 15.4393 - tf.nn.log_softmax_loss: 19.6657 - final_logsoft_Lev: 0.8049 - tf.nn.log_softmax_Lev: 0.7547\n","Epoch 131: val_loss did not improve from 16.09035\n","Memory usage on epoch end: 4.20 GB\n","891/891 [==============================] - 503s 563ms/step - loss: 17.5525 - final_logsoft_loss: 15.4393 - tf.nn.log_softmax_loss: 19.6657 - final_logsoft_Lev: 0.8049 - tf.nn.log_softmax_Lev: 0.7547 - val_loss: 16.4278 - val_final_logsoft_loss: 16.1471 - val_tf.nn.log_softmax_loss: 16.7060 - val_final_logsoft_Lev: 0.7837 - val_tf.nn.log_softmax_Lev: 0.7644\n","Epoch 132/200\n","891/891 [==============================] - ETA: 0s - loss: 17.5365 - final_logsoft_loss: 15.4130 - tf.nn.log_softmax_loss: 19.6599 - final_logsoft_Lev: 0.8050 - tf.nn.log_softmax_Lev: 0.7549\n","Epoch 132: val_loss did not improve from 16.09035\n","Memory usage on epoch end: 5.02 GB\n","891/891 [==============================] - 501s 560ms/step - loss: 17.5365 - final_logsoft_loss: 15.4130 - tf.nn.log_softmax_loss: 19.6599 - final_logsoft_Lev: 0.8050 - tf.nn.log_softmax_Lev: 0.7549 - val_loss: 16.2249 - val_final_logsoft_loss: 15.9372 - val_tf.nn.log_softmax_loss: 16.5121 - val_final_logsoft_Lev: 0.7836 - val_tf.nn.log_softmax_Lev: 0.7639\n","Epoch 133/200\n","891/891 [==============================] - ETA: 0s - loss: 17.4945 - final_logsoft_loss: 15.3762 - tf.nn.log_softmax_loss: 19.6130 - final_logsoft_Lev: 0.8056 - tf.nn.log_softmax_Lev: 0.7555\n","Epoch 133: val_loss did not improve from 16.09035\n","Memory usage on epoch end: 4.55 GB\n","891/891 [==============================] - 502s 561ms/step - loss: 17.4945 - final_logsoft_loss: 15.3762 - tf.nn.log_softmax_loss: 19.6130 - final_logsoft_Lev: 0.8056 - tf.nn.log_softmax_Lev: 0.7555 - val_loss: 16.1928 - val_final_logsoft_loss: 15.9818 - val_tf.nn.log_softmax_loss: 16.4037 - val_final_logsoft_Lev: 0.7833 - val_tf.nn.log_softmax_Lev: 0.7629\n","Epoch 134/200\n","891/891 [==============================] - ETA: 0s - loss: 17.4265 - final_logsoft_loss: 15.2977 - tf.nn.log_softmax_loss: 19.5548 - final_logsoft_Lev: 0.8066 - tf.nn.log_softmax_Lev: 0.7564\n","Epoch 134: val_loss did not improve from 16.09035\n","Memory usage on epoch end: 5.09 GB\n","891/891 [==============================] - 505s 565ms/step - loss: 17.4265 - final_logsoft_loss: 15.2977 - tf.nn.log_softmax_loss: 19.5548 - final_logsoft_Lev: 0.8066 - tf.nn.log_softmax_Lev: 0.7564 - val_loss: 16.3273 - val_final_logsoft_loss: 15.9778 - val_tf.nn.log_softmax_loss: 16.6776 - val_final_logsoft_Lev: 0.7834 - val_tf.nn.log_softmax_Lev: 0.7632\n","Epoch 135/200\n","891/891 [==============================] - ETA: 0s - loss: 17.4047 - final_logsoft_loss: 15.2860 - tf.nn.log_softmax_loss: 19.5232 - final_logsoft_Lev: 0.8066 - tf.nn.log_softmax_Lev: 0.7565\n","Epoch 135: val_loss did not improve from 16.09035\n","Memory usage on epoch end: 4.84 GB\n","891/891 [==============================] - 520s 582ms/step - loss: 17.4047 - final_logsoft_loss: 15.2860 - tf.nn.log_softmax_loss: 19.5232 - final_logsoft_Lev: 0.8066 - tf.nn.log_softmax_Lev: 0.7565 - val_loss: 16.1975 - val_final_logsoft_loss: 15.8536 - val_tf.nn.log_softmax_loss: 16.5421 - val_final_logsoft_Lev: 0.7835 - val_tf.nn.log_softmax_Lev: 0.7637\n","Epoch 136/200\n","891/891 [==============================] - ETA: 0s - loss: 17.3568 - final_logsoft_loss: 15.2291 - tf.nn.log_softmax_loss: 19.4840 - final_logsoft_Lev: 0.8073 - tf.nn.log_softmax_Lev: 0.7571\n","Epoch 136: val_loss did not improve from 16.09035\n","Memory usage on epoch end: 5.01 GB\n","891/891 [==============================] - 505s 564ms/step - loss: 17.3568 - final_logsoft_loss: 15.2291 - tf.nn.log_softmax_loss: 19.4840 - final_logsoft_Lev: 0.8073 - tf.nn.log_softmax_Lev: 0.7571 - val_loss: 16.2379 - val_final_logsoft_loss: 15.9229 - val_tf.nn.log_softmax_loss: 16.5515 - val_final_logsoft_Lev: 0.7847 - val_tf.nn.log_softmax_Lev: 0.7640\n","Epoch 137/200\n","891/891 [==============================] - ETA: 0s - loss: 17.2810 - final_logsoft_loss: 15.1706 - tf.nn.log_softmax_loss: 19.3916 - final_logsoft_Lev: 0.8080 - tf.nn.log_softmax_Lev: 0.7583\n","Epoch 137: val_loss did not improve from 16.09035\n","Memory usage on epoch end: 5.05 GB\n","891/891 [==============================] - 502s 561ms/step - loss: 17.2810 - final_logsoft_loss: 15.1706 - tf.nn.log_softmax_loss: 19.3916 - final_logsoft_Lev: 0.8080 - tf.nn.log_softmax_Lev: 0.7583 - val_loss: 16.2300 - val_final_logsoft_loss: 15.9560 - val_tf.nn.log_softmax_loss: 16.5037 - val_final_logsoft_Lev: 0.7843 - val_tf.nn.log_softmax_Lev: 0.7639\n","Epoch 138/200\n","891/891 [==============================] - ETA: 0s - loss: 17.2745 - final_logsoft_loss: 15.1389 - tf.nn.log_softmax_loss: 19.4100 - final_logsoft_Lev: 0.8084 - tf.nn.log_softmax_Lev: 0.7579\n","Epoch 138: val_loss did not improve from 16.09035\n","Memory usage on epoch end: 5.36 GB\n","891/891 [==============================] - 499s 558ms/step - loss: 17.2745 - final_logsoft_loss: 15.1389 - tf.nn.log_softmax_loss: 19.4100 - final_logsoft_Lev: 0.8084 - tf.nn.log_softmax_Lev: 0.7579 - val_loss: 16.1695 - val_final_logsoft_loss: 15.8653 - val_tf.nn.log_softmax_loss: 16.4723 - val_final_logsoft_Lev: 0.7832 - val_tf.nn.log_softmax_Lev: 0.7635\n","Epoch 139/200\n","891/891 [==============================] - ETA: 0s - loss: 17.1978 - final_logsoft_loss: 15.0700 - tf.nn.log_softmax_loss: 19.3258 - final_logsoft_Lev: 0.8093 - tf.nn.log_softmax_Lev: 0.7589\n","Epoch 139: val_loss did not improve from 16.09035\n","Memory usage on epoch end: 4.40 GB\n","891/891 [==============================] - 500s 559ms/step - loss: 17.1978 - final_logsoft_loss: 15.0700 - tf.nn.log_softmax_loss: 19.3258 - final_logsoft_Lev: 0.8093 - tf.nn.log_softmax_Lev: 0.7589 - val_loss: 16.1566 - val_final_logsoft_loss: 15.8190 - val_tf.nn.log_softmax_loss: 16.4947 - val_final_logsoft_Lev: 0.7847 - val_tf.nn.log_softmax_Lev: 0.7652\n","Epoch 140/200\n","891/891 [==============================] - ETA: 0s - loss: 17.2376 - final_logsoft_loss: 15.1138 - tf.nn.log_softmax_loss: 19.3615 - final_logsoft_Lev: 0.8088 - tf.nn.log_softmax_Lev: 0.7583\n","Epoch 140: val_loss did not improve from 16.09035\n","Memory usage on epoch end: 5.17 GB\n","891/891 [==============================] - 502s 561ms/step - loss: 17.2376 - final_logsoft_loss: 15.1138 - tf.nn.log_softmax_loss: 19.3615 - final_logsoft_Lev: 0.8088 - tf.nn.log_softmax_Lev: 0.7583 - val_loss: 16.2775 - val_final_logsoft_loss: 16.0375 - val_tf.nn.log_softmax_loss: 16.5187 - val_final_logsoft_Lev: 0.7847 - val_tf.nn.log_softmax_Lev: 0.7652\n","Epoch 141/200\n","891/891 [==============================] - ETA: 0s - loss: 17.1286 - final_logsoft_loss: 15.0114 - tf.nn.log_softmax_loss: 19.2454 - final_logsoft_Lev: 0.8098 - tf.nn.log_softmax_Lev: 0.7597\n","Epoch 141: val_loss did not improve from 16.09035\n","Memory usage on epoch end: 4.52 GB\n","891/891 [==============================] - 502s 562ms/step - loss: 17.1286 - final_logsoft_loss: 15.0114 - tf.nn.log_softmax_loss: 19.2454 - final_logsoft_Lev: 0.8098 - tf.nn.log_softmax_Lev: 0.7597 - val_loss: 16.3225 - val_final_logsoft_loss: 16.1102 - val_tf.nn.log_softmax_loss: 16.5340 - val_final_logsoft_Lev: 0.7847 - val_tf.nn.log_softmax_Lev: 0.7650\n","Epoch 142/200\n","891/891 [==============================] - ETA: 0s - loss: 17.1212 - final_logsoft_loss: 14.9904 - tf.nn.log_softmax_loss: 19.2524 - final_logsoft_Lev: 0.8104 - tf.nn.log_softmax_Lev: 0.7596\n","Epoch 142: val_loss did not improve from 16.09035\n","Memory usage on epoch end: 5.17 GB\n","891/891 [==============================] - 501s 560ms/step - loss: 17.1212 - final_logsoft_loss: 14.9904 - tf.nn.log_softmax_loss: 19.2524 - final_logsoft_Lev: 0.8104 - tf.nn.log_softmax_Lev: 0.7596 - val_loss: 16.3711 - val_final_logsoft_loss: 16.1248 - val_tf.nn.log_softmax_loss: 16.6165 - val_final_logsoft_Lev: 0.7843 - val_tf.nn.log_softmax_Lev: 0.7654\n","Epoch 143/200\n","891/891 [==============================] - ETA: 0s - loss: 17.0948 - final_logsoft_loss: 14.9636 - tf.nn.log_softmax_loss: 19.2265 - final_logsoft_Lev: 0.8104 - tf.nn.log_softmax_Lev: 0.7599\n","Epoch 143: val_loss did not improve from 16.09035\n","Memory usage on epoch end: 4.46 GB\n","891/891 [==============================] - 493s 548ms/step - loss: 17.0948 - final_logsoft_loss: 14.9636 - tf.nn.log_softmax_loss: 19.2265 - final_logsoft_Lev: 0.8104 - tf.nn.log_softmax_Lev: 0.7599 - val_loss: 16.2522 - val_final_logsoft_loss: 15.9859 - val_tf.nn.log_softmax_loss: 16.5185 - val_final_logsoft_Lev: 0.7841 - val_tf.nn.log_softmax_Lev: 0.7654\n","Epoch 144/200\n","891/891 [==============================] - ETA: 0s - loss: 17.0560 - final_logsoft_loss: 14.9327 - tf.nn.log_softmax_loss: 19.1795 - final_logsoft_Lev: 0.8109 - tf.nn.log_softmax_Lev: 0.7609\n","Epoch 144: val_loss did not improve from 16.09035\n","Memory usage on epoch end: 5.55 GB\n","891/891 [==============================] - 491s 549ms/step - loss: 17.0560 - final_logsoft_loss: 14.9327 - tf.nn.log_softmax_loss: 19.1795 - final_logsoft_Lev: 0.8109 - tf.nn.log_softmax_Lev: 0.7609 - val_loss: 16.1923 - val_final_logsoft_loss: 15.9090 - val_tf.nn.log_softmax_loss: 16.4750 - val_final_logsoft_Lev: 0.7853 - val_tf.nn.log_softmax_Lev: 0.7658\n","Epoch 145/200\n","891/891 [==============================] - ETA: 0s - loss: 16.9837 - final_logsoft_loss: 14.8345 - tf.nn.log_softmax_loss: 19.1324 - final_logsoft_Lev: 0.8122 - tf.nn.log_softmax_Lev: 0.7612\n","Epoch 145: val_loss improved from 16.09035 to 16.08832, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 4.61 GB\n","891/891 [==============================] - 498s 556ms/step - loss: 16.9837 - final_logsoft_loss: 14.8345 - tf.nn.log_softmax_loss: 19.1324 - final_logsoft_Lev: 0.8122 - tf.nn.log_softmax_Lev: 0.7612 - val_loss: 16.0883 - val_final_logsoft_loss: 15.7765 - val_tf.nn.log_softmax_loss: 16.4017 - val_final_logsoft_Lev: 0.7854 - val_tf.nn.log_softmax_Lev: 0.7659\n","Epoch 146/200\n","891/891 [==============================] - ETA: 0s - loss: 16.9714 - final_logsoft_loss: 14.8403 - tf.nn.log_softmax_loss: 19.1024 - final_logsoft_Lev: 0.8119 - tf.nn.log_softmax_Lev: 0.7613\n","Epoch 146: val_loss did not improve from 16.08832\n","Memory usage on epoch end: 5.65 GB\n","891/891 [==============================] - 489s 547ms/step - loss: 16.9714 - final_logsoft_loss: 14.8403 - tf.nn.log_softmax_loss: 19.1024 - final_logsoft_Lev: 0.8119 - tf.nn.log_softmax_Lev: 0.7613 - val_loss: 16.2821 - val_final_logsoft_loss: 16.1084 - val_tf.nn.log_softmax_loss: 16.4570 - val_final_logsoft_Lev: 0.7848 - val_tf.nn.log_softmax_Lev: 0.7658\n","Epoch 147/200\n","891/891 [==============================] - ETA: 0s - loss: 16.9274 - final_logsoft_loss: 14.7877 - tf.nn.log_softmax_loss: 19.0668 - final_logsoft_Lev: 0.8126 - tf.nn.log_softmax_Lev: 0.7616\n","Epoch 147: val_loss improved from 16.08832 to 16.04042, saving model to /kaggle/working/model-384-seed42-exp0-best.h5\n","Memory usage on epoch end: 4.98 GB\n","891/891 [==============================] - 463s 517ms/step - loss: 16.9274 - final_logsoft_loss: 14.7877 - tf.nn.log_softmax_loss: 19.0668 - final_logsoft_Lev: 0.8126 - tf.nn.log_softmax_Lev: 0.7616 - val_loss: 16.0404 - val_final_logsoft_loss: 15.7313 - val_tf.nn.log_softmax_loss: 16.3480 - val_final_logsoft_Lev: 0.7857 - val_tf.nn.log_softmax_Lev: 0.7662\n","Epoch 148/200\n","891/891 [==============================] - ETA: 0s - loss: 16.8532 - final_logsoft_loss: 14.7053 - tf.nn.log_softmax_loss: 19.0011 - final_logsoft_Lev: 0.8137 - tf.nn.log_softmax_Lev: 0.7625\n","Epoch 148: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.50 GB\n","891/891 [==============================] - 463s 517ms/step - loss: 16.8532 - final_logsoft_loss: 14.7053 - tf.nn.log_softmax_loss: 19.0011 - final_logsoft_Lev: 0.8137 - tf.nn.log_softmax_Lev: 0.7625 - val_loss: 16.2418 - val_final_logsoft_loss: 16.0469 - val_tf.nn.log_softmax_loss: 16.4365 - val_final_logsoft_Lev: 0.7862 - val_tf.nn.log_softmax_Lev: 0.7672\n","Epoch 149/200\n","891/891 [==============================] - ETA: 0s - loss: 16.8840 - final_logsoft_loss: 14.7510 - tf.nn.log_softmax_loss: 19.0171 - final_logsoft_Lev: 0.8130 - tf.nn.log_softmax_Lev: 0.7624\n","Epoch 149: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.82 GB\n","891/891 [==============================] - 463s 516ms/step - loss: 16.8840 - final_logsoft_loss: 14.7510 - tf.nn.log_softmax_loss: 19.0171 - final_logsoft_Lev: 0.8130 - tf.nn.log_softmax_Lev: 0.7624 - val_loss: 16.1883 - val_final_logsoft_loss: 15.9319 - val_tf.nn.log_softmax_loss: 16.4448 - val_final_logsoft_Lev: 0.7860 - val_tf.nn.log_softmax_Lev: 0.7666\n","Epoch 150/200\n","891/891 [==============================] - ETA: 0s - loss: 16.8272 - final_logsoft_loss: 14.6873 - tf.nn.log_softmax_loss: 18.9669 - final_logsoft_Lev: 0.8139 - tf.nn.log_softmax_Lev: 0.7631\n","Epoch 150: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.49 GB\n","891/891 [==============================] - 464s 518ms/step - loss: 16.8272 - final_logsoft_loss: 14.6873 - tf.nn.log_softmax_loss: 18.9669 - final_logsoft_Lev: 0.8139 - tf.nn.log_softmax_Lev: 0.7631 - val_loss: 16.1627 - val_final_logsoft_loss: 15.9305 - val_tf.nn.log_softmax_loss: 16.3942 - val_final_logsoft_Lev: 0.7856 - val_tf.nn.log_softmax_Lev: 0.7665\n","Epoch 151/200\n","891/891 [==============================] - ETA: 0s - loss: 16.7931 - final_logsoft_loss: 14.6494 - tf.nn.log_softmax_loss: 18.9367 - final_logsoft_Lev: 0.8145 - tf.nn.log_softmax_Lev: 0.7634\n","Epoch 151: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.64 GB\n","891/891 [==============================] - 462s 516ms/step - loss: 16.7931 - final_logsoft_loss: 14.6494 - tf.nn.log_softmax_loss: 18.9367 - final_logsoft_Lev: 0.8145 - tf.nn.log_softmax_Lev: 0.7634 - val_loss: 16.2368 - val_final_logsoft_loss: 16.0234 - val_tf.nn.log_softmax_loss: 16.4487 - val_final_logsoft_Lev: 0.7868 - val_tf.nn.log_softmax_Lev: 0.7669\n","Epoch 152/200\n","891/891 [==============================] - ETA: 0s - loss: 16.7926 - final_logsoft_loss: 14.6565 - tf.nn.log_softmax_loss: 18.9292 - final_logsoft_Lev: 0.8143 - tf.nn.log_softmax_Lev: 0.7635\n","Epoch 152: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.75 GB\n","891/891 [==============================] - 460s 514ms/step - loss: 16.7926 - final_logsoft_loss: 14.6565 - tf.nn.log_softmax_loss: 18.9292 - final_logsoft_Lev: 0.8143 - tf.nn.log_softmax_Lev: 0.7635 - val_loss: 16.1906 - val_final_logsoft_loss: 15.9271 - val_tf.nn.log_softmax_loss: 16.4562 - val_final_logsoft_Lev: 0.7864 - val_tf.nn.log_softmax_Lev: 0.7664\n","Epoch 153/200\n","891/891 [==============================] - ETA: 0s - loss: 16.7505 - final_logsoft_loss: 14.6124 - tf.nn.log_softmax_loss: 18.8884 - final_logsoft_Lev: 0.8148 - tf.nn.log_softmax_Lev: 0.7641\n","Epoch 153: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.58 GB\n","891/891 [==============================] - 462s 517ms/step - loss: 16.7505 - final_logsoft_loss: 14.6124 - tf.nn.log_softmax_loss: 18.8884 - final_logsoft_Lev: 0.8148 - tf.nn.log_softmax_Lev: 0.7641 - val_loss: 16.2532 - val_final_logsoft_loss: 16.0464 - val_tf.nn.log_softmax_loss: 16.4608 - val_final_logsoft_Lev: 0.7864 - val_tf.nn.log_softmax_Lev: 0.7675\n","Epoch 154/200\n","891/891 [==============================] - ETA: 0s - loss: 16.7365 - final_logsoft_loss: 14.5944 - tf.nn.log_softmax_loss: 18.8787 - final_logsoft_Lev: 0.8149 - tf.nn.log_softmax_Lev: 0.7644\n","Epoch 154: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.76 GB\n","891/891 [==============================] - 459s 513ms/step - loss: 16.7365 - final_logsoft_loss: 14.5944 - tf.nn.log_softmax_loss: 18.8787 - final_logsoft_Lev: 0.8149 - tf.nn.log_softmax_Lev: 0.7644 - val_loss: 16.2335 - val_final_logsoft_loss: 16.0887 - val_tf.nn.log_softmax_loss: 16.3787 - val_final_logsoft_Lev: 0.7870 - val_tf.nn.log_softmax_Lev: 0.7681\n","Epoch 155/200\n","891/891 [==============================] - ETA: 0s - loss: 16.6958 - final_logsoft_loss: 14.5432 - tf.nn.log_softmax_loss: 18.8485 - final_logsoft_Lev: 0.8156 - tf.nn.log_softmax_Lev: 0.7647\n","Epoch 155: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.68 GB\n","891/891 [==============================] - 463s 518ms/step - loss: 16.6958 - final_logsoft_loss: 14.5432 - tf.nn.log_softmax_loss: 18.8485 - final_logsoft_Lev: 0.8156 - tf.nn.log_softmax_Lev: 0.7647 - val_loss: 16.2548 - val_final_logsoft_loss: 16.0526 - val_tf.nn.log_softmax_loss: 16.4577 - val_final_logsoft_Lev: 0.7872 - val_tf.nn.log_softmax_Lev: 0.7675\n","Epoch 156/200\n","891/891 [==============================] - ETA: 0s - loss: 16.6489 - final_logsoft_loss: 14.5081 - tf.nn.log_softmax_loss: 18.7896 - final_logsoft_Lev: 0.8164 - tf.nn.log_softmax_Lev: 0.7651\n","Epoch 156: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.53 GB\n","891/891 [==============================] - 459s 513ms/step - loss: 16.6489 - final_logsoft_loss: 14.5081 - tf.nn.log_softmax_loss: 18.7896 - final_logsoft_Lev: 0.8164 - tf.nn.log_softmax_Lev: 0.7651 - val_loss: 16.1136 - val_final_logsoft_loss: 15.8572 - val_tf.nn.log_softmax_loss: 16.3696 - val_final_logsoft_Lev: 0.7865 - val_tf.nn.log_softmax_Lev: 0.7666\n","Epoch 157/200\n","891/891 [==============================] - ETA: 0s - loss: 16.6435 - final_logsoft_loss: 14.5158 - tf.nn.log_softmax_loss: 18.7710 - final_logsoft_Lev: 0.8161 - tf.nn.log_softmax_Lev: 0.7651\n","Epoch 157: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.60 GB\n","891/891 [==============================] - 464s 519ms/step - loss: 16.6435 - final_logsoft_loss: 14.5158 - tf.nn.log_softmax_loss: 18.7710 - final_logsoft_Lev: 0.8161 - tf.nn.log_softmax_Lev: 0.7651 - val_loss: 16.2790 - val_final_logsoft_loss: 16.0997 - val_tf.nn.log_softmax_loss: 16.4596 - val_final_logsoft_Lev: 0.7872 - val_tf.nn.log_softmax_Lev: 0.7674\n","Epoch 158/200\n","891/891 [==============================] - ETA: 0s - loss: 16.6047 - final_logsoft_loss: 14.4432 - tf.nn.log_softmax_loss: 18.7662 - final_logsoft_Lev: 0.8168 - tf.nn.log_softmax_Lev: 0.7653\n","Epoch 158: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.77 GB\n","891/891 [==============================] - 461s 515ms/step - loss: 16.6047 - final_logsoft_loss: 14.4432 - tf.nn.log_softmax_loss: 18.7662 - final_logsoft_Lev: 0.8168 - tf.nn.log_softmax_Lev: 0.7653 - val_loss: 16.1547 - val_final_logsoft_loss: 15.9263 - val_tf.nn.log_softmax_loss: 16.3806 - val_final_logsoft_Lev: 0.7877 - val_tf.nn.log_softmax_Lev: 0.7680\n","Epoch 159/200\n","891/891 [==============================] - ETA: 0s - loss: 16.5675 - final_logsoft_loss: 14.4153 - tf.nn.log_softmax_loss: 18.7202 - final_logsoft_Lev: 0.8172 - tf.nn.log_softmax_Lev: 0.7659\n","Epoch 159: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.90 GB\n","891/891 [==============================] - 463s 518ms/step - loss: 16.5675 - final_logsoft_loss: 14.4153 - tf.nn.log_softmax_loss: 18.7202 - final_logsoft_Lev: 0.8172 - tf.nn.log_softmax_Lev: 0.7659 - val_loss: 16.2892 - val_final_logsoft_loss: 16.1036 - val_tf.nn.log_softmax_loss: 16.4767 - val_final_logsoft_Lev: 0.7874 - val_tf.nn.log_softmax_Lev: 0.7686\n","Epoch 160/200\n","891/891 [==============================] - ETA: 0s - loss: 16.5526 - final_logsoft_loss: 14.4032 - tf.nn.log_softmax_loss: 18.7017 - final_logsoft_Lev: 0.8174 - tf.nn.log_softmax_Lev: 0.7661\n","Epoch 160: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.77 GB\n","891/891 [==============================] - 459s 513ms/step - loss: 16.5526 - final_logsoft_loss: 14.4032 - tf.nn.log_softmax_loss: 18.7017 - final_logsoft_Lev: 0.8174 - tf.nn.log_softmax_Lev: 0.7661 - val_loss: 16.1899 - val_final_logsoft_loss: 16.0024 - val_tf.nn.log_softmax_loss: 16.3796 - val_final_logsoft_Lev: 0.7877 - val_tf.nn.log_softmax_Lev: 0.7682\n","Epoch 161/200\n","891/891 [==============================] - ETA: 0s - loss: 16.5321 - final_logsoft_loss: 14.3757 - tf.nn.log_softmax_loss: 18.6888 - final_logsoft_Lev: 0.8178 - tf.nn.log_softmax_Lev: 0.7664\n","Epoch 161: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.16 GB\n","891/891 [==============================] - 462s 516ms/step - loss: 16.5321 - final_logsoft_loss: 14.3757 - tf.nn.log_softmax_loss: 18.6888 - final_logsoft_Lev: 0.8178 - tf.nn.log_softmax_Lev: 0.7664 - val_loss: 16.1744 - val_final_logsoft_loss: 15.9941 - val_tf.nn.log_softmax_loss: 16.3573 - val_final_logsoft_Lev: 0.7878 - val_tf.nn.log_softmax_Lev: 0.7679\n","Epoch 162/200\n","891/891 [==============================] - ETA: 0s - loss: 16.4930 - final_logsoft_loss: 14.3377 - tf.nn.log_softmax_loss: 18.6483 - final_logsoft_Lev: 0.8182 - tf.nn.log_softmax_Lev: 0.7669\n","Epoch 162: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 6.10 GB\n","891/891 [==============================] - 467s 522ms/step - loss: 16.4930 - final_logsoft_loss: 14.3377 - tf.nn.log_softmax_loss: 18.6483 - final_logsoft_Lev: 0.8182 - tf.nn.log_softmax_Lev: 0.7669 - val_loss: 16.1479 - val_final_logsoft_loss: 15.9489 - val_tf.nn.log_softmax_loss: 16.3500 - val_final_logsoft_Lev: 0.7881 - val_tf.nn.log_softmax_Lev: 0.7676\n","Epoch 163/200\n","891/891 [==============================] - ETA: 0s - loss: 16.4954 - final_logsoft_loss: 14.3493 - tf.nn.log_softmax_loss: 18.6413 - final_logsoft_Lev: 0.8180 - tf.nn.log_softmax_Lev: 0.7668\n","Epoch 163: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.80 GB\n","891/891 [==============================] - 497s 556ms/step - loss: 16.4954 - final_logsoft_loss: 14.3493 - tf.nn.log_softmax_loss: 18.6413 - final_logsoft_Lev: 0.8180 - tf.nn.log_softmax_Lev: 0.7668 - val_loss: 16.2695 - val_final_logsoft_loss: 16.1326 - val_tf.nn.log_softmax_loss: 16.4069 - val_final_logsoft_Lev: 0.7879 - val_tf.nn.log_softmax_Lev: 0.7678\n","Epoch 164/200\n","891/891 [==============================] - ETA: 0s - loss: 16.4839 - final_logsoft_loss: 14.3313 - tf.nn.log_softmax_loss: 18.6362 - final_logsoft_Lev: 0.8182 - tf.nn.log_softmax_Lev: 0.7669\n","Epoch 164: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.47 GB\n","891/891 [==============================] - 496s 553ms/step - loss: 16.4839 - final_logsoft_loss: 14.3313 - tf.nn.log_softmax_loss: 18.6362 - final_logsoft_Lev: 0.8182 - tf.nn.log_softmax_Lev: 0.7669 - val_loss: 16.1564 - val_final_logsoft_loss: 15.9190 - val_tf.nn.log_softmax_loss: 16.3923 - val_final_logsoft_Lev: 0.7879 - val_tf.nn.log_softmax_Lev: 0.7689\n","Epoch 165/200\n","891/891 [==============================] - ETA: 0s - loss: 16.4571 - final_logsoft_loss: 14.3037 - tf.nn.log_softmax_loss: 18.6106 - final_logsoft_Lev: 0.8185 - tf.nn.log_softmax_Lev: 0.7671\n","Epoch 165: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.86 GB\n","891/891 [==============================] - 498s 557ms/step - loss: 16.4571 - final_logsoft_loss: 14.3037 - tf.nn.log_softmax_loss: 18.6106 - final_logsoft_Lev: 0.8185 - tf.nn.log_softmax_Lev: 0.7671 - val_loss: 16.2284 - val_final_logsoft_loss: 16.0878 - val_tf.nn.log_softmax_loss: 16.3714 - val_final_logsoft_Lev: 0.7882 - val_tf.nn.log_softmax_Lev: 0.7681\n","Epoch 166/200\n","891/891 [==============================] - ETA: 0s - loss: 16.4015 - final_logsoft_loss: 14.2587 - tf.nn.log_softmax_loss: 18.5441 - final_logsoft_Lev: 0.8190 - tf.nn.log_softmax_Lev: 0.7679\n","Epoch 166: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.59 GB\n","891/891 [==============================] - 500s 559ms/step - loss: 16.4015 - final_logsoft_loss: 14.2587 - tf.nn.log_softmax_loss: 18.5441 - final_logsoft_Lev: 0.8190 - tf.nn.log_softmax_Lev: 0.7679 - val_loss: 16.2328 - val_final_logsoft_loss: 16.0392 - val_tf.nn.log_softmax_loss: 16.4266 - val_final_logsoft_Lev: 0.7885 - val_tf.nn.log_softmax_Lev: 0.7692\n","Epoch 167/200\n","891/891 [==============================] - ETA: 0s - loss: 16.4055 - final_logsoft_loss: 14.2390 - tf.nn.log_softmax_loss: 18.5717 - final_logsoft_Lev: 0.8194 - tf.nn.log_softmax_Lev: 0.7676\n","Epoch 167: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.63 GB\n","891/891 [==============================] - 504s 563ms/step - loss: 16.4055 - final_logsoft_loss: 14.2390 - tf.nn.log_softmax_loss: 18.5717 - final_logsoft_Lev: 0.8194 - tf.nn.log_softmax_Lev: 0.7676 - val_loss: 16.2371 - val_final_logsoft_loss: 16.0679 - val_tf.nn.log_softmax_loss: 16.4064 - val_final_logsoft_Lev: 0.7889 - val_tf.nn.log_softmax_Lev: 0.7688\n","Epoch 168/200\n","891/891 [==============================] - ETA: 0s - loss: 16.3803 - final_logsoft_loss: 14.2267 - tf.nn.log_softmax_loss: 18.5336 - final_logsoft_Lev: 0.8195 - tf.nn.log_softmax_Lev: 0.7682\n","Epoch 168: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.30 GB\n","891/891 [==============================] - 501s 560ms/step - loss: 16.3803 - final_logsoft_loss: 14.2267 - tf.nn.log_softmax_loss: 18.5336 - final_logsoft_Lev: 0.8195 - tf.nn.log_softmax_Lev: 0.7682 - val_loss: 16.3169 - val_final_logsoft_loss: 16.1940 - val_tf.nn.log_softmax_loss: 16.4417 - val_final_logsoft_Lev: 0.7883 - val_tf.nn.log_softmax_Lev: 0.7686\n","Epoch 169/200\n","891/891 [==============================] - ETA: 0s - loss: 16.3488 - final_logsoft_loss: 14.1910 - tf.nn.log_softmax_loss: 18.5065 - final_logsoft_Lev: 0.8200 - tf.nn.log_softmax_Lev: 0.7683\n","Epoch 169: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.33 GB\n","891/891 [==============================] - 505s 564ms/step - loss: 16.3488 - final_logsoft_loss: 14.1910 - tf.nn.log_softmax_loss: 18.5065 - final_logsoft_Lev: 0.8200 - tf.nn.log_softmax_Lev: 0.7683 - val_loss: 16.2271 - val_final_logsoft_loss: 16.0647 - val_tf.nn.log_softmax_loss: 16.3882 - val_final_logsoft_Lev: 0.7881 - val_tf.nn.log_softmax_Lev: 0.7686\n","Epoch 170/200\n","891/891 [==============================] - ETA: 0s - loss: 16.3286 - final_logsoft_loss: 14.1835 - tf.nn.log_softmax_loss: 18.4736 - final_logsoft_Lev: 0.8201 - tf.nn.log_softmax_Lev: 0.7689\n","Epoch 170: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.02 GB\n","891/891 [==============================] - 505s 564ms/step - loss: 16.3286 - final_logsoft_loss: 14.1835 - tf.nn.log_softmax_loss: 18.4736 - final_logsoft_Lev: 0.8201 - tf.nn.log_softmax_Lev: 0.7689 - val_loss: 16.2762 - val_final_logsoft_loss: 16.1131 - val_tf.nn.log_softmax_loss: 16.4385 - val_final_logsoft_Lev: 0.7886 - val_tf.nn.log_softmax_Lev: 0.7688\n","Epoch 171/200\n","891/891 [==============================] - ETA: 0s - loss: 16.3370 - final_logsoft_loss: 14.1885 - tf.nn.log_softmax_loss: 18.4855 - final_logsoft_Lev: 0.8201 - tf.nn.log_softmax_Lev: 0.7689\n","Epoch 171: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.08 GB\n","891/891 [==============================] - 523s 585ms/step - loss: 16.3370 - final_logsoft_loss: 14.1885 - tf.nn.log_softmax_loss: 18.4855 - final_logsoft_Lev: 0.8201 - tf.nn.log_softmax_Lev: 0.7689 - val_loss: 16.1943 - val_final_logsoft_loss: 16.0700 - val_tf.nn.log_softmax_loss: 16.3173 - val_final_logsoft_Lev: 0.7883 - val_tf.nn.log_softmax_Lev: 0.7691\n","Epoch 172/200\n","891/891 [==============================] - ETA: 0s - loss: 16.3028 - final_logsoft_loss: 14.1484 - tf.nn.log_softmax_loss: 18.4573 - final_logsoft_Lev: 0.8206 - tf.nn.log_softmax_Lev: 0.7690\n","Epoch 172: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.97 GB\n","891/891 [==============================] - 521s 583ms/step - loss: 16.3028 - final_logsoft_loss: 14.1484 - tf.nn.log_softmax_loss: 18.4573 - final_logsoft_Lev: 0.8206 - tf.nn.log_softmax_Lev: 0.7690 - val_loss: 16.2425 - val_final_logsoft_loss: 16.0830 - val_tf.nn.log_softmax_loss: 16.4010 - val_final_logsoft_Lev: 0.7883 - val_tf.nn.log_softmax_Lev: 0.7697\n","Epoch 173/200\n","891/891 [==============================] - ETA: 0s - loss: 16.2725 - final_logsoft_loss: 14.1194 - tf.nn.log_softmax_loss: 18.4256 - final_logsoft_Lev: 0.8208 - tf.nn.log_softmax_Lev: 0.7694\n","Epoch 173: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.01 GB\n","891/891 [==============================] - 507s 566ms/step - loss: 16.2725 - final_logsoft_loss: 14.1194 - tf.nn.log_softmax_loss: 18.4256 - final_logsoft_Lev: 0.8208 - tf.nn.log_softmax_Lev: 0.7694 - val_loss: 16.2289 - val_final_logsoft_loss: 16.0579 - val_tf.nn.log_softmax_loss: 16.4000 - val_final_logsoft_Lev: 0.7885 - val_tf.nn.log_softmax_Lev: 0.7696\n","Epoch 174/200\n","891/891 [==============================] - ETA: 0s - loss: 16.3011 - final_logsoft_loss: 14.1433 - tf.nn.log_softmax_loss: 18.4586 - final_logsoft_Lev: 0.8205 - tf.nn.log_softmax_Lev: 0.7690\n","Epoch 174: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.02 GB\n","891/891 [==============================] - 502s 561ms/step - loss: 16.3011 - final_logsoft_loss: 14.1433 - tf.nn.log_softmax_loss: 18.4586 - final_logsoft_Lev: 0.8205 - tf.nn.log_softmax_Lev: 0.7690 - val_loss: 16.2296 - val_final_logsoft_loss: 16.0817 - val_tf.nn.log_softmax_loss: 16.3787 - val_final_logsoft_Lev: 0.7888 - val_tf.nn.log_softmax_Lev: 0.7696\n","Epoch 175/200\n","891/891 [==============================] - ETA: 0s - loss: 16.2910 - final_logsoft_loss: 14.1320 - tf.nn.log_softmax_loss: 18.4498 - final_logsoft_Lev: 0.8206 - tf.nn.log_softmax_Lev: 0.7693\n","Epoch 175: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.33 GB\n","891/891 [==============================] - 502s 561ms/step - loss: 16.2910 - final_logsoft_loss: 14.1320 - tf.nn.log_softmax_loss: 18.4498 - final_logsoft_Lev: 0.8206 - tf.nn.log_softmax_Lev: 0.7693 - val_loss: 16.2030 - val_final_logsoft_loss: 16.0579 - val_tf.nn.log_softmax_loss: 16.3490 - val_final_logsoft_Lev: 0.7891 - val_tf.nn.log_softmax_Lev: 0.7696\n","Epoch 176/200\n","891/891 [==============================] - ETA: 0s - loss: 16.2322 - final_logsoft_loss: 14.0547 - tf.nn.log_softmax_loss: 18.4103 - final_logsoft_Lev: 0.8215 - tf.nn.log_softmax_Lev: 0.7696\n","Epoch 176: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.19 GB\n","891/891 [==============================] - 497s 555ms/step - loss: 16.2322 - final_logsoft_loss: 14.0547 - tf.nn.log_softmax_loss: 18.4103 - final_logsoft_Lev: 0.8215 - tf.nn.log_softmax_Lev: 0.7696 - val_loss: 16.2300 - val_final_logsoft_loss: 16.0768 - val_tf.nn.log_softmax_loss: 16.3833 - val_final_logsoft_Lev: 0.7887 - val_tf.nn.log_softmax_Lev: 0.7696\n","Epoch 177/200\n","891/891 [==============================] - ETA: 0s - loss: 16.2378 - final_logsoft_loss: 14.0918 - tf.nn.log_softmax_loss: 18.3838 - final_logsoft_Lev: 0.8209 - tf.nn.log_softmax_Lev: 0.7700\n","Epoch 177: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.17 GB\n","891/891 [==============================] - 500s 558ms/step - loss: 16.2378 - final_logsoft_loss: 14.0918 - tf.nn.log_softmax_loss: 18.3838 - final_logsoft_Lev: 0.8209 - tf.nn.log_softmax_Lev: 0.7700 - val_loss: 16.2435 - val_final_logsoft_loss: 16.0992 - val_tf.nn.log_softmax_loss: 16.3847 - val_final_logsoft_Lev: 0.7891 - val_tf.nn.log_softmax_Lev: 0.7698\n","Epoch 178/200\n","891/891 [==============================] - ETA: 0s - loss: 16.1933 - final_logsoft_loss: 14.0241 - tf.nn.log_softmax_loss: 18.3624 - final_logsoft_Lev: 0.8219 - tf.nn.log_softmax_Lev: 0.7701\n","Epoch 178: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.13 GB\n","891/891 [==============================] - 499s 558ms/step - loss: 16.1933 - final_logsoft_loss: 14.0241 - tf.nn.log_softmax_loss: 18.3624 - final_logsoft_Lev: 0.8219 - tf.nn.log_softmax_Lev: 0.7701 - val_loss: 16.2442 - val_final_logsoft_loss: 16.1009 - val_tf.nn.log_softmax_loss: 16.3886 - val_final_logsoft_Lev: 0.7890 - val_tf.nn.log_softmax_Lev: 0.7696\n","Epoch 179/200\n","891/891 [==============================] - ETA: 0s - loss: 16.2234 - final_logsoft_loss: 14.0627 - tf.nn.log_softmax_loss: 18.3843 - final_logsoft_Lev: 0.8215 - tf.nn.log_softmax_Lev: 0.7699\n","Epoch 179: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.25 GB\n","891/891 [==============================] - 501s 560ms/step - loss: 16.2234 - final_logsoft_loss: 14.0627 - tf.nn.log_softmax_loss: 18.3843 - final_logsoft_Lev: 0.8215 - tf.nn.log_softmax_Lev: 0.7699 - val_loss: 16.2563 - val_final_logsoft_loss: 16.1289 - val_tf.nn.log_softmax_loss: 16.3849 - val_final_logsoft_Lev: 0.7890 - val_tf.nn.log_softmax_Lev: 0.7697\n","Epoch 180/200\n","891/891 [==============================] - ETA: 0s - loss: 16.2114 - final_logsoft_loss: 14.0602 - tf.nn.log_softmax_loss: 18.3623 - final_logsoft_Lev: 0.8215 - tf.nn.log_softmax_Lev: 0.7702\n","Epoch 180: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.13 GB\n","891/891 [==============================] - 494s 553ms/step - loss: 16.2114 - final_logsoft_loss: 14.0602 - tf.nn.log_softmax_loss: 18.3623 - final_logsoft_Lev: 0.8215 - tf.nn.log_softmax_Lev: 0.7702 - val_loss: 16.2323 - val_final_logsoft_loss: 16.0759 - val_tf.nn.log_softmax_loss: 16.3877 - val_final_logsoft_Lev: 0.7898 - val_tf.nn.log_softmax_Lev: 0.7697\n","Epoch 181/200\n","891/891 [==============================] - ETA: 0s - loss: 16.1634 - final_logsoft_loss: 13.9966 - tf.nn.log_softmax_loss: 18.3296 - final_logsoft_Lev: 0.8224 - tf.nn.log_softmax_Lev: 0.7703\n","Epoch 181: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.40 GB\n","891/891 [==============================] - 498s 557ms/step - loss: 16.1634 - final_logsoft_loss: 13.9966 - tf.nn.log_softmax_loss: 18.3296 - final_logsoft_Lev: 0.8224 - tf.nn.log_softmax_Lev: 0.7703 - val_loss: 16.2609 - val_final_logsoft_loss: 16.1345 - val_tf.nn.log_softmax_loss: 16.3862 - val_final_logsoft_Lev: 0.7890 - val_tf.nn.log_softmax_Lev: 0.7699\n","Epoch 182/200\n","891/891 [==============================] - ETA: 0s - loss: 16.1878 - final_logsoft_loss: 14.0211 - tf.nn.log_softmax_loss: 18.3545 - final_logsoft_Lev: 0.8219 - tf.nn.log_softmax_Lev: 0.7699\n","Epoch 182: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.34 GB\n","891/891 [==============================] - 497s 556ms/step - loss: 16.1878 - final_logsoft_loss: 14.0211 - tf.nn.log_softmax_loss: 18.3545 - final_logsoft_Lev: 0.8219 - tf.nn.log_softmax_Lev: 0.7699 - val_loss: 16.2345 - val_final_logsoft_loss: 16.0791 - val_tf.nn.log_softmax_loss: 16.3928 - val_final_logsoft_Lev: 0.7893 - val_tf.nn.log_softmax_Lev: 0.7702\n","Epoch 183/200\n","891/891 [==============================] - ETA: 0s - loss: 16.1498 - final_logsoft_loss: 13.9790 - tf.nn.log_softmax_loss: 18.3207 - final_logsoft_Lev: 0.8225 - tf.nn.log_softmax_Lev: 0.7705\n","Epoch 183: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.52 GB\n","891/891 [==============================] - 497s 556ms/step - loss: 16.1498 - final_logsoft_loss: 13.9790 - tf.nn.log_softmax_loss: 18.3207 - final_logsoft_Lev: 0.8225 - tf.nn.log_softmax_Lev: 0.7705 - val_loss: 16.2427 - val_final_logsoft_loss: 16.1009 - val_tf.nn.log_softmax_loss: 16.3854 - val_final_logsoft_Lev: 0.7893 - val_tf.nn.log_softmax_Lev: 0.7698\n","Epoch 184/200\n","891/891 [==============================] - ETA: 0s - loss: 16.1440 - final_logsoft_loss: 13.9948 - tf.nn.log_softmax_loss: 18.2936 - final_logsoft_Lev: 0.8224 - tf.nn.log_softmax_Lev: 0.7707\n","Epoch 184: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.27 GB\n","891/891 [==============================] - 498s 557ms/step - loss: 16.1440 - final_logsoft_loss: 13.9948 - tf.nn.log_softmax_loss: 18.2936 - final_logsoft_Lev: 0.8224 - tf.nn.log_softmax_Lev: 0.7707 - val_loss: 16.2030 - val_final_logsoft_loss: 16.0610 - val_tf.nn.log_softmax_loss: 16.3431 - val_final_logsoft_Lev: 0.7895 - val_tf.nn.log_softmax_Lev: 0.7697\n","Epoch 185/200\n","891/891 [==============================] - ETA: 0s - loss: 16.1875 - final_logsoft_loss: 14.0409 - tf.nn.log_softmax_loss: 18.3344 - final_logsoft_Lev: 0.8219 - tf.nn.log_softmax_Lev: 0.7705\n","Epoch 185: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.44 GB\n","891/891 [==============================] - 501s 558ms/step - loss: 16.1875 - final_logsoft_loss: 14.0409 - tf.nn.log_softmax_loss: 18.3344 - final_logsoft_Lev: 0.8219 - tf.nn.log_softmax_Lev: 0.7705 - val_loss: 16.2787 - val_final_logsoft_loss: 16.1515 - val_tf.nn.log_softmax_loss: 16.4056 - val_final_logsoft_Lev: 0.7892 - val_tf.nn.log_softmax_Lev: 0.7703\n","Epoch 186/200\n","891/891 [==============================] - ETA: 0s - loss: 16.1618 - final_logsoft_loss: 13.9984 - tf.nn.log_softmax_loss: 18.3253 - final_logsoft_Lev: 0.8224 - tf.nn.log_softmax_Lev: 0.7705\n","Epoch 186: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.47 GB\n","891/891 [==============================] - 497s 556ms/step - loss: 16.1618 - final_logsoft_loss: 13.9984 - tf.nn.log_softmax_loss: 18.3253 - final_logsoft_Lev: 0.8224 - tf.nn.log_softmax_Lev: 0.7705 - val_loss: 16.2658 - val_final_logsoft_loss: 16.1666 - val_tf.nn.log_softmax_loss: 16.3641 - val_final_logsoft_Lev: 0.7892 - val_tf.nn.log_softmax_Lev: 0.7707\n","Epoch 187/200\n","891/891 [==============================] - ETA: 0s - loss: 16.1040 - final_logsoft_loss: 13.9478 - tf.nn.log_softmax_loss: 18.2601 - final_logsoft_Lev: 0.8230 - tf.nn.log_softmax_Lev: 0.7713\n","Epoch 187: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.66 GB\n","891/891 [==============================] - 501s 561ms/step - loss: 16.1040 - final_logsoft_loss: 13.9478 - tf.nn.log_softmax_loss: 18.2601 - final_logsoft_Lev: 0.8230 - tf.nn.log_softmax_Lev: 0.7713 - val_loss: 16.2128 - val_final_logsoft_loss: 16.0822 - val_tf.nn.log_softmax_loss: 16.3429 - val_final_logsoft_Lev: 0.7895 - val_tf.nn.log_softmax_Lev: 0.7707\n","Epoch 188/200\n","891/891 [==============================] - ETA: 0s - loss: 16.1086 - final_logsoft_loss: 13.9590 - tf.nn.log_softmax_loss: 18.2584 - final_logsoft_Lev: 0.8227 - tf.nn.log_softmax_Lev: 0.7714\n","Epoch 188: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.55 GB\n","891/891 [==============================] - 498s 554ms/step - loss: 16.1086 - final_logsoft_loss: 13.9590 - tf.nn.log_softmax_loss: 18.2584 - final_logsoft_Lev: 0.8227 - tf.nn.log_softmax_Lev: 0.7714 - val_loss: 16.2405 - val_final_logsoft_loss: 16.1214 - val_tf.nn.log_softmax_loss: 16.3592 - val_final_logsoft_Lev: 0.7896 - val_tf.nn.log_softmax_Lev: 0.7703\n","Epoch 189/200\n","891/891 [==============================] - ETA: 0s - loss: 16.0923 - final_logsoft_loss: 13.9324 - tf.nn.log_softmax_loss: 18.2523 - final_logsoft_Lev: 0.8229 - tf.nn.log_softmax_Lev: 0.7713\n","Epoch 189: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.74 GB\n","891/891 [==============================] - 498s 556ms/step - loss: 16.0923 - final_logsoft_loss: 13.9324 - tf.nn.log_softmax_loss: 18.2523 - final_logsoft_Lev: 0.8229 - tf.nn.log_softmax_Lev: 0.7713 - val_loss: 16.2383 - val_final_logsoft_loss: 16.1109 - val_tf.nn.log_softmax_loss: 16.3665 - val_final_logsoft_Lev: 0.7898 - val_tf.nn.log_softmax_Lev: 0.7707\n","Epoch 190/200\n","891/891 [==============================] - ETA: 0s - loss: 16.1088 - final_logsoft_loss: 13.9486 - tf.nn.log_softmax_loss: 18.2694 - final_logsoft_Lev: 0.8228 - tf.nn.log_softmax_Lev: 0.7710\n","Epoch 190: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.41 GB\n","891/891 [==============================] - 503s 563ms/step - loss: 16.1088 - final_logsoft_loss: 13.9486 - tf.nn.log_softmax_loss: 18.2694 - final_logsoft_Lev: 0.8228 - tf.nn.log_softmax_Lev: 0.7710 - val_loss: 16.2717 - val_final_logsoft_loss: 16.1588 - val_tf.nn.log_softmax_loss: 16.3843 - val_final_logsoft_Lev: 0.7897 - val_tf.nn.log_softmax_Lev: 0.7704\n","Epoch 191/200\n","891/891 [==============================] - ETA: 0s - loss: 16.1128 - final_logsoft_loss: 13.9659 - tf.nn.log_softmax_loss: 18.2599 - final_logsoft_Lev: 0.8227 - tf.nn.log_softmax_Lev: 0.7713\n","Epoch 191: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.68 GB\n","891/891 [==============================] - 501s 560ms/step - loss: 16.1128 - final_logsoft_loss: 13.9659 - tf.nn.log_softmax_loss: 18.2599 - final_logsoft_Lev: 0.8227 - tf.nn.log_softmax_Lev: 0.7713 - val_loss: 16.2271 - val_final_logsoft_loss: 16.0909 - val_tf.nn.log_softmax_loss: 16.3631 - val_final_logsoft_Lev: 0.7897 - val_tf.nn.log_softmax_Lev: 0.7705\n","Epoch 192/200\n","891/891 [==============================] - ETA: 0s - loss: 16.0938 - final_logsoft_loss: 13.9465 - tf.nn.log_softmax_loss: 18.2412 - final_logsoft_Lev: 0.8228 - tf.nn.log_softmax_Lev: 0.7716\n","Epoch 192: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.38 GB\n","891/891 [==============================] - 491s 548ms/step - loss: 16.0938 - final_logsoft_loss: 13.9465 - tf.nn.log_softmax_loss: 18.2412 - final_logsoft_Lev: 0.8228 - tf.nn.log_softmax_Lev: 0.7716 - val_loss: 16.2481 - val_final_logsoft_loss: 16.1191 - val_tf.nn.log_softmax_loss: 16.3787 - val_final_logsoft_Lev: 0.7898 - val_tf.nn.log_softmax_Lev: 0.7706\n","Epoch 193/200\n","891/891 [==============================] - ETA: 0s - loss: 16.1117 - final_logsoft_loss: 13.9556 - tf.nn.log_softmax_loss: 18.2680 - final_logsoft_Lev: 0.8229 - tf.nn.log_softmax_Lev: 0.7713\n","Epoch 193: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 4.97 GB\n","891/891 [==============================] - 489s 547ms/step - loss: 16.1117 - final_logsoft_loss: 13.9556 - tf.nn.log_softmax_loss: 18.2680 - final_logsoft_Lev: 0.8229 - tf.nn.log_softmax_Lev: 0.7713 - val_loss: 16.2519 - val_final_logsoft_loss: 16.1145 - val_tf.nn.log_softmax_loss: 16.3869 - val_final_logsoft_Lev: 0.7892 - val_tf.nn.log_softmax_Lev: 0.7706\n","Epoch 194/200\n","891/891 [==============================] - ETA: 0s - loss: 16.1175 - final_logsoft_loss: 13.9673 - tf.nn.log_softmax_loss: 18.2676 - final_logsoft_Lev: 0.8226 - tf.nn.log_softmax_Lev: 0.7712\n","Epoch 194: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.33 GB\n","891/891 [==============================] - 490s 547ms/step - loss: 16.1175 - final_logsoft_loss: 13.9673 - tf.nn.log_softmax_loss: 18.2676 - final_logsoft_Lev: 0.8226 - tf.nn.log_softmax_Lev: 0.7712 - val_loss: 16.2541 - val_final_logsoft_loss: 16.1298 - val_tf.nn.log_softmax_loss: 16.3777 - val_final_logsoft_Lev: 0.7893 - val_tf.nn.log_softmax_Lev: 0.7705\n","Epoch 195/200\n","891/891 [==============================] - ETA: 0s - loss: 16.0892 - final_logsoft_loss: 13.9480 - tf.nn.log_softmax_loss: 18.2306 - final_logsoft_Lev: 0.8229 - tf.nn.log_softmax_Lev: 0.7716\n","Epoch 195: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.41 GB\n","891/891 [==============================] - 477s 531ms/step - loss: 16.0892 - final_logsoft_loss: 13.9480 - tf.nn.log_softmax_loss: 18.2306 - final_logsoft_Lev: 0.8229 - tf.nn.log_softmax_Lev: 0.7716 - val_loss: 16.2734 - val_final_logsoft_loss: 16.1444 - val_tf.nn.log_softmax_loss: 16.4015 - val_final_logsoft_Lev: 0.7895 - val_tf.nn.log_softmax_Lev: 0.7704\n","Epoch 196/200\n","891/891 [==============================] - ETA: 0s - loss: 16.1000 - final_logsoft_loss: 13.9516 - tf.nn.log_softmax_loss: 18.2484 - final_logsoft_Lev: 0.8228 - tf.nn.log_softmax_Lev: 0.7716\n","Epoch 196: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.38 GB\n","891/891 [==============================] - 491s 549ms/step - loss: 16.1000 - final_logsoft_loss: 13.9516 - tf.nn.log_softmax_loss: 18.2484 - final_logsoft_Lev: 0.8228 - tf.nn.log_softmax_Lev: 0.7716 - val_loss: 16.2619 - val_final_logsoft_loss: 16.1355 - val_tf.nn.log_softmax_loss: 16.3874 - val_final_logsoft_Lev: 0.7896 - val_tf.nn.log_softmax_Lev: 0.7703\n","Epoch 197/200\n","891/891 [==============================] - ETA: 0s - loss: 16.0913 - final_logsoft_loss: 13.9375 - tf.nn.log_softmax_loss: 18.2457 - final_logsoft_Lev: 0.8229 - tf.nn.log_softmax_Lev: 0.7714\n","Epoch 197: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.51 GB\n","891/891 [==============================] - 482s 539ms/step - loss: 16.0913 - final_logsoft_loss: 13.9375 - tf.nn.log_softmax_loss: 18.2457 - final_logsoft_Lev: 0.8229 - tf.nn.log_softmax_Lev: 0.7714 - val_loss: 16.2503 - val_final_logsoft_loss: 16.1136 - val_tf.nn.log_softmax_loss: 16.3857 - val_final_logsoft_Lev: 0.7895 - val_tf.nn.log_softmax_Lev: 0.7702\n","Epoch 198/200\n","891/891 [==============================] - ETA: 0s - loss: 16.1037 - final_logsoft_loss: 13.9612 - tf.nn.log_softmax_loss: 18.2460 - final_logsoft_Lev: 0.8230 - tf.nn.log_softmax_Lev: 0.7715\n","Epoch 198: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.41 GB\n","891/891 [==============================] - 494s 551ms/step - loss: 16.1037 - final_logsoft_loss: 13.9612 - tf.nn.log_softmax_loss: 18.2460 - final_logsoft_Lev: 0.8230 - tf.nn.log_softmax_Lev: 0.7715 - val_loss: 16.2227 - val_final_logsoft_loss: 16.0776 - val_tf.nn.log_softmax_loss: 16.3682 - val_final_logsoft_Lev: 0.7898 - val_tf.nn.log_softmax_Lev: 0.7703\n","Epoch 199/200\n","891/891 [==============================] - ETA: 0s - loss: 16.1039 - final_logsoft_loss: 13.9449 - tf.nn.log_softmax_loss: 18.2632 - final_logsoft_Lev: 0.8230 - tf.nn.log_softmax_Lev: 0.7713\n","Epoch 199: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.39 GB\n","891/891 [==============================] - 495s 553ms/step - loss: 16.1039 - final_logsoft_loss: 13.9449 - tf.nn.log_softmax_loss: 18.2632 - final_logsoft_Lev: 0.8230 - tf.nn.log_softmax_Lev: 0.7713 - val_loss: 16.2361 - val_final_logsoft_loss: 16.0977 - val_tf.nn.log_softmax_loss: 16.3762 - val_final_logsoft_Lev: 0.7897 - val_tf.nn.log_softmax_Lev: 0.7701\n","Epoch 200/200\n","891/891 [==============================] - ETA: 0s - loss: 16.0804 - final_logsoft_loss: 13.9156 - tf.nn.log_softmax_loss: 18.2455 - final_logsoft_Lev: 0.8231 - tf.nn.log_softmax_Lev: 0.7715\n","Epoch 200: val_loss did not improve from 16.04042\n","Memory usage on epoch end: 5.44 GB\n","891/891 [==============================] - 498s 556ms/step - loss: 16.0804 - final_logsoft_loss: 13.9156 - tf.nn.log_softmax_loss: 18.2455 - final_logsoft_Lev: 0.8231 - tf.nn.log_softmax_Lev: 0.7715 - val_loss: 16.2486 - val_final_logsoft_loss: 16.1160 - val_tf.nn.log_softmax_loss: 16.3804 - val_final_logsoft_Lev: 0.7897 - val_tf.nn.log_softmax_Lev: 0.7701\n","applying SWA...\n","save SWA weights to /kaggle/working//model-384-seed42-exp0-SWA.h5\n","     46/Unknown - 15s 313ms/step - loss: 15.9479 - final_logsoft_loss: 15.6804 - tf.nn.log_softmax_loss: 16.2159 - final_logsoft_Lev: 0.7876 - tf.nn.log_softmax_Lev: 0.7678"]},{"name":"stderr","output_type":"stream","text":["2023-07-22 10:11:29.509615: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5009386464961438454\n","2023-07-22 10:11:29.509908: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17380943087610856358\n","2023-07-22 10:11:29.515806: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5521428811400530445\n","2023-07-22 10:11:29.515825: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14053907474552685417\n","2023-07-22 10:11:29.515835: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10799721537814050273\n"]},{"name":"stdout","output_type":"stream","text":["46/46 [==============================] - 16s 336ms/step - loss: 15.9479 - final_logsoft_loss: 15.6804 - tf.nn.log_softmax_loss: 16.2159 - final_logsoft_Lev: 0.7876 - tf.nn.log_softmax_Lev: 0.7678\n","     46/Unknown - 15s 317ms/step - loss: 16.0404 - final_logsoft_loss: 15.7313 - tf.nn.log_softmax_loss: 16.3480 - final_logsoft_Lev: 0.7857 - tf.nn.log_softmax_Lev: 0.7662"]},{"name":"stderr","output_type":"stream","text":["2023-07-22 10:11:45.531161: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5009386464961438454\n","2023-07-22 10:11:45.531190: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17380943087610856358\n","2023-07-22 10:11:45.531198: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5521428811400530445\n","2023-07-22 10:11:45.531204: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14053907474552685417\n","2023-07-22 10:11:45.531209: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10799721537814050273\n"]},{"name":"stdout","output_type":"stream","text":["46/46 [==============================] - 15s 321ms/step - loss: 16.0404 - final_logsoft_loss: 15.7313 - tf.nn.log_softmax_loss: 16.3480 - final_logsoft_Lev: 0.7857 - tf.nn.log_softmax_Lev: 0.7662\n"]}],"source":["if 'config' not in globals():\n","  config=CFG()\n","train(config,use_supplemental=True)"]},{"cell_type":"markdown","metadata":{"id":"Cdtx5fW3-DPm"},"source":["# Inference"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.523717Z","iopub.status.busy":"2023-07-18T09:04:44.523335Z","iopub.status.idle":"2023-07-18T09:04:44.53874Z","shell.execute_reply":"2023-07-18T09:04:44.536985Z","shell.execute_reply.started":"2023-07-18T09:04:44.523687Z"},"executionInfo":{"elapsed":22,"status":"aborted","timestamp":1689879900488,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"-uAqcPmSYi1-","trusted":true},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.541198Z","iopub.status.busy":"2023-07-18T09:04:44.540385Z","iopub.status.idle":"2023-07-18T09:04:44.556953Z","shell.execute_reply":"2023-07-18T09:04:44.555702Z","shell.execute_reply.started":"2023-07-18T09:04:44.541158Z"},"executionInfo":{"elapsed":21,"status":"aborted","timestamp":1689879900488,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"ZWJM1IC5Yi1-","trusted":true},"outputs":[],"source":["class InferModel(tf.Module):\n","    def __init__(self, model,config=CFG):\n","        super().__init__()\n","\n","        self.model = model\n","        self.max_len=config.max_len\n","\n","    @tf.function(\n","        input_signature=[tf.TensorSpec(shape=(None,Constants.NUM_INPUT_FEATURES), dtype=tf.float32, name=\"inputs\")]\n","    )\n","    def __call__(self, inputs):\n","        \"\"\"\n","        Applies the feature generation model and main model to the input tensor.\n","\n","        Args:\n","            inputs: Input tensor with shape (T, F).\n","\n","        Returns:\n","            A dictionary with a single key 'outputs' and corresponding output tensor.\n","        \"\"\"\n","        x=tf.cast(inputs,tf.float32)\n","        x = x[None] # trick to deal with empty frames\n","        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, Constants.NUM_INPUT_FEATURES)), lambda: tf.identity(x))\n","        x = x[0]\n","        x = preprocess(x,max_len=self.max_len)\n","\n","        x = self.model(x[None],training=False)[0][0]\n","\n","        x=decode_phrase(x)\n","        x = tf.cond(tf.shape(x)[0] == 0, lambda: tf.zeros(1, tf.int64), lambda: tf.identity(x))\n","\n","        outputs=tf.one_hot(x,depth=59,dtype=tf.float32)\n","        return {\"outputs\": outputs}\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:44.558943Z","iopub.status.busy":"2023-07-18T09:04:44.558502Z","iopub.status.idle":"2023-07-18T09:04:48.116622Z","shell.execute_reply":"2023-07-18T09:04:48.11492Z","shell.execute_reply.started":"2023-07-18T09:04:44.55891Z"},"executionInfo":{"elapsed":22,"status":"aborted","timestamp":1689879900489,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"7xpye2JQYi1_","trusted":true},"outputs":[{"ename":"TypeError","evalue":"get_model() missing 2 required positional arguments: 'dropout_step' and 'drop_rate'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m config\u001b[39m=\u001b[39mCFG\n\u001b[0;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m get_model(\n\u001b[1;32m      4\u001b[0m     max_len\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mmax_len,\n\u001b[1;32m      5\u001b[0m     output_dim\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49moutput_dim,\n\u001b[1;32m      6\u001b[0m     dim\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mdim,\n\u001b[1;32m      7\u001b[0m     input_pad\u001b[39m=\u001b[39;49mConstants\u001b[39m.\u001b[39;49mINPUT_PAD,\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m experiment_id\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     11\u001b[0m saved_based_model \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/kaggle/input/weights/\u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39mcomment\u001b[39m}\u001b[39;00m\u001b[39m-exp\u001b[39m\u001b[39m{\u001b[39;00mexperiment_id\u001b[39m}\u001b[39;00m\u001b[39m-best.h5\u001b[39m\u001b[39m\"\u001b[39m\n","\u001b[0;31mTypeError\u001b[0m: get_model() missing 2 required positional arguments: 'dropout_step' and 'drop_rate'"]}],"source":["\n","config=CFG\n","\n","model = get_model(\n","    max_len=config.max_len,\n","    output_dim=config.output_dim,\n","    dim=config.dim,\n","    input_pad=Constants.INPUT_PAD,\n",")\n","experiment_id=0\n","\n","saved_based_model = f\"/kaggle/input/weights/{config.comment}-exp{experiment_id}-best.h5\"\n","#saved_based_model = f\"/kaggle/input/weights-from-12h-run/{config.comment}-exp{experiment_id}-best.h5\"\n","model.load_weights(saved_based_model)\n","print(f\"model with weights {saved_based_model}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:48.118699Z","iopub.status.busy":"2023-07-18T09:04:48.1183Z","iopub.status.idle":"2023-07-18T09:04:55.78829Z","shell.execute_reply":"2023-07-18T09:04:55.784098Z","shell.execute_reply.started":"2023-07-18T09:04:48.118667Z"},"executionInfo":{"elapsed":20,"status":"aborted","timestamp":1689879900489,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"jwxBqovnYi1_","trusted":true},"outputs":[],"source":["# Sanity Check\n","import json\n","with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n","    character_map = json.load(f)\n","rev_character_map = {j:i for i,j in character_map.items()}\n","\n","infer_keras_model=InferModel(model)\n","\n","main_dir = '/kaggle/input/asl-fingerspelling/'\n","path = f'{main_dir}train_landmarks/5414471.parquet'\n","cols=selected_columns(path)\n","df = pd.read_parquet(path, engine = 'auto', columns = cols)\n","seq_id=1816796431\n","seq=df.loc[seq_id]\n","data = seq[cols].to_numpy()\n","print(f'input shape: {data.shape}, dtype: {data.dtype}')\n","output = infer_keras_model(data)[\"outputs\"]\n","prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output, axis=1)])\n","\n","print(prediction_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:04:55.79275Z","iopub.status.busy":"2023-07-18T09:04:55.792139Z","iopub.status.idle":"2023-07-18T09:05:47.089729Z","shell.execute_reply":"2023-07-18T09:05:47.088743Z","shell.execute_reply.started":"2023-07-18T09:04:55.792699Z"},"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1689879900490,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"YQM7GMtfYi1_","trusted":true},"outputs":[],"source":["SAVED_MODEL_PATH=\"/kaggle/working/infer_model\"\n","\n","tf.saved_model.save(infer_keras_model,SAVED_MODEL_PATH)\n","keras_model_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_PATH)\n","keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","#keras_model_converter.target_spec.supported_types = [tf.float16]\n","#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n","#converter.allow_custom_ops=True\n","tflite_model = keras_model_converter.convert()\n","TFLITE_FILE_PATH=\"/kaggle/working/model.tflite\"\n","with open(TFLITE_FILE_PATH, \"wb\") as f:\n","    f.write(tflite_model)\n","\n","with open('/kaggle/working/inference_args.json', 'w') as f:\n","     json.dump({ 'selected_columns': cols }, f)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:05:47.092039Z","iopub.status.busy":"2023-07-18T09:05:47.091681Z","iopub.status.idle":"2023-07-18T09:05:47.257675Z","shell.execute_reply":"2023-07-18T09:05:47.255994Z","shell.execute_reply.started":"2023-07-18T09:05:47.092009Z"},"executionInfo":{"elapsed":18,"status":"aborted","timestamp":1689879900490,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"2KNUGXjBYi2A","trusted":true},"outputs":[],"source":["interpreter = tf.lite.Interpreter(TFLITE_FILE_PATH)\n","REQUIRED_SIGNATURE = \"serving_default\"\n","REQUIRED_OUTPUT = \"outputs\"\n","found_signatures = list(interpreter.get_signature_list().keys())\n","if REQUIRED_SIGNATURE not in found_signatures:\n","    print(\"Required input signature not found.\")\n","\n","prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n","output = prediction_fn(inputs=data)\n","prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n","print(prediction_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:05:47.26006Z","iopub.status.busy":"2023-07-18T09:05:47.25933Z","iopub.status.idle":"2023-07-18T09:05:48.830171Z","shell.execute_reply":"2023-07-18T09:05:48.828963Z","shell.execute_reply.started":"2023-07-18T09:05:47.260006Z"},"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1689879900490,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"aTeDBunuYi2A","trusted":true},"outputs":[],"source":["!zip submission.zip \"/kaggle/working/model.tflite\" \"/kaggle/working/inference_args.json\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:05:48.832628Z","iopub.status.busy":"2023-07-18T09:05:48.832209Z","iopub.status.idle":"2023-07-18T09:05:48.840337Z","shell.execute_reply":"2023-07-18T09:05:48.838608Z","shell.execute_reply.started":"2023-07-18T09:05:48.832593Z"},"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1689879900492,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"ZWcEOwClYi2A","trusted":true},"outputs":[],"source":["#!pip install /kaggle/input/tflite-wheels-2140/tflite_runtime_nightly-2.14.0.dev20230508-cp310-cp310-manylinux2014_x86_64.whl"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:05:48.84292Z","iopub.status.busy":"2023-07-18T09:05:48.842282Z","iopub.status.idle":"2023-07-18T09:05:48.857104Z","shell.execute_reply":"2023-07-18T09:05:48.856148Z","shell.execute_reply.started":"2023-07-18T09:05:48.842886Z"},"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1689879900492,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"6W11OFc7Yi2A","trusted":true},"outputs":[],"source":["\"\"\"\n","import json\n","import pandas as pd\n","import tflite_runtime.interpreter as tflite\n","import numpy as np\n","import time\n","from tqdm import tqdm\n","import Levenshtein as Lev\n","import glob\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-18T09:05:48.859108Z","iopub.status.busy":"2023-07-18T09:05:48.858802Z","iopub.status.idle":"2023-07-18T09:05:48.878044Z","shell.execute_reply":"2023-07-18T09:05:48.876595Z","shell.execute_reply.started":"2023-07-18T09:05:48.859082Z"},"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1689879900492,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"zoac5lq4Yi2B","trusted":true},"outputs":[],"source":["\"\"\"\n","SEL_FEATURES = json.load(open('/kaggle/working/inference_args.json'))['selected_columns']\n","\n","def load_relevant_data_subset(pq_path):\n","        return pd.read_parquet(pq_path, columns=SEL_FEATURES) #selected_columns)\n","\n","with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n","    character_map = json.load(f)\n","rev_character_map = {j:i for i,j in character_map.items()}\n","\n","\n","df = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\n","\n","idx = 0\n","sample = df.loc[idx]\n","loaded = load_relevant_data_subset('/kaggle/input/asl-fingerspelling/' + sample['path'])\n","loaded = loaded[loaded.index==sample['sequence_id']].values\n","print(loaded.shape)\n","frames = loaded\n","\n","def wer__(s1, s2):\n","    w1 = len(s1.split())\n","    lvd = Lev.distance(s1, s2)\n","    return lvd / w1\n","\n","interpreter = tflite.Interpreter('model.tflite')\n","found_signatures = list(interpreter.get_signature_list().keys())\n","\n","REQUIRED_SIGNATURE = 'serving_default'\n","REQUIRED_OUTPUT = 'outputs'\n","if REQUIRED_SIGNATURE not in found_signatures:\n","    raise KernelEvalException('Required input signature not found.')\n","\n","prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n","output_lite = prediction_fn(inputs=frames)\n","prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output_lite[REQUIRED_OUTPUT], axis=1)])\n","print(prediction_str)\n","\n","\n","st = time.time()\n","count=0\n","model_time = 0\n","\n","levs = []\n","\n","files=glob.glob('/kaggle/input/asl-fingerspelling/train_landmarks/*.parquet')\n","for f in files:\n","    df = load_relevant_data_subset(f)\n","    seq=df.index.drop_duplicates()\n","    for ind in tqdm(seq):\n","        loaded = df.loc[ind].values\n","        count+=1\n","        md_st = time.time()\n","        output_ = prediction_fn(inputs=loaded)\n","        out= output_[REQUIRED_OUTPUT]\n","        assert out.ndim==2\n","        assert out.shape[1]==59\n","        assert out.dtype==np.float32\n","        assert np.all(np.isfinite(out))\n","\n","        prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output_[REQUIRED_OUTPUT], axis=1)])\n","        model_time += time.time() - md_st\n","\n","        #cur_lev = wer__(sample['phrase'], prediction_str)\n","        #print(sample['phrase'], '|', prediction_str, '|', cur_lev)\n","        #print()\n","\n","        #levs.append(cur_lev)\n","\n","#print(f'WER: {np.mean(levs):.5f}')\n","print(f'Mean time: {(time.time() - st)/count:.2f}')\n","print(f'Mean time only infer: {model_time/count:.2f}')\n","\n","out=prediction_fn(inputs=np.empty(0,dtype=np.float32))[\"outputs\"]\n","print(out.shape,output_.dtype)\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1689879900493,"user":{"displayName":"Shai Ronen","userId":"11425665314369101818"},"user_tz":360},"id":"etCjY_x2Yi2B"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["Z-x7GxsSYi1y","eErQkrlpYi11","Vh1J_bDCYi13","MMEA9LflYi14","YQAPz56nYi15"],"gpuType":"T4","provenance":[{"file_id":"14U3Nc8k9OZX9B7knujs6fmnaLU76-VQh","timestamp":1689878620727},{"file_id":"https://github.com/sronen71/asl/blob/main/kaggle-try-again/asl-try-again.ipynb","timestamp":1689828702868}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
