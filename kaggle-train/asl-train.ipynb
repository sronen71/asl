{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENYLFLzurs3S"
      },
      "source": [
        "# For Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "agm4BN62kwwE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "IN_COLAB = 'COLAB_GPU' in os.environ\n",
        "if IN_COLAB:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "  drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TB_VhXDvhCT2"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /kaggle/working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Asq1MGhs41ZX",
        "outputId": "8d162085-b861-4603-f075-3a9e984d595c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-22 22:36:17.847123: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:8893] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-07-22 22:36:17.847147: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-07-22 22:36:17.847173: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-07-22 22:36:17.853217: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-22 22:36:18.616006: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/sronen/code/.venv/lib/python3.10/site-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /home/sronen/code/.venv/lib/python3.10/site-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "Not using TPU\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "tpu_strategy=None\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "\n",
        "except ValueError:\n",
        "  print(\"Not using TPU\")\n",
        "  #raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "\n",
        "#from tensorflow.python.framework.ops import disable_eager_execution\n",
        "#disable_eager_execution()  # LSTM layer can't use bfloat16 unless we do this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-x7GxsSYi1y"
      },
      "source": [
        "# Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoljCzzvYi1z",
        "outputId": "bcd5cad0-52be-41a7-dc75-07825ef1e51d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://packagecloud.io/github/git-lfs/pypi/simple\n",
            "Requirement already satisfied: tensorflow-addons in /home/sronen/code/.venv/lib/python3.10/site-packages (0.21.0)\n",
            "Requirement already satisfied: packaging in /home/sronen/code/.venv/lib/python3.10/site-packages (from tensorflow-addons) (23.0)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /home/sronen/code/.venv/lib/python3.10/site-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sronen/code/.venv/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "/home/sronen/code/.venv/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:37: UserWarning: You are currently using a nightly version of TensorFlow (2.14.0-dev20230718). \n",
            "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
            "If you encounter a bug, do not file an issue on GitHub.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install tensorflow-addons\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import psutil\n",
        "import gc\n",
        "import math\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UV3hsk-bYi1z"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-22 22:36:23.484635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-22 22:36:23.491836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-22 22:36:23.492015: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        }
      ],
      "source": [
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZpjjT02Yi10",
        "outputId": "f4cc9531-4099-4772-c46f-389f5d6f97a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow v2.14.0-dev20230718\n"
          ]
        }
      ],
      "source": [
        "print(\"TensorFlow v\" + tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0gUj7mAgYi10"
      },
      "outputs": [],
      "source": [
        "class MemoryUsageCallbackExtended(tf.keras.callbacks.Callback):\n",
        "    \"\"\"Monitor memory usage on epoch begin and end, collect garbage\"\"\"\n",
        "\n",
        "    # def on_epoch_begin(self, epoch, logs=None):\n",
        "    #    print(\"**Epoch {}**\".format(epoch))\n",
        "    #    print(\n",
        "    #        f\"Memory usage on epoch begin: {int(psutil.Process(os.getpid()).memory_info().rss)/1e9:.2f GB}\"\n",
        "    #    )\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(\n",
        "            f\"Memory usage on epoch end: {int(psutil.Process(os.getpid()).memory_info().rss)/1e9:.2f} GB\"\n",
        "        )\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eErQkrlpYi11"
      },
      "source": [
        "# Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "umV9OdZCYi11"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\"A LearningRateSchedule that uses a cosine decay with optional warmup.\n",
        "\n",
        "    See [Loshchilov & Hutter, ICLR2016](https://arxiv.org/abs/1608.03983),\n",
        "    SGDR: Stochastic Gradient Descent with Warm Restarts.\n",
        "\n",
        "    For the idea of a linear warmup of our learning rate,\n",
        "    see [Goyal et al.](https://arxiv.org/pdf/1706.02677.pdf).\n",
        "\n",
        "    When we begin training a model, we often want an initial increase in our\n",
        "    learning rate followed by a decay. If `warmup_target` is an int, this\n",
        "    schedule applies a linear increase per optimizer step to our learning rate\n",
        "    from `initial_learning_rate` to `warmup_target` for a duration of\n",
        "    `warmup_steps`. Afterwards, it applies a cosine decay function taking our\n",
        "    learning rate from `warmup_target` to `alpha` for a duration of\n",
        "    `decay_steps`. If `warmup_target` is None we skip warmup and our decay\n",
        "    will take our learning rate from `initial_learning_rate` to `alpha`.\n",
        "    It requires a `step` value to  compute the learning rate. You can\n",
        "    just pass a TensorFlow variable that you increment at each training step.\n",
        "\n",
        "    The schedule is a 1-arg callable that produces a warmup followed by a\n",
        "    decayed learning rate when passed the current optimizer step. This can be\n",
        "    useful for changing the learning rate value across different invocations of\n",
        "    optimizer functions.\n",
        "\n",
        "    Our warmup is computed as:\n",
        "\n",
        "    ```python\n",
        "    def warmup_learning_rate(step):\n",
        "        completed_fraction = step / warmup_steps\n",
        "        total_delta = target_warmup - initial_learning_rate\n",
        "        return completed_fraction * total_delta\n",
        "    ```\n",
        "\n",
        "    And our decay is computed as:\n",
        "\n",
        "    ```python\n",
        "    if warmup_target is None:\n",
        "        initial_decay_lr = initial_learning_rate\n",
        "    else:\n",
        "        initial_decay_lr = warmup_target\n",
        "\n",
        "    def decayed_learning_rate(step):\n",
        "        step = min(step, decay_steps)\n",
        "        cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))\n",
        "        decayed = (1 - alpha) * cosine_decay + alpha\n",
        "        return initial_decay_lr * decayed\n",
        "    ```\n",
        "\n",
        "    Example usage without warmup:\n",
        "\n",
        "    ```python\n",
        "    decay_steps = 1000\n",
        "    initial_learning_rate = 0.1\n",
        "    lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
        "        initial_learning_rate, decay_steps)\n",
        "    ```\n",
        "\n",
        "    Example usage with warmup:\n",
        "\n",
        "    ```python\n",
        "    decay_steps = 1000\n",
        "    initial_learning_rate = 0\n",
        "    warmup_steps = 1000\n",
        "    target_learning_rate = 0.1\n",
        "    lr_warmup_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
        "        initial_learning_rate, decay_steps, warmup_target=target_learning_rate,\n",
        "        warmup_steps=warmup_steps\n",
        "    )\n",
        "    ```\n",
        "\n",
        "    You can pass this schedule directly into a `tf.keras.optimizers.Optimizer`\n",
        "    as the learning rate. The learning rate schedule is also serializable and\n",
        "    deserializable using `tf.keras.optimizers.schedules.serialize` and\n",
        "    `tf.keras.optimizers.schedules.deserialize`.\n",
        "\n",
        "    Returns:\n",
        "      A 1-arg callable learning rate schedule that takes the current optimizer\n",
        "      step and outputs the decayed learning rate, a scalar `Tensor` of the same\n",
        "      type as `initial_learning_rate`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        initial_learning_rate,\n",
        "        decay_steps,\n",
        "        alpha=0.0,\n",
        "        name=None,\n",
        "        warmup_target=None,\n",
        "        warmup_steps=0,\n",
        "    ):\n",
        "        \"\"\"Applies cosine decay to the learning rate.\n",
        "\n",
        "        Args:\n",
        "          initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a\n",
        "            Python int. The initial learning rate.\n",
        "          decay_steps: A scalar `int32` or `int64` `Tensor` or a Python int.\n",
        "            Number of steps to decay over.\n",
        "          alpha: A scalar `float32` or `float64` `Tensor` or a Python int.\n",
        "            Minimum learning rate value for decay as a fraction of\n",
        "            `initial_learning_rate`.\n",
        "          name: String. Optional name of the operation.  Defaults to\n",
        "            'CosineDecay'.\n",
        "          warmup_target: None or a scalar `float32` or `float64` `Tensor` or a\n",
        "            Python int. The target learning rate for our warmup phase. Will cast\n",
        "            to the `initial_learning_rate` datatype. Setting to None will skip\n",
        "            warmup and begins decay phase from `initial_learning_rate`.\n",
        "            Otherwise scheduler will warmup from `initial_learning_rate` to\n",
        "            `warmup_target`.\n",
        "          warmup_steps: A scalar `int32` or `int64` `Tensor` or a Python int.\n",
        "            Number of steps to warmup over.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.initial_learning_rate = initial_learning_rate\n",
        "        self.decay_steps = decay_steps\n",
        "        self.alpha = alpha\n",
        "        self.name = name\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.warmup_target = warmup_target\n",
        "\n",
        "    def _decay_function(self, step, decay_steps, decay_from_lr, dtype):\n",
        "        with tf.name_scope(self.name or \"CosineDecay\"):\n",
        "            completed_fraction = step / decay_steps\n",
        "            tf_pi = tf.constant(math.pi, dtype=dtype)\n",
        "            cosine_decayed = 0.5 * (1.0 + tf.cos(tf_pi * completed_fraction))\n",
        "            decayed = (1 - self.alpha) * cosine_decayed + self.alpha\n",
        "            return tf.multiply(decay_from_lr, decayed)\n",
        "\n",
        "    def _warmup_function(self, step, warmup_steps, warmup_target, initial_learning_rate):\n",
        "        with tf.name_scope(self.name or \"CosineDecay\"):\n",
        "            completed_fraction = step / warmup_steps\n",
        "            total_step_delta = warmup_target - initial_learning_rate\n",
        "            return total_step_delta * completed_fraction + initial_learning_rate\n",
        "\n",
        "    def __call__(self, step):\n",
        "        with tf.name_scope(self.name or \"CosineDecay\"):\n",
        "            initial_learning_rate = tf.convert_to_tensor(\n",
        "                self.initial_learning_rate, name=\"initial_learning_rate\"\n",
        "            )\n",
        "            dtype = initial_learning_rate.dtype\n",
        "            decay_steps = tf.cast(self.decay_steps, dtype)\n",
        "            global_step_recomp = tf.cast(step, dtype)\n",
        "\n",
        "            if self.warmup_target is None:\n",
        "                global_step_recomp = tf.minimum(global_step_recomp, decay_steps)\n",
        "                return self._decay_function(\n",
        "                    global_step_recomp,\n",
        "                    decay_steps,\n",
        "                    initial_learning_rate,\n",
        "                    dtype,\n",
        "                )\n",
        "\n",
        "            warmup_target = tf.cast(self.warmup_target, dtype)\n",
        "            warmup_steps = tf.cast(self.warmup_steps, dtype)\n",
        "\n",
        "            global_step_recomp = tf.minimum(global_step_recomp, decay_steps + warmup_steps)\n",
        "\n",
        "            return tf.cond(\n",
        "                global_step_recomp < warmup_steps,\n",
        "                lambda: self._warmup_function(\n",
        "                    global_step_recomp,\n",
        "                    warmup_steps,\n",
        "                    warmup_target,\n",
        "                    initial_learning_rate,\n",
        "                ),\n",
        "                lambda: self._decay_function(\n",
        "                    global_step_recomp - warmup_steps,\n",
        "                    decay_steps,\n",
        "                    warmup_target,\n",
        "                    dtype,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"initial_learning_rate\": self.initial_learning_rate,\n",
        "            \"decay_steps\": self.decay_steps,\n",
        "            \"alpha\": self.alpha,\n",
        "            \"name\": self.name,\n",
        "            \"warmup_target\": self.warmup_target,\n",
        "            \"warmup_steps\": self.warmup_steps,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh1J_bDCYi13"
      },
      "source": [
        "# Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5icpG0BeYi13"
      },
      "outputs": [],
      "source": [
        "def get_char_dict():\n",
        "    char_dict = {\n",
        "        \" \": 0,\n",
        "        \"!\": 1,\n",
        "        \"#\": 2,\n",
        "        \"$\": 3,\n",
        "        \"%\": 4,\n",
        "        \"&\": 5,\n",
        "        \"'\": 6,\n",
        "        \"(\": 7,\n",
        "        \")\": 8,\n",
        "        \"*\": 9,\n",
        "        \"+\": 10,\n",
        "        \",\": 11,\n",
        "        \"-\": 12,\n",
        "        \".\": 13,\n",
        "        \"/\": 14,\n",
        "        \"0\": 15,\n",
        "        \"1\": 16,\n",
        "        \"2\": 17,\n",
        "        \"3\": 18,\n",
        "        \"4\": 19,\n",
        "        \"5\": 20,\n",
        "        \"6\": 21,\n",
        "        \"7\": 22,\n",
        "        \"8\": 23,\n",
        "        \"9\": 24,\n",
        "        \":\": 25,\n",
        "        \";\": 26,\n",
        "        \"=\": 27,\n",
        "        \"?\": 28,\n",
        "        \"@\": 29,\n",
        "        \"[\": 30,\n",
        "        \"_\": 31,\n",
        "        \"a\": 32,\n",
        "        \"b\": 33,\n",
        "        \"c\": 34,\n",
        "        \"d\": 35,\n",
        "        \"e\": 36,\n",
        "        \"f\": 37,\n",
        "        \"g\": 38,\n",
        "        \"h\": 39,\n",
        "        \"i\": 40,\n",
        "        \"j\": 41,\n",
        "        \"k\": 42,\n",
        "        \"l\": 43,\n",
        "        \"m\": 44,\n",
        "        \"n\": 45,\n",
        "        \"o\": 46,\n",
        "        \"p\": 47,\n",
        "        \"q\": 48,\n",
        "        \"r\": 49,\n",
        "        \"s\": 50,\n",
        "        \"t\": 51,\n",
        "        \"u\": 52,\n",
        "        \"v\": 53,\n",
        "        \"w\": 54,\n",
        "        \"x\": 55,\n",
        "        \"y\": 56,\n",
        "        \"z\": 57,\n",
        "        \"~\": 58,\n",
        "    }\n",
        "    char_dict[\"P\"] = 59\n",
        "    #char_dict[\"SOS\"] = 60\n",
        "    #char_dict[\"EOS\"] = 61\n",
        "    return char_dict\n",
        "\n",
        "\n",
        "class Constants:\n",
        "    ROWS_PER_FRAME = 543\n",
        "    MAX_STRING_LEN = 50\n",
        "    INPUT_PAD = -100.0\n",
        "    char_dict = get_char_dict()\n",
        "    LABEL_PAD = char_dict[\"P\"]\n",
        "    inv_dict = {v: k for k, v in char_dict.items()}\n",
        "    NOSE = [1, 2, 98, 327]\n",
        "\n",
        "    REYE = [33, 7, 163, 144, 145, 153, 154, 155, 133, 246, 161, 160, 159, 158, 157, 173]\n",
        "    LEYE = [263, 249, 390, 373, 374, 380, 381, 382, 362, 466, 388, 387, 386, 385, 384, 398]\n",
        "\n",
        "    LHAND = list(range(468, 489))\n",
        "    RHAND = list(range(522, 543))\n",
        "\n",
        "    LNOSE = [98]\n",
        "    RNOSE = [327]\n",
        "\n",
        "    LLIP = [84, 181, 91, 146, 61, 185, 40, 39, 37, 87, 178, 88, 95, 78, 191, 80, 81, 82]\n",
        "    RLIP = [\n",
        "        314,\n",
        "        405,\n",
        "        321,\n",
        "        375,\n",
        "        291,\n",
        "        409,\n",
        "        270,\n",
        "        269,\n",
        "        267,\n",
        "        317,\n",
        "        402,\n",
        "        318,\n",
        "        324,\n",
        "        308,\n",
        "        415,\n",
        "        310,\n",
        "        311,\n",
        "        312,\n",
        "    ]\n",
        "    POSE = [500, 502, 504, 501, 503, 505, 512, 513]\n",
        "    LPOSE = [513, 505, 503, 501]\n",
        "    RPOSE = [512, 504, 502, 500]\n",
        "\n",
        "    POINT_LANDMARKS_PARTS = [LHAND, RHAND, LLIP, RLIP, LPOSE, RPOSE, NOSE, REYE, LEYE]\n",
        "    # POINT_LANDMARKS_PARTS = [LHAND, RHAND, NOSE]\n",
        "    POINT_LANDMARKS = [item for sublist in POINT_LANDMARKS_PARTS for item in sublist]\n",
        "    parts = {\n",
        "        \"LLIP\": LLIP,\n",
        "        \"RLIP\": RLIP,\n",
        "        \"LHAND\": LHAND,\n",
        "        \"RHAND\": RHAND,\n",
        "        \"LPOSE\": LPOSE,\n",
        "        \"RPOSE\": RPOSE,\n",
        "        \"LNOSE\": LNOSE,\n",
        "        \"RNOSE\": RNOSE,\n",
        "        \"REYE\": REYE,\n",
        "        \"LEYE\": LEYE,\n",
        "    }\n",
        "\n",
        "    LANDMARK_INDICES = {}  # type: ignore\n",
        "    for part in parts:\n",
        "        LANDMARK_INDICES[part] = []\n",
        "        for landmark in parts[part]:\n",
        "            if landmark in POINT_LANDMARKS:\n",
        "                LANDMARK_INDICES[part].append(POINT_LANDMARKS.index(landmark))\n",
        "\n",
        "    CENTER_LANDMARKS = LNOSE + RNOSE\n",
        "    CENTER_INDICES = LANDMARK_INDICES[\"LNOSE\"] + LANDMARK_INDICES[\"RNOSE\"]\n",
        "\n",
        "    NUM_NODES = len(POINT_LANDMARKS)\n",
        "    NUM_INPUT_FEATURES = 2 * NUM_NODES # (x,y)\n",
        "    CHANNELS = 6 * NUM_NODES #(x,y,dx,dy,dx2,dy2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMEA9LflYi14"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "P-kprZ1_Yi14"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Seed all random number generators\n",
        "def seed_everything(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "def selected_columns(file_example):\n",
        "    df = pd.read_parquet(file_example)\n",
        "    selected_x = df.columns[[x + 1 for x in Constants.POINT_LANDMARKS]].tolist()\n",
        "    selected_y = [c.replace(\"x\", \"y\") for c in selected_x]\n",
        "    selected = []\n",
        "    for i in range(Constants.NUM_NODES):\n",
        "        selected.append(selected_x[i])\n",
        "        selected.append(selected_y[i])\n",
        "    return selected  # x1,y1,x2,y2,...\n",
        "\n",
        "\n",
        "\n",
        "def num_to_char_fn(y):\n",
        "    return [Constants.inv_dict.get(x, \"\") for x in y]\n",
        "\n",
        "\n",
        "# A callback class to output a few transcriptions during training\n",
        "class CallbackEval(tf.keras.callbacks.Callback):\n",
        "    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n",
        "\n",
        "    def __init__(self, model, dataset):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.model = model\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs=None):\n",
        "        predictions = []\n",
        "        targets = []\n",
        "        for batch in self.dataset:\n",
        "            X, y = batch\n",
        "            batch_predictions = self.model(X)\n",
        "            batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "            predictions.extend(batch_predictions)\n",
        "            for label in y:\n",
        "                label = \"\".join(num_to_char_fn(label.numpy()))\n",
        "                targets.append(label)\n",
        "        print(\"-\" * 100)\n",
        "        # for i in np.random.randint(0, len(predictions), 2):\n",
        "        for i in range(10):\n",
        "            print(f\"Target    : {targets[i]}\")\n",
        "            print(f\"Prediction: {predictions[i]}, len: {len(predictions[i])}\")\n",
        "            print(\"-\" * 100)\n",
        "\n",
        "\n",
        "def decode_phrase(pred):\n",
        "    # decode cts prediction by prunning\n",
        "    # (T,CHAR_NUMS)\n",
        "    x = tf.argmax(pred, axis=1)\n",
        "    paddings = tf.constant(\n",
        "        [\n",
        "            [0, 1],\n",
        "        ]\n",
        "    )\n",
        "    x = tf.pad(x, paddings)\n",
        "    diff = tf.not_equal(x[:-1], x[1:])\n",
        "    adjacent_indices = tf.where(diff)[:, 0]\n",
        "    x = tf.gather(x, adjacent_indices)\n",
        "    mask = x != Constants.LABEL_PAD\n",
        "    x = tf.boolean_mask(x, mask, axis=0)\n",
        "    return x\n",
        "\n",
        "\n",
        "# A utility function to decode the output of the network\n",
        "def decode_batch_predictions(pred):\n",
        "    output_text = []\n",
        "    for result in pred:\n",
        "        result = \"\".join(num_to_char_fn(decode_phrase(result).numpy()))\n",
        "        output_text.append(result)\n",
        "    return output_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def code_to_label(label_code):\n",
        "    label = [Constants.inv_dict[x] for x in label_code if Constants.inv_dict[x] != \"P\"]\n",
        "    label = \"\".join(label)\n",
        "    return label\n",
        "\n",
        "\n",
        "def convert_to_strings(batch_label_code):\n",
        "    output = []\n",
        "    for label_code in batch_label_code:\n",
        "        output.append(code_to_label(label_code))\n",
        "    return output\n",
        "\n",
        "\n",
        "def global_metric(val_ds, model):\n",
        "    global_N, global_D = 0, 0\n",
        "    count = 0\n",
        "    metric = LevDistanceMetric()\n",
        "    for batch in val_ds:\n",
        "        count += 1\n",
        "        print(count)\n",
        "        feature, label = batch\n",
        "        logits = model(feature)\n",
        "        _, _, D = batch_edit_distance(label, logits)\n",
        "        metric.update_state(label, logits)\n",
        "\n",
        "    result = metric.result().numpy()\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def sparse_from_dense_ignore_value(dense_tensor):\n",
        "    mask = tf.not_equal(dense_tensor, Constants.LABEL_PAD)\n",
        "    indices = tf.where(mask)\n",
        "    values = tf.boolean_mask(dense_tensor, mask)\n",
        "\n",
        "    return tf.SparseTensor(indices, values, tf.shape(dense_tensor, out_type=tf.int64))\n",
        "\n",
        "\n",
        "def batch_edit_distance(y_true, y_logits):\n",
        "    blank = Constants.LABEL_PAD\n",
        "    #y_true=tf.ensure_shape(y_true,(128,Constants.MAX_STRING_LEN))\n",
        "    #y_logits=tf.ensure_shape(y_logits,(128,128,60))\n",
        "    #tf.print(\"edit distance true shape\",tf.shape(y_true))\n",
        "    #tf.print(\"edit distance logits shape\",tf.shape(y_logits))\n",
        "\n",
        "    B = tf.shape(y_logits)[0]\n",
        "    seq_length = tf.shape(y_logits)[1]\n",
        "    to_decode = tf.transpose(y_logits, perm=[1, 0, 2])\n",
        "    sequence_length = tf.fill(dims=[B], value=seq_length)\n",
        "    hypothesis = tf.nn.ctc_greedy_decoder(\n",
        "        tf.cast(to_decode, tf.float32), sequence_length, blank_index=blank\n",
        "    )[0][\n",
        "        0\n",
        "    ]  # full is [B,...]\n",
        "    truth = sparse_from_dense_ignore_value(y_true)  # full is [B,...]\n",
        "    truth = tf.cast(truth, tf.int64)\n",
        "    edit_dist = tf.edit_distance(hypothesis, truth, normalize=False)\n",
        "\n",
        "    non_ignore_mask = tf.not_equal(y_true, blank)\n",
        "    N = tf.reduce_sum(tf.cast(non_ignore_mask, tf.float32))\n",
        "    D = tf.reduce_sum(edit_dist)\n",
        "    result = (N - D) / N\n",
        "    result = tf.clip_by_value(result, 0.0, 1.0)\n",
        "    return result, N, D\n",
        "\n",
        "\n",
        "class LevDistanceMetric(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name=\"Lev\", **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.distance = self.add_weight(name=\"dist\", initializer=\"zeros\")\n",
        "        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
        "\n",
        "    def update_state(self, y_true, y_logits, sample_weight=None):\n",
        "        # if using with keras compile, make sure the model outputs logits, not softmax probabilities\n",
        "        _, N, D = batch_edit_distance(y_true, y_logits)\n",
        "        self.distance.assign_add(D)\n",
        "        self.count.assign_add(N)\n",
        "\n",
        "    def result(self):\n",
        "        result = (self.count - self.distance) / self.count\n",
        "        result = tf.clip_by_value(result, 0.0, 1.0)\n",
        "        return result\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.count.assign(0.0)\n",
        "        self.distance.assign(0.0)\n",
        "\n",
        "\n",
        "\n",
        "class SWA(tf.keras.callbacks.Callback):\n",
        "    # Stochastic Weight Averaging\n",
        "    def __init__(\n",
        "        self,\n",
        "        save_name,\n",
        "        swa_epochs=[],\n",
        "        strategy=None,\n",
        "        train_ds=None,\n",
        "        valid_ds=None,\n",
        "        train_steps=1000,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.swa_epochs = swa_epochs\n",
        "        self.swa_weights = None\n",
        "        self.save_name = save_name\n",
        "        self.train_ds = train_ds\n",
        "        self.valid_ds = valid_ds\n",
        "        self.train_steps = train_steps\n",
        "        self.strategy = strategy\n",
        "\n",
        "    # @tf.function(jit_compile=True)\n",
        "    def train_step(self, iterator):\n",
        "        \"\"\"The step function for one training step.\"\"\"\n",
        "\n",
        "        def step_fn(inputs):\n",
        "            \"\"\"The computation to run on each device.\"\"\"\n",
        "            x, y = inputs\n",
        "            _ = self.model(x, training=True)\n",
        "\n",
        "        for x in iterator:\n",
        "            self.strategy.run(step_fn, args=(x,))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch in self.swa_epochs:\n",
        "            if self.swa_weights is None:\n",
        "                self.swa_weights = self.model.get_weights()\n",
        "            else:\n",
        "                w = self.model.get_weights()\n",
        "                for i in range(len(self.swa_weights)):\n",
        "                    self.swa_weights[i] += w[i]\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if len(self.swa_epochs):\n",
        "            print(\"applying SWA...\")\n",
        "            for i in range(len(self.swa_weights)):\n",
        "                self.swa_weights[i] = self.swa_weights[i] / len(self.swa_epochs)\n",
        "            self.model.set_weights(self.swa_weights)\n",
        "            if self.train_ds is not None:  # for the re-calculation of running mean and var\n",
        "                self.train_step(self.train_ds.take(self.train_steps))\n",
        "            print(f\"save SWA weights to {self.save_name}-SWA.h5\")\n",
        "            self.model.save_weights(f\"{self.save_name}-SWA.h5\")\n",
        "            if self.valid_ds is not None:\n",
        "                self.model.evaluate(self.valid_ds)\n",
        "\n",
        "\n",
        "class AWP(tf.keras.Model):\n",
        "    # Adversarial Weight Perturbation\n",
        "    def __init__(self, *args, delta=0.1, eps=1e-4, start_step=0, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.delta = delta\n",
        "        self.eps = eps\n",
        "        self.start_step = start_step\n",
        "\n",
        "    def train_step_awp(self, data):\n",
        "        # Unpack the data. Its structure depends on your model and\n",
        "        # on what you pass to `fit()`.\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "        params = self.trainable_variables\n",
        "        params_gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        for i in range(len(params_gradients)):\n",
        "            grad = tf.zeros_like(params[i]) + params_gradients[i]\n",
        "            delta = tf.math.divide_no_nan(\n",
        "                self.delta * grad, tf.math.sqrt(tf.reduce_sum(grad**2)) + self.eps\n",
        "            )\n",
        "            self.trainable_variables[i].assign_add(delta)\n",
        "        with tf.GradientTape() as tape2:\n",
        "            y_pred = self(x, training=True)\n",
        "            new_loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "            if hasattr(self.optimizer, \"get_scaled_loss\"):\n",
        "                new_loss = self.optimizer.get_scaled_loss(new_loss)\n",
        "\n",
        "        gradients = tape2.gradient(new_loss, self.trainable_variables)\n",
        "        if hasattr(self.optimizer, \"get_unscaled_gradients\"):\n",
        "            gradients = self.optimizer.get_unscaled_gradients(gradients)\n",
        "        for i in range(len(params_gradients)):\n",
        "            grad = tf.zeros_like(params[i]) + params_gradients[i]\n",
        "            delta = tf.math.divide_no_nan(\n",
        "                self.delta * grad, tf.math.sqrt(tf.reduce_sum(grad**2)) + self.eps\n",
        "            )\n",
        "            self.trainable_variables[i].assign_sub(delta)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        # self_loss.update_state(loss)\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def train_step(self, data):\n",
        "        return tf.cond(\n",
        "            self._train_counter < self.start_step,\n",
        "            lambda: super(AWP, self).train_step(data),\n",
        "            lambda: self.train_step_awp(data),\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQAPz56nYi15"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wnoZyLAcYi15"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CTCLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, pad_token_idx,batch_size,max_string_len,output_dim,output_steps,replicas):\n",
        "        self.pad_token_idx = pad_token_idx\n",
        "        self.batch_size=batch_size\n",
        "        self.max_string_len=max_string_len\n",
        "        self.output_steps=output_steps\n",
        "        self.output_dim=output_dim\n",
        "        self.replicas=replicas\n",
        "        super().__init__()\n",
        "\n",
        "    def call(self, labels, logits):\n",
        "\n",
        "        #logits=tf.ensure_shape(logits,(self.batch_size//self.replicas,self.output_steps,self.output_dim))\n",
        "        #labels=tf.ensure_shape(labels,(self.batch_size//self.replicas,self.max_string_len))\n",
        "        label_length = tf.reduce_sum(tf.cast(labels != self.pad_token_idx, tf.int32), axis=-1)\n",
        "        logit_length = tf.ones(tf.shape(logits)[0], dtype=tf.int32) * tf.shape(logits)[1]\n",
        "\n",
        "        ctc_loss = tf.nn.ctc_loss(\n",
        "            labels=labels,\n",
        "            logits=logits,\n",
        "            label_length=label_length,\n",
        "            logit_length=logit_length,\n",
        "            blank_index=self.pad_token_idx,\n",
        "            logits_time_major=False,\n",
        "        )\n",
        "\n",
        "        return ctc_loss\n",
        "\n",
        "\n",
        "class ECA(tf.keras.layers.Layer):\n",
        "    # Efficient Channel Attention\n",
        "    def __init__(self, kernel_size=5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.kernel_size = kernel_size\n",
        "        self.conv = tf.keras.layers.Conv1D(\n",
        "            1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n",
        "        nn = tf.expand_dims(nn, -1)\n",
        "        nn = self.conv(nn)\n",
        "        nn = tf.squeeze(nn, -1)\n",
        "        nn = tf.nn.sigmoid(nn)\n",
        "        nn = nn[:, None, :]\n",
        "        return inputs * nn\n",
        "\n",
        "\n",
        "class LateDropout(tf.keras.layers.Layer):\n",
        "    def __init__(self, rate, noise_shape=None, start_step=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.rate = rate\n",
        "        self.start_step = start_step\n",
        "        self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super().build(input_shape)\n",
        "        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n",
        "        self._train_counter = tf.Variable(0, dtype=\"int64\", aggregation=agg, trainable=False)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = tf.cond(\n",
        "            self._train_counter < self.start_step,\n",
        "            lambda: inputs,\n",
        "            lambda: self.dropout(inputs, training=training),\n",
        "        )\n",
        "        if training:\n",
        "            self._train_counter.assign_add(1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CausalDWConv1D(tf.keras.layers.Layer):\n",
        "    # Causal Depth Wise Convolution\n",
        "    def __init__(\n",
        "        self,\n",
        "        kernel_size=17,\n",
        "        dilation_rate=1,\n",
        "        use_bias=False,\n",
        "        depthwise_initializer=\"glorot_uniform\",\n",
        "        name=\"\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.causal_pad = tf.keras.layers.ZeroPadding1D(\n",
        "            (dilation_rate * (kernel_size - 1), 0), name=name + \"_pad\"\n",
        "        )\n",
        "        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n",
        "            kernel_size,\n",
        "            strides=1,\n",
        "            dilation_rate=dilation_rate,\n",
        "            padding=\"valid\",\n",
        "            use_bias=use_bias,\n",
        "            depthwise_initializer=depthwise_initializer,\n",
        "            name=name + \"_dwconv\",\n",
        "        )\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.causal_pad(inputs)\n",
        "        x = self.dw_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def Conv1DBlock(\n",
        "    channel_size,\n",
        "    kernel_size,\n",
        "    dilation_rate=1,\n",
        "    drop_rate=0.0,\n",
        "    expand_ratio=2,\n",
        "    # se_ratio=0.25,\n",
        "    activation=\"swish\",\n",
        "    name=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    efficient conv1d block, @hoyso48\n",
        "    \"\"\"\n",
        "    if name is None:\n",
        "        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n",
        "\n",
        "    # Expansion phase\n",
        "    def apply(inputs):\n",
        "        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n",
        "        channels_expand = channels_in * expand_ratio\n",
        "\n",
        "        skip = inputs\n",
        "\n",
        "        x = tf.keras.layers.Dense(\n",
        "            channels_expand, use_bias=True, activation=activation, name=name + \"_expand_conv\"\n",
        "        )(inputs)\n",
        "\n",
        "        # Depthwise Convolution\n",
        "        x = CausalDWConv1D(\n",
        "            kernel_size, dilation_rate=dilation_rate, use_bias=False, name=name + \"_dwconv\"\n",
        "        )(x)\n",
        "\n",
        "        #x = tf.keras.layers.LayerNormalization(name=name + \"_bn\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization(name=name + \"_bn\")(x)\n",
        "\n",
        "        x = ECA()(x)  # efficient channel attention\n",
        "\n",
        "        x = tf.keras.layers.Dense(channel_size, use_bias=True, name=name + \"_project_conv\")(x)\n",
        "\n",
        "        if drop_rate > 0:\n",
        "            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + \"_drop\")(x)\n",
        "\n",
        "        if channels_in == channel_size:\n",
        "            x = tf.keras.layers.add([x, skip], name=name + \"_add\")\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.scale = self.dim**-0.5\n",
        "        self.num_heads = num_heads\n",
        "        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n",
        "        self.drop1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        qkv = self.qkv(inputs)\n",
        "        qkv = tf.keras.layers.Permute((2, 1, 3))(\n",
        "            tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv)\n",
        "        )\n",
        "        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n",
        "\n",
        "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask[:, None, None, :]\n",
        "\n",
        "        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n",
        "        attn = self.drop1(attn)\n",
        "\n",
        "        x = attn @ v\n",
        "        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def TransformerBlock(\n",
        "    dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation=\"swish\"\n",
        "):\n",
        "    def apply(inputs):\n",
        "        x = inputs\n",
        "        x = tf.keras.layers.LayerNormalization()(x)\n",
        "        x = MultiHeadSelfAttention(dim=dim, num_heads=num_heads, dropout=attn_dropout)(x)\n",
        "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1))(x)\n",
        "        x = tf.keras.layers.Add()([inputs, x])\n",
        "        attn_out = x\n",
        "\n",
        "        x = tf.keras.layers.LayerNormalization()(x)\n",
        "        x = tf.keras.layers.Dense(dim * expand, use_bias=False, activation=activation)(x)\n",
        "        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n",
        "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1))(x)\n",
        "        x = tf.keras.layers.Add()([attn_out, x])\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "def build_model1(\n",
        "    output_dim,\n",
        "    max_len=64,\n",
        "    dropout_step=0,\n",
        "    dim=192,\n",
        "    input_pad=-100,\n",
        "    with_transformer=False,\n",
        "    drop_rate=0.2,\n",
        "):\n",
        "    inp = tf.keras.Input(shape=(max_len, Constants.CHANNELS), dtype=tf.float32, name=\"inputs\")\n",
        "    x = tf.keras.layers.Masking(mask_value=input_pad, input_shape=(max_len, Constants.CHANNELS))(\n",
        "        inp\n",
        "    )\n",
        "    ksize = 17\n",
        "    x = tf.keras.layers.Dense(dim, use_bias=False, name=\"stem_conv\")(x)\n",
        "    #x = tf.keras.layers.LayerNormalization(name=\"stem_bn\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization(name=\"stem_bn\")(x)\n",
        "\n",
        "    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "    if with_transformer:\n",
        "        x = TransformerBlock(dim, expand=2)(x)\n",
        "\n",
        "    #x = tf.keras.layers.AvgPool1D(2, 2)(x)\n",
        "\n",
        "\n",
        "    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "    if with_transformer:\n",
        "        x = TransformerBlock(dim, expand=2)(x)\n",
        "\n",
        "    x = tf.keras.layers.AvgPool1D(2, 2)(x) # [B,T,dim]\n",
        "\n",
        "    lstm2 = tf.keras.layers.LSTM(units=dim//2, return_sequences=True,dtype=\"float32\") #[B,T,dim//2]\n",
        "    x2 = tf.keras.layers.Bidirectional(lstm2)(x) #[B,T,dim]\n",
        "\n",
        "    x2=tf.keras.layers.BatchNormalization()(x2)\n",
        "    x2=tf.keras.layers.Dense(output_dim)(x2)\n",
        "    soft=tf.keras.layers.Activation('softmax', dtype='float32')(x2)\n",
        "    logsoft=tf.keras.layers.Activation('log_softmax',dtype='float32',name=\"internal\")(x2)\n",
        "\n",
        "    x=tf.keras.layers.Dense(dim)(soft)+x\n",
        "    x=tf.keras.layers.BatchNormalization()(x)\n",
        "    if dim == 384:  # for the 4x sized model\n",
        "        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "        if with_transformer:\n",
        "            x = TransformerBlock(dim, expand=2)(x)\n",
        "\n",
        "        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "        if with_transformer:\n",
        "            x = TransformerBlock(dim, expand=2)(x)\n",
        "\n",
        "\n",
        "    lstm = tf.keras.layers.LSTM(units=dim//2, return_sequences=True,dtype=\"float32\")\n",
        "    x = tf.keras.layers.Bidirectional(lstm)(x)\n",
        "\n",
        "    x = LateDropout(0.6, start_step=dropout_step)(x)\n",
        "\n",
        "    # x = tf.keras.layers.LayerNormalization()(x)\n",
        "\n",
        "    #output = tf.keras.layers.Dense(output_dim, activation=\"log_softmax\",name=\"final_logsoft\")(x)  # logits\n",
        "    x=tf.keras.layers.Dense(output_dim)(x)\n",
        "    output = tf.keras.layers.Activation(\"log_softmax\",name=\"final\",dtype=\"float32\")(x)  # logits\n",
        "\n",
        "    model = tf.keras.Model(inp, outputs=[output,logsoft])\n",
        "    #model = tf.keras.Model(inp, outputs=output)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_model(output_dim, max_len, dim, input_pad,dropout_step=0,drop_rate=0.):\n",
        "\n",
        "    model = build_model1(output_dim, max_len=max_len, input_pad=input_pad, dim=dim,  dropout_step=dropout_step,drop_rate=drop_rate)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piKWNcVHYi15"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7e1LGyFWYi16"
      },
      "outputs": [],
      "source": [
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def get_strategy():\n",
        "    logical_devices = tf.config.list_logical_devices()\n",
        "    # Check if TPU is available\n",
        "\n",
        "    gpu_available = any(\"GPU\" in device.name for device in logical_devices)\n",
        "    strategy = None\n",
        "    is_tpu = False\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        print(\"Running on TPU \", tpu.master())\n",
        "        is_tpu = True\n",
        "    except ValueError:\n",
        "        is_tpu = False\n",
        "\n",
        "    if is_tpu:\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "\n",
        "        print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "        strategy = tf.distribute.TPUStrategy(tpu)\n",
        "        #disable_eager_execution()  # LSTM layer can't use bfloat16 unless we do this.\n",
        "\n",
        "    else:\n",
        "        if gpu_available:\n",
        "\n",
        "            ngpu = len(gpus)\n",
        "            print(\"Num GPUs Available: \", ngpu)\n",
        "            if ngpu > 1:\n",
        "                strategy = tf.distribute.MirroredStrategy()\n",
        "            else:\n",
        "                strategy = tf.distribute.get_strategy()\n",
        "\n",
        "        else:\n",
        "            print(\"Runing on CPU\")\n",
        "            strategy = tf.distribute.get_strategy()\n",
        "    replicas = strategy.num_replicas_in_sync\n",
        "\n",
        "    print(f\"get strategy replicas: {replicas}\")\n",
        "\n",
        "    return strategy, replicas, is_tpu\n",
        "\n",
        "\n",
        "class CFG:\n",
        "    # These 3 variables are update dynamically later by calling update_config_with_strategy.\n",
        "    strategy = None  # type: ignore\n",
        "    replicas = 1\n",
        "    is_tpu = False\n",
        "\n",
        "    save_output = True\n",
        "    input_path = \"/kaggle/input\"\n",
        "    output_path = \"/kaggle/working\"\n",
        "\n",
        "    seed = 42\n",
        "    verbose = 1  # 0) silent 1) progress bar 2) one line per epoch\n",
        "\n",
        "    # max number of frames\n",
        "    #max_len = 256\n",
        "    max_len = 256\n",
        "    replicas = 1\n",
        "\n",
        "    lr = 3e-4   # 5e-4\n",
        "    weight_decay = 1e-4  # 4e-4\n",
        "    epochs = 300\n",
        "\n",
        "    batch_size=128\n",
        "\n",
        "    snapshot_epochs = []  # type: ignore\n",
        "    swa_epochs = list(range(3*(epochs//4),epochs+1))\n",
        "\n",
        "    fp16=False\n",
        "\n",
        "    awp = True\n",
        "    awp_lambda = 0.1\n",
        "    awp_start_epoch = 15\n",
        "    dropout_start_epoch = 15\n",
        "    resume = 0\n",
        "\n",
        "    dim = 384\n",
        "\n",
        "    comment = f\"model-{dim}-seed{seed}\"\n",
        "    output_dim = 60\n",
        "    num_eval = 6\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HCKh-AxTYi16"
      },
      "outputs": [],
      "source": [
        "\n",
        "def update_config_with_strategy(config):\n",
        "    # cfg is configuration instance\n",
        "    #strategy, replicas, is_tpu = get_strategy()\n",
        "    if tpu_strategy is not None:\n",
        "      strategy=tpu_strategy\n",
        "      replicas=8\n",
        "      is_tpu=True\n",
        "      config.input_path=config.input_path.replace(\"/kaggle\",\"gs://asl-bucket71\")\n",
        "      config.output_path = config.output_path.replace(\"/kaggle\",\"/content/drive/MyDrive/kaggle\")\n",
        "\n",
        "    else:\n",
        "      strategy,replicas,is_tpu=get_strategy()\n",
        "    print(\"Strategy\",strategy)\n",
        "\n",
        "    config.strategy = strategy\n",
        "    config.replicas = replicas\n",
        "    config.is_tpu = is_tpu\n",
        "    config.lr = config.lr * replicas\n",
        "    config.batch_size = config.batch_size * replicas\n",
        "    return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5msRCmt0Yi16"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrUlKKykrs3c"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "j_tHWSs8Yi16"
      },
      "outputs": [],
      "source": [
        "\n",
        "def count_data_items(dataset):\n",
        "    dataset_size = 0\n",
        "    for _ in dataset:\n",
        "        dataset_size += 1\n",
        "    return dataset_size\n",
        "\n",
        "\n",
        "def interp1d_(x, target_len):\n",
        "    target_len = tf.maximum(1, target_len)\n",
        "    x = tf.image.resize(x, (target_len, tf.shape(x)[1]))\n",
        "    return x\n",
        "\n",
        "\n",
        "def tf_nan_mean(x, axis=0, keepdims=False):\n",
        "    return tf.reduce_sum(\n",
        "        tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims\n",
        "    ) / tf.reduce_sum(\n",
        "        tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims\n",
        "    )\n",
        "\n",
        "\n",
        "def tf_nan_std(x, center=None, axis=0, keepdims=False):\n",
        "    if center is None:\n",
        "        center = tf_nan_mean(x, axis=axis, keepdims=True)\n",
        "    d = x - center\n",
        "    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n",
        "\n",
        "\n",
        "def flip_lr(x):\n",
        "    if x.shape[1] == Constants.ROWS_PER_FRAME:\n",
        "        LHAND = Constants.LHAND\n",
        "        RHAND = Constants.RHAND\n",
        "        LLIP = Constants.LLIP\n",
        "        RLIP = Constants.RLIP\n",
        "        LEYE = Constants.LEYE\n",
        "        REYE = Constants.REYE\n",
        "        LNOSE = Constants.LNOSE\n",
        "        RNOSE = Constants.RNOSE\n",
        "        LPOSE = Constants.LPOSE\n",
        "        RPOSE = Constants.RPOSE\n",
        "    else:\n",
        "        LHAND = Constants.LANDMARK_INDICES[\"LHAND\"]\n",
        "        RHAND = Constants.LANDMARK_INDICES[\"RHAND\"]\n",
        "        LLIP = Constants.LANDMARK_INDICES[\"LLIP\"]\n",
        "        RLIP = Constants.LANDMARK_INDICES[\"RLIP\"]\n",
        "        LEYE = Constants.LANDMARK_INDICES[\"LEYE\"]\n",
        "        REYE = Constants.LANDMARK_INDICES[\"REYE\"]\n",
        "        LNOSE = Constants.LANDMARK_INDICES[\"LNOSE\"]\n",
        "        RNOSE = Constants.LANDMARK_INDICES[\"RNOSE\"]\n",
        "        LPOSE = Constants.LANDMARK_INDICES[\"LPOSE\"]\n",
        "        RPOSE = Constants.LANDMARK_INDICES[\"RPOSE\"]\n",
        "\n",
        "    x, y = tf.unstack(x, axis=-1)\n",
        "    x = 1 - x\n",
        "    new_x = tf.stack([x, y], -1)\n",
        "    new_x = tf.transpose(new_x, [1, 0, 2])\n",
        "    lhand = tf.gather(new_x, LHAND, axis=0)\n",
        "    rhand = tf.gather(new_x, RHAND, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LHAND)[..., None], rhand)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RHAND)[..., None], lhand)\n",
        "    llip = tf.gather(new_x, LLIP, axis=0)\n",
        "    rlip = tf.gather(new_x, RLIP, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LLIP)[..., None], rlip)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RLIP)[..., None], llip)\n",
        "    lpose = tf.gather(new_x, LPOSE, axis=0)\n",
        "    rpose = tf.gather(new_x, RPOSE, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LPOSE)[..., None], rpose)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RPOSE)[..., None], lpose)\n",
        "    leye = tf.gather(new_x, LEYE, axis=0)\n",
        "    reye = tf.gather(new_x, REYE, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LEYE)[..., None], reye)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(REYE)[..., None], leye)\n",
        "    lnose = tf.gather(new_x, LNOSE, axis=0)\n",
        "    rnose = tf.gather(new_x, RNOSE, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LNOSE)[..., None], rnose)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RNOSE)[..., None], lnose)\n",
        "    new_x = tf.transpose(new_x, [1, 0, 2])\n",
        "    return new_x\n",
        "\n",
        "\n",
        "def resample(x, rate=(0.8, 1.2)):\n",
        "    rate = tf.random.uniform((), rate[0], rate[1])\n",
        "    length = tf.shape(x)[0]\n",
        "    new_size = tf.cast(rate * tf.cast(length, tf.float32), tf.int32)\n",
        "    new_x = interp1d_(x, new_size)\n",
        "    return new_x\n",
        "\n",
        "\n",
        "def spatial_random_affine(\n",
        "    xyz,\n",
        "    scale=(0.8, 1.2),\n",
        "    shear=(-0.1, 0.1),\n",
        "    shift=(-0.1, 0.1),\n",
        "    degree=(-20, 20),\n",
        "):\n",
        "    center = tf.constant([0.5, 0.5])\n",
        "    if degree is not None:\n",
        "        xy = xyz[..., :2]\n",
        "        z = xyz[..., 2:]\n",
        "        xy -= center\n",
        "        degree = tf.random.uniform((), *degree)\n",
        "        radian = degree / 180 * np.pi\n",
        "        c = tf.math.cos(radian)\n",
        "        s = tf.math.sin(radian)\n",
        "        rotate_mat = tf.identity(\n",
        "            [\n",
        "                [c, s],\n",
        "                [-s, c],\n",
        "            ]\n",
        "        )\n",
        "        xy = xy @ rotate_mat\n",
        "        xy = xy + center\n",
        "        xyz = tf.concat([xy, z], axis=-1)\n",
        "\n",
        "    if scale is not None:\n",
        "        scale = tf.random.uniform((), *scale)\n",
        "        xyz = scale * xyz\n",
        "\n",
        "    if shear is not None:\n",
        "        xy = xyz[..., :2]\n",
        "        z = xyz[..., 2:]\n",
        "        shear_x = shear_y = tf.random.uniform((), *shear)\n",
        "        if tf.random.uniform(()) < 0.5:\n",
        "            shear_x = 0.0\n",
        "        else:\n",
        "            shear_y = 0.0\n",
        "        shear_mat = tf.identity([[1.0, shear_x], [shear_y, 1.0]])\n",
        "        xy = xy @ shear_mat\n",
        "        xyz = tf.concat([xy, z], axis=-1)\n",
        "\n",
        "    if shift is not None:\n",
        "        shift = tf.random.uniform((), *shift)\n",
        "        xyz = xyz + shift\n",
        "\n",
        "    return xyz\n",
        "\n",
        "\n",
        "def temporal_mask(x, size=[1, 15], mask_value=float(\"nan\")):\n",
        "    l0 = tf.shape(x)[0]\n",
        "    if size[1] > l0 // 8:\n",
        "        size[1] = l0 // 8\n",
        "        if size[1] <= 1:\n",
        "            size[1] = 2\n",
        "    mask_size = tf.random.uniform((), *size, dtype=tf.int32)\n",
        "    mask_offset = tf.random.uniform((), 0, tf.clip_by_value(l0 - mask_size, 1, l0), dtype=tf.int32)\n",
        "    x = tf.tensor_scatter_nd_update(\n",
        "        x,\n",
        "        tf.range(mask_offset, mask_offset + mask_size)[..., None],\n",
        "        tf.fill([mask_size, tf.shape(x)[1], 2], mask_value),\n",
        "    )\n",
        "    return x\n",
        "\n",
        "\n",
        "def spatial_mask(x, size=(0.05, 0.2), mask_value=float(\"nan\")):\n",
        "    mask_offset_y = tf.random.uniform(())\n",
        "    mask_offset_x = tf.random.uniform(())\n",
        "    mask_size = tf.random.uniform((), *size)\n",
        "    mask_x = (mask_offset_x < x[..., 0]) & (x[..., 0] < mask_offset_x + mask_size)\n",
        "    mask_y = (mask_offset_y < x[..., 1]) & (x[..., 1] < mask_offset_y + mask_size)\n",
        "    mask = mask_x & mask_y\n",
        "    x = tf.where(mask[..., None], mask_value, x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "def augment_fn(x):\n",
        "    # shape (T,F)\n",
        "    x = tf.reshape(x, (tf.shape(x)[0], -1, 2))\n",
        "    if tf.random.uniform(()) < 0.6:\n",
        "        x = resample(x, (0.5, 1.5))\n",
        "    if tf.random.uniform(()) < 0.6:\n",
        "        x = flip_lr(x)\n",
        "    if tf.random.uniform(()) < 0.6:\n",
        "        x = spatial_random_affine(x)\n",
        "    if tf.random.uniform(()) < 0.4:\n",
        "        x = temporal_mask(x)\n",
        "    if tf.random.uniform(()) < 0.4:\n",
        "        x = spatial_mask(x)\n",
        "    x = tf.reshape(x, (tf.shape(x)[0], -1))\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jYUGdwEDYi17"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Preprocess(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_len, normalize=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.max_len = max_len\n",
        "        self.center = Constants.CENTER_INDICES\n",
        "        self.normalize = normalize\n",
        "\n",
        "    # preprocess a batch of data\n",
        "    def call(self, x):\n",
        "        # rank is 3: [B,T,F]\n",
        "        # if your input is just [T,F], extend its dimesnion before calling.\n",
        "\n",
        "        x = tf.reshape(x, (tf.shape(x)[0], tf.shape(x)[1], Constants.NUM_NODES, 2))\n",
        "        # dimensions now are [B,T,F//2,2]\n",
        "\n",
        "        x_selected = x\n",
        "        if self.normalize:\n",
        "            mean = tf_nan_mean(tf.gather(x, self.center, axis=2), axis=[1, 2], keepdims=True)\n",
        "            mean = tf.where(tf.math.is_nan(mean), tf.constant(0.5, x.dtype), mean)\n",
        "            std = tf_nan_std(x_selected, center=mean, axis=[1, 2], keepdims=True)\n",
        "            x = (x_selected - mean) / std\n",
        "        else:\n",
        "            x = x_selected\n",
        "\n",
        "        dx = tf.cond(\n",
        "            tf.shape(x)[1] > 1,\n",
        "            lambda: tf.pad(x[:, 1:] - x[:, :-1], [[0, 0], [0, 1], [0, 0], [0, 0]]),\n",
        "            lambda: tf.zeros_like(x),\n",
        "        )\n",
        "\n",
        "        dx2 = tf.cond(\n",
        "            tf.shape(x)[1] > 2,\n",
        "            lambda: tf.pad(x[:, 2:] - x[:, :-2], [[0, 0], [0, 2], [0, 0], [0, 0]]),\n",
        "            lambda: tf.zeros_like(x),\n",
        "        )\n",
        "        length = tf.shape(x)[1]\n",
        "\n",
        "        x = tf.concat(\n",
        "            [\n",
        "                tf.reshape(x, (-1, length, 2 * Constants.NUM_NODES)),  # x1,y1,x2,y2,...\n",
        "                tf.reshape(dx, (-1, length, 2 * Constants.NUM_NODES)),\n",
        "                tf.reshape(dx2, (-1, length, 2 * Constants.NUM_NODES)),\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "\n",
        "        # x1,y1,x2,y2,...dx1,dy1,dx2,dy2,...\n",
        "        x = tf.where(tf.math.is_nan(x), tf.constant(0.0, x.dtype), x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def pad_if_short(x, max_len):\n",
        "    # shape (T,F)\n",
        "    pad_len = max_len - tf.shape(x)[0]\n",
        "    padding = tf.ones((pad_len, tf.shape(x)[1]), dtype=x.dtype) * Constants.INPUT_PAD\n",
        "    x = tf.concat([x, padding], axis=0)\n",
        "    return x\n",
        "\n",
        "\n",
        "def shrink_if_long(x, max_len):\n",
        "    # shape is [T,F]\n",
        "    if tf.shape(x)[0] > max_len:\n",
        "        # we need to extend the dimension to [T,F,channels]  for tf.image.resize\n",
        "        x = tf.image.resize(x[..., None], (max_len, tf.shape(x)[1]))\n",
        "        x = tf.squeeze(x, axis=2)\n",
        "\n",
        "    return x\n",
        "\n",
        "def preprocess(x, max_len, do_pad=True):\n",
        "    # shape (T,F)\n",
        "    x = shrink_if_long(x, max_len=max_len)\n",
        "    # Preprocess expects a batch, so we extend the dimension to (None,T,F), then reduce the output back to (T,F).\n",
        "    x = tf.cast(Preprocess(max_len=max_len)(x[None, ...])[0], tf.float32)\n",
        "\n",
        "    if do_pad:  # we can avoid this step if there is batch padding\n",
        "        x = pad_if_short(x, max_len=max_len)\n",
        "        #x=tf.ensure_shape(x,(max_len,Constants.CHANNELS))\n",
        "    else:\n",
        "        #x=tf.ensure_shape(x,(None,Constants.CHANNELS))\n",
        "        pad\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FdtpVVSbYi17"
      },
      "outputs": [],
      "source": [
        "\n",
        "def decode_tfrec(record_bytes):\n",
        "    features = tf.io.parse_single_example(\n",
        "        record_bytes,\n",
        "        {\n",
        "            \"coordinates\": tf.io.VarLenFeature(tf.float32),\n",
        "            \"label\": tf.io.VarLenFeature(tf.int64),\n",
        "        },\n",
        "    )\n",
        "    coords = tf.sparse.to_dense(features[\"coordinates\"])\n",
        "    coords = tf.reshape(coords, (-1, Constants.NUM_INPUT_FEATURES))\n",
        "    label = tf.sparse.to_dense(features[\"label\"])\n",
        "\n",
        "    #coords=tf.ensure_shape(coords,(None,Constants.NUM_INPUT_FEATURES))\n",
        "    #label=tf.ensure_shape(label,(None,))\n",
        "\n",
        "\n",
        "    return (coords, label)\n",
        "\n",
        "def ensure_shapes(x,y,batch_size,max_len):\n",
        "  x=tf.ensure_shape(x,(batch_size,max_len,Constants.CHANNELS))\n",
        "  y=tf.ensure_shape(y,(batch_size,Constants.MAX_STRING_LEN))\n",
        "  return x,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ImAgehPoYi18"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_dataset(\n",
        "    filenames,\n",
        "    input_path,\n",
        "    max_len,\n",
        "    batch_size=64,\n",
        "    drop_remainder=False,\n",
        "    augment=False,\n",
        "    shuffle_buffer=None,\n",
        "    repeat=False,\n",
        "    use_tfrecords=True,\n",
        "):\n",
        "    ignore_order = tf.data.Options()\n",
        "    ignore_order.experimental_deterministic = False\n",
        "\n",
        "\n",
        "    ds = tf.data.TFRecordDataset(\n",
        "        filenames, num_parallel_reads=tf.data.AUTOTUNE, compression_type=\"GZIP\"\n",
        "    )\n",
        "    ds.with_options(ignore_order)\n",
        "    ds = ds.map(decode_tfrec, tf.data.AUTOTUNE)\n",
        "\n",
        "    if augment:\n",
        "        ds = ds.map(lambda x, y: (augment_fn(x), y), tf.data.AUTOTUNE)\n",
        "\n",
        "    ds = ds.map(lambda x, y: (preprocess(x, max_len=max_len, do_pad=False), y), tf.data.AUTOTUNE)\n",
        "    #if repeat:\n",
        "    #    ds = ds.repeat()\n",
        "\n",
        "    if shuffle_buffer is not None:\n",
        "        ds = ds.shuffle(shuffle_buffer)\n",
        "\n",
        "    ds = ds.padded_batch(\n",
        "        batch_size,\n",
        "        padding_values=(\n",
        "            tf.constant(Constants.INPUT_PAD, dtype=tf.float32),\n",
        "            tf.constant(Constants.LABEL_PAD, dtype=tf.int64),\n",
        "        ),\n",
        "        padded_shapes=([max_len, Constants.CHANNELS], [Constants.MAX_STRING_LEN]),\n",
        "        drop_remainder=drop_remainder,\n",
        "    )\n",
        "\n",
        "    #tf.data.experimental.assert_cardinality(len(labels) // BATCH_SIZE)\n",
        "    ds.map(lambda x,y: ensure_shapes(x,y,batch_size,max_len),tf.data.AUTOTUNE)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4HDYkwwgYi18"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_run(train_files, valid_files, config, num_train, num_valid,experiment_id=0, use_tfrecords=True,summary=False):\n",
        "    #gc.collect()\n",
        "    #tf.keras.backend.clear_session()\n",
        "\n",
        "\n",
        "    if config.fp16:\n",
        "        if config.is_tpu:\n",
        "            policy = \"mixed_bfloat16\"\n",
        "        else:\n",
        "            policy = \"mixed_float16\"\n",
        "    else:\n",
        "        policy = \"float32\"\n",
        "\n",
        "\n",
        "    tf.keras.mixed_precision.set_global_policy(policy)\n",
        "    print(f\"\\n... TWO IMPORTANT ASPECTS OF THE GLOBAL MIXED PRECISION POLICY:\")\n",
        "    print(f'\\t--> COMPUTE DTYPE  : {tf.keras.mixed_precision.global_policy().compute_dtype}')\n",
        "    print(f'\\t--> VARIABLE DTYPE : {tf.keras.mixed_precision.global_policy().variable_dtype}')\n",
        "    augment_train= True\n",
        "    repeat_train = True\n",
        "    if config.is_tpu:\n",
        "      shuffle_buffer = 16384 #4096\n",
        "    else:\n",
        "      shuffle_buffer=4096\n",
        "    print(\"shuffle_buffer\",shuffle_buffer)\n",
        "    train_ds = get_dataset(\n",
        "        train_files,\n",
        "        input_path=config.input_path,\n",
        "        max_len=config.max_len,\n",
        "        batch_size=config.batch_size,\n",
        "        drop_remainder=True,\n",
        "        augment=augment_train,\n",
        "        repeat=repeat_train,\n",
        "        shuffle_buffer=shuffle_buffer,\n",
        "        use_tfrecords=True,\n",
        "    )\n",
        "    if valid_files is not None:\n",
        "        valid_ds = get_dataset(\n",
        "            valid_files,\n",
        "            input_path=config.input_path,\n",
        "            max_len=config.max_len,\n",
        "            batch_size=config.batch_size,\n",
        "            use_tfrecords=True,\n",
        "            drop_remainder=True\n",
        "        )\n",
        "    else:\n",
        "        valid_ds = None\n",
        "        valid_files = []\n",
        "\n",
        "    # num_train = count_data_items(train_ds)\n",
        "    # num_valid = count_data_items(valid_ds)\n",
        "    # print(num_train, num_valid, config.batch_size)\n",
        "    # exit()\n",
        "\n",
        "    steps_per_epoch = num_train // config.batch_size\n",
        "    dropout_step = config.dropout_start_epoch * steps_per_epoch\n",
        "    strategy = config.strategy\n",
        "    with strategy.scope():\n",
        "        model = get_model(\n",
        "            max_len=config.max_len,\n",
        "            output_dim=config.output_dim,\n",
        "            input_pad=Constants.INPUT_PAD,\n",
        "            dim=config.dim,\n",
        "            dropout_step=dropout_step,\n",
        "            drop_rate=0.2\n",
        "        )\n",
        "\n",
        "        base_lr = config.lr\n",
        "        lr_schedule = CosineDecay(\n",
        "            initial_learning_rate=base_lr / 10,\n",
        "            decay_steps=int(0.95 * steps_per_epoch * config.epochs),\n",
        "            alpha=0.005,\n",
        "            name=None,\n",
        "            warmup_target=base_lr,\n",
        "            warmup_steps=int(0.05 * steps_per_epoch * config.epochs),\n",
        "        )\n",
        "\n",
        "        #opt = tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=config.weight_decay)\n",
        "        radam=tfa.optimizers.RectifiedAdam(learning_rate=lr_schedule,weight_decay=config.weight_decay)\n",
        "        ranger = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)\n",
        "        opt=ranger\n",
        "        awp_step = config.awp_start_epoch * steps_per_epoch\n",
        "        if config.awp:\n",
        "            model = AWP(model.input, model.output, delta=config.awp_lambda, eps=0., start_step=awp_step)\n",
        "            print(\"Using AWP\")\n",
        "\n",
        "        ctc_loss1 = CTCLoss(pad_token_idx=Constants.LABEL_PAD,batch_size=config.batch_size,\n",
        "                           max_string_len=Constants.MAX_STRING_LEN,\n",
        "                           output_dim=config.output_dim,\n",
        "                           output_steps=config.max_len//2,replicas=config.replicas)\n",
        "        ctc_loss2 = CTCLoss(pad_token_idx=Constants.LABEL_PAD,batch_size=config.batch_size,\n",
        "                           max_string_len=Constants.MAX_STRING_LEN,\n",
        "                           output_dim=config.output_dim,\n",
        "                           output_steps=config.max_len//2,replicas=config.replicas)\n",
        "\n",
        "        if not config.is_tpu:\n",
        "          metrics=metrics= [LevDistanceMetric(),]\n",
        "        else:\n",
        "          metrics=None\n",
        "        model.compile(\n",
        "          optimizer=opt,\n",
        "          loss=[ctc_loss1,ctc_loss2],\n",
        "          loss_weights=[0.5,0.5],\n",
        "          metrics= metrics,\n",
        "          #steps_per_execution=16\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    if summary:\n",
        "        print()\n",
        "        model.summary()\n",
        "        print()\n",
        "        print(train_ds, valid_ds)\n",
        "        print()\n",
        "    print(f\"---------experiment {experiment_id}---------\")\n",
        "    print(f\"train:{num_train} \")\n",
        "    print()\n",
        "\n",
        "    if config.resume:\n",
        "        print(f\"resume from epoch{config.resume}\")\n",
        "        model.load_weights(f\"{config.output_path}/{config.comment}-exp{experiment_id}-last.h5\")\n",
        "        if train_ds is not None:\n",
        "            model.evaluate(train_ds.take(steps_per_epoch))\n",
        "        if valid_ds is not None:\n",
        "            model.evaluate(valid_ds)\n",
        "\n",
        "    tb_logger = tf.keras.callbacks.TensorBoard(\n",
        "        log_dir=config.output_path,\n",
        "    )\n",
        "    sv_loss = tf.keras.callbacks.ModelCheckpoint(\n",
        "        f\"{config.output_path}/{config.comment}-exp{experiment_id}-best.h5\",\n",
        "        monitor=\"val_final_loss\",\n",
        "        verbose=1,\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        mode=\"min\",\n",
        "        save_freq=\"epoch\",\n",
        "    )\n",
        "\n",
        "    # Callback function to check transcription on the val set.\n",
        "    # validation_callback = CallbackEval(model, valid_ds)\n",
        "    memory_usage = MemoryUsageCallbackExtended()\n",
        "    swa = SWA(\n",
        "        f\"{config.output_path}/{config.comment}-exp{experiment_id}\",\n",
        "        config.swa_epochs,\n",
        "        strategy=strategy,\n",
        "        train_ds=train_ds,\n",
        "        valid_ds=valid_ds,\n",
        "    )\n",
        "    callbacks = []\n",
        "    if config.save_output:\n",
        "        #callbacks.append(tb_logger)\n",
        "        callbacks.append(swa)\n",
        "        callbacks.append(sv_loss)\n",
        "    #callbacks.append(memory_usage)\n",
        "        callbacks.append(tf.keras.callbacks.TerminateOnNaN())\n",
        "    # callbacks.append(validation_callback)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        epochs=config.epochs - config.resume,\n",
        "        #steps_per_epoch=steps_per_epoch,\n",
        "        #validation_steps=num_valid // config.batch_size,\n",
        "        callbacks=callbacks,\n",
        "        validation_data=valid_ds,\n",
        "        verbose=config.verbose,\n",
        "    )\n",
        "\n",
        "    if config.save_output:  # reload the saved best weights checkpoint\n",
        "        saved_based_model = f\"{config.output_path}/{config.comment}-exp{experiment_id}-best.h5\"\n",
        "        if os.path.exists(saved_based_model):\n",
        "            model.load_weights(saved_based_model)\n",
        "        else:\n",
        "            print(f\"Warning: could not find {saved_based_model}\")\n",
        "    if valid_ds is not None:\n",
        "        cv = model.evaluate(valid_ds, verbose=config.verbose)\n",
        "    else:\n",
        "        cv = None\n",
        "    return model, cv, history\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YpxT9BTFYi18"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(config, experiment_id=0, use_supplemental=True):\n",
        "    #tf.keras.backend.clear_session()\n",
        "    if config.strategy is None:\n",
        "      update_config_with_strategy(config)\n",
        "    print(f\"using {config.replicas} replicas\")\n",
        "    print(f\"batch size {config.batch_size}\")\n",
        "    print(f\"learning rate {config.lr}\")\n",
        "    print(f\"fp16={config.fp16}\")\n",
        "    seed_everything(config.seed)\n",
        "\n",
        "\n",
        "    all_filenames = sorted(tf.io.gfile.glob(config.input_path+\"/asl-preprocessing/records/*.tfrecord\"))\n",
        "    regular = [x for x in all_filenames if \"supp\" not in x]\n",
        "    supp = [x for x in all_filenames if \"supp\" in x]\n",
        "\n",
        "    data_filenames = regular\n",
        "    if use_supplemental:\n",
        "        data_filenames += supp\n",
        "    print(\"Using TFRECORDS\")\n",
        "\n",
        "\n",
        "    valid_files = data_filenames[: config.num_eval]  # first part in list\n",
        "    train_files = data_filenames[config.num_eval :]\n",
        "    random.shuffle(train_files)\n",
        "\n",
        "\n",
        "    df1 = pd.read_csv(config.input_path + \"/asl-fingerspelling/train.csv\")\n",
        "    df2 = pd.read_csv(config.input_path + \"/asl-fingerspelling/supplemental_metadata.csv\")\n",
        "    df_info = pd.concat([df1, df2])\n",
        "\n",
        "    #ds = get_dataset(train_files, CFG.input_path,max_len=CFG.max_len, augment=False, batch_size=64)\n",
        "    #print(ds)\n",
        "    #for x,y in ds:\n",
        "    #    print(x,y)\n",
        "    #raise\n",
        "\n",
        "    if use_supplemental:\n",
        "        num_train = 3567 * 32  # with supplemental\n",
        "    else:\n",
        "        num_train = 1912 * 32  # without supplemental\n",
        "\n",
        "    num_valid=config.num_eval*1000\n",
        "\n",
        "    train_run(\n",
        "        train_files,\n",
        "        valid_files,\n",
        "        config,\n",
        "        num_train,\n",
        "        num_valid,\n",
        "        summary=False,\n",
        "        experiment_id=experiment_id,\n",
        "        use_tfrecords=True,\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lfC8z_wLYi19"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVIhC2jeYi19"
      },
      "source": [
        "# Train It!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5UdMh9V7Yi19",
        "outputId": "d51989b7-b5b4-45e1-c2c9-1541942fd4c3"
      },
      "outputs": [],
      "source": [
        "if 'config' not in globals():\n",
        "  config=CFG()\n",
        "tf.debugging.disable_traceback_filtering()\n",
        "#train(config,use_supplemental=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdtx5fW3-DPm"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-uAqcPmSYi1-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ZWJM1IC5Yi1-"
      },
      "outputs": [],
      "source": [
        "class InferModel(tf.Module):\n",
        "    def __init__(self, model,config=CFG):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.max_len=config.max_len\n",
        "\n",
        "    @tf.function(\n",
        "        input_signature=[tf.TensorSpec(shape=(None,Constants.NUM_INPUT_FEATURES), dtype=tf.float32, name=\"inputs\")]\n",
        "    )\n",
        "    def __call__(self, inputs):\n",
        "        \"\"\"\n",
        "        Applies the feature generation model and main model to the input tensor.\n",
        "\n",
        "        Args:\n",
        "            inputs: Input tensor with shape (T, F).\n",
        "\n",
        "        Returns:\n",
        "            A dictionary with a single key 'outputs' and corresponding output tensor.\n",
        "        \"\"\"\n",
        "        x=tf.cast(inputs,tf.float32)\n",
        "        x = x[None] # trick to deal with empty frames\n",
        "        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, Constants.NUM_INPUT_FEATURES)), lambda: tf.identity(x))\n",
        "        x = x[0]\n",
        "        x = preprocess(x,max_len=self.max_len)\n",
        "\n",
        "        x = self.model(x[None],training=False)[0][0]\n",
        "\n",
        "        x=decode_phrase(x)\n",
        "        x = tf.cond(tf.shape(x)[0] == 0, lambda: tf.zeros(1, tf.int64), lambda: tf.identity(x))\n",
        "\n",
        "        outputs=tf.one_hot(x,depth=59,dtype=tf.float32)\n",
        "        return {\"outputs\": outputs}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7xpye2JQYi1_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-22 22:36:23.804146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-22 22:36:23.804286: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-22 22:36:23.804375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-22 22:36:24.145696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-22 22:36:24.145837: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-22 22:36:24.145963: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-22 22:36:24.146034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1831] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5692 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
            "2023-07-22 22:36:24.742308: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model with weights /kaggle/input/weights/model-384-seed42-exp0-best.h5\n"
          ]
        }
      ],
      "source": [
        "\n",
        "config=CFG\n",
        "\n",
        "model = get_model(\n",
        "    max_len=config.max_len,\n",
        "    output_dim=config.output_dim,\n",
        "    dim=config.dim,\n",
        "    input_pad=Constants.INPUT_PAD,\n",
        ")\n",
        "experiment_id=0\n",
        "\n",
        "saved_based_model = f\"{config.input_path}/weights/{config.comment}-exp{experiment_id}-best.h5\"\n",
        "model.load_weights(saved_based_model)\n",
        "print(f\"model with weights {saved_based_model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jwxBqovnYi1_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input shape: (123, 244), dtype: float32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-07-22 22:36:30.912571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:440] Loaded cuDNN version 8901\n",
            "2023-07-22 22:36:30.981649: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 creekhouse\n"
          ]
        }
      ],
      "source": [
        "# Sanity Check\n",
        "import json\n",
        "with open (config.input_path+\"/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
        "    character_map = json.load(f)\n",
        "rev_character_map = {j:i for i,j in character_map.items()}\n",
        "\n",
        "infer_keras_model=InferModel(model)\n",
        "\n",
        "main_dir = config.input_path+'/asl-fingerspelling'\n",
        "path = f'{main_dir}/train_landmarks/5414471.parquet'\n",
        "cols=selected_columns(path)\n",
        "df = pd.read_parquet(path, engine = 'auto', columns = cols)\n",
        "seq_id=1816796431\n",
        "seq=df.loc[seq_id]\n",
        "data = seq[cols].to_numpy()\n",
        "print(f'input shape: {data.shape}, dtype: {data.dtype}')\n",
        "output = infer_keras_model(data)[\"outputs\"]\n",
        "prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output, axis=1)])\n",
        "\n",
        "print(prediction_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "YQM7GMtfYi1_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /kaggle/working/infer_model/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /kaggle/working/infer_model/assets\n",
            "2023-07-22 22:37:02.026064: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
            "2023-07-22 22:37:02.026086: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
            "2023-07-22 22:37:02.026326: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /kaggle/working/infer_model\n",
            "2023-07-22 22:37:02.078122: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
            "2023-07-22 22:37:02.078147: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /kaggle/working/infer_model\n",
            "2023-07-22 22:37:02.181549: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\n",
            "2023-07-22 22:37:02.217285: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
            "2023-07-22 22:37:02.611110: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /kaggle/working/infer_model\n",
            "2023-07-22 22:37:02.869587: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 843262 microseconds.\n",
            "2023-07-22 22:37:03.082267: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
          ]
        }
      ],
      "source": [
        "SAVED_MODEL_PATH=config.output_path+\"/infer_model\"\n",
        "\n",
        "tf.saved_model.save(infer_keras_model,SAVED_MODEL_PATH)\n",
        "keras_model_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_PATH)\n",
        "keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "#keras_model_converter.target_spec.supported_types = [tf.float16]\n",
        "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "#converter.allow_custom_ops=True\n",
        "tflite_model = keras_model_converter.convert()\n",
        "TFLITE_FILE_PATH=config.output_path+\"/model.tflite\"\n",
        "with open(TFLITE_FILE_PATH, \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "with open(config.output_path+'/inference_args.json', 'w') as f:\n",
        "     json.dump({ 'selected_columns': cols }, f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "2KNUGXjBYi2A"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 creek house\n"
          ]
        }
      ],
      "source": [
        "interpreter = tf.lite.Interpreter(TFLITE_FILE_PATH)\n",
        "REQUIRED_SIGNATURE = \"serving_default\"\n",
        "REQUIRED_OUTPUT = \"outputs\"\n",
        "found_signatures = list(interpreter.get_signature_list().keys())\n",
        "if REQUIRED_SIGNATURE not in found_signatures:\n",
        "    print(\"Required input signature not found.\")\n",
        "\n",
        "prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
        "output = prediction_fn(inputs=data)\n",
        "prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n",
        "print(prediction_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "aTeDBunuYi2A"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: kaggle/working/model.tflite (deflated 19%)\n",
            "  adding: kaggle/working/inference_args.json (deflated 83%)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!zip  submission.zip \"/kaggle/working/model.tflite\" \"/kaggle/working/inference_args.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ZWcEOwClYi2A"
      },
      "outputs": [],
      "source": [
        "#!pip install /kaggle/input/tflite-wheels-2140/tflite_runtime_nightly-2.14.0.dev20230508-cp310-cp310-manylinux2014_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6W11OFc7Yi2A"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nimport json\\nimport pandas as pd\\nimport tflite_runtime.interpreter as tflite\\nimport numpy as np\\nimport time\\nfrom tqdm import tqdm\\nimport Levenshtein as Lev\\nimport glob\\n'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "import json\n",
        "import pandas as pd\n",
        "import tflite_runtime.interpreter as tflite\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import Levenshtein as Lev\n",
        "import glob\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "zoac5lq4Yi2B"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nSEL_FEATURES = json.load(open(\\'/kaggle/working/inference_args.json\\'))[\\'selected_columns\\']\\n\\ndef load_relevant_data_subset(pq_path):\\n        return pd.read_parquet(pq_path, columns=SEL_FEATURES) #selected_columns)\\n\\nwith open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\\n    character_map = json.load(f)\\nrev_character_map = {j:i for i,j in character_map.items()}\\n\\n\\ndf = pd.read_csv(\\'/kaggle/input/asl-fingerspelling/train.csv\\')\\n\\nidx = 0\\nsample = df.loc[idx]\\nloaded = load_relevant_data_subset(\\'/kaggle/input/asl-fingerspelling/\\' + sample[\\'path\\'])\\nloaded = loaded[loaded.index==sample[\\'sequence_id\\']].values\\nprint(loaded.shape)\\nframes = loaded\\n\\ndef wer__(s1, s2):\\n    w1 = len(s1.split())\\n    lvd = Lev.distance(s1, s2)\\n    return lvd / w1\\n\\ninterpreter = tflite.Interpreter(\\'model.tflite\\')\\nfound_signatures = list(interpreter.get_signature_list().keys())\\n\\nREQUIRED_SIGNATURE = \\'serving_default\\'\\nREQUIRED_OUTPUT = \\'outputs\\'\\nif REQUIRED_SIGNATURE not in found_signatures:\\n    raise KernelEvalException(\\'Required input signature not found.\\')\\n\\nprediction_fn = interpreter.get_signature_runner(\"serving_default\")\\noutput_lite = prediction_fn(inputs=frames)\\nprediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output_lite[REQUIRED_OUTPUT], axis=1)])\\nprint(prediction_str)\\n\\n\\nst = time.time()\\ncount=0\\nmodel_time = 0\\n\\nlevs = []\\n\\nfiles=glob.glob(\\'/kaggle/input/asl-fingerspelling/train_landmarks/*.parquet\\')\\nfor f in files:\\n    df = load_relevant_data_subset(f)\\n    seq=df.index.drop_duplicates()\\n    for ind in tqdm(seq):\\n        loaded = df.loc[ind].values\\n        count+=1\\n        md_st = time.time()\\n        output_ = prediction_fn(inputs=loaded)\\n        out= output_[REQUIRED_OUTPUT]\\n        assert out.ndim==2\\n        assert out.shape[1]==59\\n        assert out.dtype==np.float32\\n        assert np.all(np.isfinite(out))\\n\\n        prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output_[REQUIRED_OUTPUT], axis=1)])\\n        model_time += time.time() - md_st\\n\\n        #cur_lev = wer__(sample[\\'phrase\\'], prediction_str)\\n        #print(sample[\\'phrase\\'], \\'|\\', prediction_str, \\'|\\', cur_lev)\\n        #print()\\n\\n        #levs.append(cur_lev)\\n\\n#print(f\\'WER: {np.mean(levs):.5f}\\')\\nprint(f\\'Mean time: {(time.time() - st)/count:.2f}\\')\\nprint(f\\'Mean time only infer: {model_time/count:.2f}\\')\\n\\nout=prediction_fn(inputs=np.empty(0,dtype=np.float32))[\"outputs\"]\\nprint(out.shape,output_.dtype)\\n'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "SEL_FEATURES = json.load(open('/kaggle/working/inference_args.json'))['selected_columns']\n",
        "\n",
        "def load_relevant_data_subset(pq_path):\n",
        "        return pd.read_parquet(pq_path, columns=SEL_FEATURES) #selected_columns)\n",
        "\n",
        "with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
        "    character_map = json.load(f)\n",
        "rev_character_map = {j:i for i,j in character_map.items()}\n",
        "\n",
        "\n",
        "df = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\n",
        "\n",
        "idx = 0\n",
        "sample = df.loc[idx]\n",
        "loaded = load_relevant_data_subset('/kaggle/input/asl-fingerspelling/' + sample['path'])\n",
        "loaded = loaded[loaded.index==sample['sequence_id']].values\n",
        "print(loaded.shape)\n",
        "frames = loaded\n",
        "\n",
        "def wer__(s1, s2):\n",
        "    w1 = len(s1.split())\n",
        "    lvd = Lev.distance(s1, s2)\n",
        "    return lvd / w1\n",
        "\n",
        "interpreter = tflite.Interpreter('model.tflite')\n",
        "found_signatures = list(interpreter.get_signature_list().keys())\n",
        "\n",
        "REQUIRED_SIGNATURE = 'serving_default'\n",
        "REQUIRED_OUTPUT = 'outputs'\n",
        "if REQUIRED_SIGNATURE not in found_signatures:\n",
        "    raise KernelEvalException('Required input signature not found.')\n",
        "\n",
        "prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
        "output_lite = prediction_fn(inputs=frames)\n",
        "prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output_lite[REQUIRED_OUTPUT], axis=1)])\n",
        "print(prediction_str)\n",
        "\n",
        "\n",
        "st = time.time()\n",
        "count=0\n",
        "model_time = 0\n",
        "\n",
        "levs = []\n",
        "\n",
        "files=glob.glob('/kaggle/input/asl-fingerspelling/train_landmarks/*.parquet')\n",
        "for f in files:\n",
        "    df = load_relevant_data_subset(f)\n",
        "    seq=df.index.drop_duplicates()\n",
        "    for ind in tqdm(seq):\n",
        "        loaded = df.loc[ind].values\n",
        "        count+=1\n",
        "        md_st = time.time()\n",
        "        output_ = prediction_fn(inputs=loaded)\n",
        "        out= output_[REQUIRED_OUTPUT]\n",
        "        assert out.ndim==2\n",
        "        assert out.shape[1]==59\n",
        "        assert out.dtype==np.float32\n",
        "        assert np.all(np.isfinite(out))\n",
        "\n",
        "        prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output_[REQUIRED_OUTPUT], axis=1)])\n",
        "        model_time += time.time() - md_st\n",
        "\n",
        "        #cur_lev = wer__(sample['phrase'], prediction_str)\n",
        "        #print(sample['phrase'], '|', prediction_str, '|', cur_lev)\n",
        "        #print()\n",
        "\n",
        "        #levs.append(cur_lev)\n",
        "\n",
        "#print(f'WER: {np.mean(levs):.5f}')\n",
        "print(f'Mean time: {(time.time() - st)/count:.2f}')\n",
        "print(f'Mean time only infer: {model_time/count:.2f}')\n",
        "\n",
        "out=prediction_fn(inputs=np.empty(0,dtype=np.float32))[\"outputs\"]\n",
        "print(out.shape,output_.dtype)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etCjY_x2Yi2B"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "Vh1J_bDCYi13"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
