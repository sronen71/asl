{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENYLFLzurs3S"
      },
      "source": [
        "# For Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "agm4BN62kwwE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "IN_COLAB = 'COLAB_GPU' in os.environ\n",
        "if IN_COLAB:\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfaggO9VRZML",
        "outputId": "faa6210d-3d83-47cd-a32b-7752f3c08416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TB_VhXDvhCT2"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /kaggle/working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Asq1MGhs41ZX",
        "outputId": "8d162085-b861-4603-f075-3a9e984d595c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  ['10.94.77.74:8470']\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "tpu_strategy=None\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "\n",
        "except ValueError:\n",
        "  print(\"Not using TPU\")\n",
        "  #raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "\n",
        "#from tensorflow.python.framework.ops import disable_eager_execution\n",
        "#disable_eager_execution()  # LSTM layer can't use bfloat16 unless we do this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-x7GxsSYi1y"
      },
      "source": [
        "# Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eoljCzzvYi1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd5cad0-52be-41a7-dc75-07825ef1e51d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.21.0 typeguard-2.13.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install tensorflow-addons\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import psutil\n",
        "import glob\n",
        "import gc\n",
        "import math\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "#from tensorflow.python.framework.ops import disable_eager_execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UV3hsk-bYi1z"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rZpjjT02Yi10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4cc9531-4099-4772-c46f-389f5d6f97a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow v2.12.0\n"
          ]
        }
      ],
      "source": [
        "print(\"TensorFlow v\" + tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0gUj7mAgYi10"
      },
      "outputs": [],
      "source": [
        "class MemoryUsageCallbackExtended(tf.keras.callbacks.Callback):\n",
        "    \"\"\"Monitor memory usage on epoch begin and end, collect garbage\"\"\"\n",
        "\n",
        "    # def on_epoch_begin(self, epoch, logs=None):\n",
        "    #    print(\"**Epoch {}**\".format(epoch))\n",
        "    #    print(\n",
        "    #        f\"Memory usage on epoch begin: {int(psutil.Process(os.getpid()).memory_info().rss)/1e9:.2f GB}\"\n",
        "    #    )\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(\n",
        "            f\"Memory usage on epoch end: {int(psutil.Process(os.getpid()).memory_info().rss)/1e9:.2f} GB\"\n",
        "        )\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eErQkrlpYi11"
      },
      "source": [
        "# Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "umV9OdZCYi11"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\"A LearningRateSchedule that uses a cosine decay with optional warmup.\n",
        "\n",
        "    See [Loshchilov & Hutter, ICLR2016](https://arxiv.org/abs/1608.03983),\n",
        "    SGDR: Stochastic Gradient Descent with Warm Restarts.\n",
        "\n",
        "    For the idea of a linear warmup of our learning rate,\n",
        "    see [Goyal et al.](https://arxiv.org/pdf/1706.02677.pdf).\n",
        "\n",
        "    When we begin training a model, we often want an initial increase in our\n",
        "    learning rate followed by a decay. If `warmup_target` is an int, this\n",
        "    schedule applies a linear increase per optimizer step to our learning rate\n",
        "    from `initial_learning_rate` to `warmup_target` for a duration of\n",
        "    `warmup_steps`. Afterwards, it applies a cosine decay function taking our\n",
        "    learning rate from `warmup_target` to `alpha` for a duration of\n",
        "    `decay_steps`. If `warmup_target` is None we skip warmup and our decay\n",
        "    will take our learning rate from `initial_learning_rate` to `alpha`.\n",
        "    It requires a `step` value to  compute the learning rate. You can\n",
        "    just pass a TensorFlow variable that you increment at each training step.\n",
        "\n",
        "    The schedule is a 1-arg callable that produces a warmup followed by a\n",
        "    decayed learning rate when passed the current optimizer step. This can be\n",
        "    useful for changing the learning rate value across different invocations of\n",
        "    optimizer functions.\n",
        "\n",
        "    Our warmup is computed as:\n",
        "\n",
        "    ```python\n",
        "    def warmup_learning_rate(step):\n",
        "        completed_fraction = step / warmup_steps\n",
        "        total_delta = target_warmup - initial_learning_rate\n",
        "        return completed_fraction * total_delta\n",
        "    ```\n",
        "\n",
        "    And our decay is computed as:\n",
        "\n",
        "    ```python\n",
        "    if warmup_target is None:\n",
        "        initial_decay_lr = initial_learning_rate\n",
        "    else:\n",
        "        initial_decay_lr = warmup_target\n",
        "\n",
        "    def decayed_learning_rate(step):\n",
        "        step = min(step, decay_steps)\n",
        "        cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))\n",
        "        decayed = (1 - alpha) * cosine_decay + alpha\n",
        "        return initial_decay_lr * decayed\n",
        "    ```\n",
        "\n",
        "    Example usage without warmup:\n",
        "\n",
        "    ```python\n",
        "    decay_steps = 1000\n",
        "    initial_learning_rate = 0.1\n",
        "    lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
        "        initial_learning_rate, decay_steps)\n",
        "    ```\n",
        "\n",
        "    Example usage with warmup:\n",
        "\n",
        "    ```python\n",
        "    decay_steps = 1000\n",
        "    initial_learning_rate = 0\n",
        "    warmup_steps = 1000\n",
        "    target_learning_rate = 0.1\n",
        "    lr_warmup_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
        "        initial_learning_rate, decay_steps, warmup_target=target_learning_rate,\n",
        "        warmup_steps=warmup_steps\n",
        "    )\n",
        "    ```\n",
        "\n",
        "    You can pass this schedule directly into a `tf.keras.optimizers.Optimizer`\n",
        "    as the learning rate. The learning rate schedule is also serializable and\n",
        "    deserializable using `tf.keras.optimizers.schedules.serialize` and\n",
        "    `tf.keras.optimizers.schedules.deserialize`.\n",
        "\n",
        "    Returns:\n",
        "      A 1-arg callable learning rate schedule that takes the current optimizer\n",
        "      step and outputs the decayed learning rate, a scalar `Tensor` of the same\n",
        "      type as `initial_learning_rate`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        initial_learning_rate,\n",
        "        decay_steps,\n",
        "        alpha=0.0,\n",
        "        name=None,\n",
        "        warmup_target=None,\n",
        "        warmup_steps=0,\n",
        "    ):\n",
        "        \"\"\"Applies cosine decay to the learning rate.\n",
        "\n",
        "        Args:\n",
        "          initial_learning_rate: A scalar `float32` or `float64` `Tensor` or a\n",
        "            Python int. The initial learning rate.\n",
        "          decay_steps: A scalar `int32` or `int64` `Tensor` or a Python int.\n",
        "            Number of steps to decay over.\n",
        "          alpha: A scalar `float32` or `float64` `Tensor` or a Python int.\n",
        "            Minimum learning rate value for decay as a fraction of\n",
        "            `initial_learning_rate`.\n",
        "          name: String. Optional name of the operation.  Defaults to\n",
        "            'CosineDecay'.\n",
        "          warmup_target: None or a scalar `float32` or `float64` `Tensor` or a\n",
        "            Python int. The target learning rate for our warmup phase. Will cast\n",
        "            to the `initial_learning_rate` datatype. Setting to None will skip\n",
        "            warmup and begins decay phase from `initial_learning_rate`.\n",
        "            Otherwise scheduler will warmup from `initial_learning_rate` to\n",
        "            `warmup_target`.\n",
        "          warmup_steps: A scalar `int32` or `int64` `Tensor` or a Python int.\n",
        "            Number of steps to warmup over.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.initial_learning_rate = initial_learning_rate\n",
        "        self.decay_steps = decay_steps\n",
        "        self.alpha = alpha\n",
        "        self.name = name\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.warmup_target = warmup_target\n",
        "\n",
        "    def _decay_function(self, step, decay_steps, decay_from_lr, dtype):\n",
        "        with tf.name_scope(self.name or \"CosineDecay\"):\n",
        "            completed_fraction = step / decay_steps\n",
        "            tf_pi = tf.constant(math.pi, dtype=dtype)\n",
        "            cosine_decayed = 0.5 * (1.0 + tf.cos(tf_pi * completed_fraction))\n",
        "            decayed = (1 - self.alpha) * cosine_decayed + self.alpha\n",
        "            return tf.multiply(decay_from_lr, decayed)\n",
        "\n",
        "    def _warmup_function(self, step, warmup_steps, warmup_target, initial_learning_rate):\n",
        "        with tf.name_scope(self.name or \"CosineDecay\"):\n",
        "            completed_fraction = step / warmup_steps\n",
        "            total_step_delta = warmup_target - initial_learning_rate\n",
        "            return total_step_delta * completed_fraction + initial_learning_rate\n",
        "\n",
        "    def __call__(self, step):\n",
        "        with tf.name_scope(self.name or \"CosineDecay\"):\n",
        "            initial_learning_rate = tf.convert_to_tensor(\n",
        "                self.initial_learning_rate, name=\"initial_learning_rate\"\n",
        "            )\n",
        "            dtype = initial_learning_rate.dtype\n",
        "            decay_steps = tf.cast(self.decay_steps, dtype)\n",
        "            global_step_recomp = tf.cast(step, dtype)\n",
        "\n",
        "            if self.warmup_target is None:\n",
        "                global_step_recomp = tf.minimum(global_step_recomp, decay_steps)\n",
        "                return self._decay_function(\n",
        "                    global_step_recomp,\n",
        "                    decay_steps,\n",
        "                    initial_learning_rate,\n",
        "                    dtype,\n",
        "                )\n",
        "\n",
        "            warmup_target = tf.cast(self.warmup_target, dtype)\n",
        "            warmup_steps = tf.cast(self.warmup_steps, dtype)\n",
        "\n",
        "            global_step_recomp = tf.minimum(global_step_recomp, decay_steps + warmup_steps)\n",
        "\n",
        "            return tf.cond(\n",
        "                global_step_recomp < warmup_steps,\n",
        "                lambda: self._warmup_function(\n",
        "                    global_step_recomp,\n",
        "                    warmup_steps,\n",
        "                    warmup_target,\n",
        "                    initial_learning_rate,\n",
        "                ),\n",
        "                lambda: self._decay_function(\n",
        "                    global_step_recomp - warmup_steps,\n",
        "                    decay_steps,\n",
        "                    warmup_target,\n",
        "                    dtype,\n",
        "                ),\n",
        "            )\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"initial_learning_rate\": self.initial_learning_rate,\n",
        "            \"decay_steps\": self.decay_steps,\n",
        "            \"alpha\": self.alpha,\n",
        "            \"name\": self.name,\n",
        "            \"warmup_target\": self.warmup_target,\n",
        "            \"warmup_steps\": self.warmup_steps,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh1J_bDCYi13"
      },
      "source": [
        "# Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5icpG0BeYi13"
      },
      "outputs": [],
      "source": [
        "def get_char_dict():\n",
        "    char_dict = {\n",
        "        \" \": 0,\n",
        "        \"!\": 1,\n",
        "        \"#\": 2,\n",
        "        \"$\": 3,\n",
        "        \"%\": 4,\n",
        "        \"&\": 5,\n",
        "        \"'\": 6,\n",
        "        \"(\": 7,\n",
        "        \")\": 8,\n",
        "        \"*\": 9,\n",
        "        \"+\": 10,\n",
        "        \",\": 11,\n",
        "        \"-\": 12,\n",
        "        \".\": 13,\n",
        "        \"/\": 14,\n",
        "        \"0\": 15,\n",
        "        \"1\": 16,\n",
        "        \"2\": 17,\n",
        "        \"3\": 18,\n",
        "        \"4\": 19,\n",
        "        \"5\": 20,\n",
        "        \"6\": 21,\n",
        "        \"7\": 22,\n",
        "        \"8\": 23,\n",
        "        \"9\": 24,\n",
        "        \":\": 25,\n",
        "        \";\": 26,\n",
        "        \"=\": 27,\n",
        "        \"?\": 28,\n",
        "        \"@\": 29,\n",
        "        \"[\": 30,\n",
        "        \"_\": 31,\n",
        "        \"a\": 32,\n",
        "        \"b\": 33,\n",
        "        \"c\": 34,\n",
        "        \"d\": 35,\n",
        "        \"e\": 36,\n",
        "        \"f\": 37,\n",
        "        \"g\": 38,\n",
        "        \"h\": 39,\n",
        "        \"i\": 40,\n",
        "        \"j\": 41,\n",
        "        \"k\": 42,\n",
        "        \"l\": 43,\n",
        "        \"m\": 44,\n",
        "        \"n\": 45,\n",
        "        \"o\": 46,\n",
        "        \"p\": 47,\n",
        "        \"q\": 48,\n",
        "        \"r\": 49,\n",
        "        \"s\": 50,\n",
        "        \"t\": 51,\n",
        "        \"u\": 52,\n",
        "        \"v\": 53,\n",
        "        \"w\": 54,\n",
        "        \"x\": 55,\n",
        "        \"y\": 56,\n",
        "        \"z\": 57,\n",
        "        \"~\": 58,\n",
        "    }\n",
        "    char_dict[\"P\"] = 59\n",
        "    #char_dict[\"SOS\"] = 60\n",
        "    #char_dict[\"EOS\"] = 61\n",
        "    return char_dict\n",
        "\n",
        "\n",
        "class Constants:\n",
        "    ROWS_PER_FRAME = 543\n",
        "    MAX_STRING_LEN = 50\n",
        "    INPUT_PAD = -100.0\n",
        "    char_dict = get_char_dict()\n",
        "    LABEL_PAD = char_dict[\"P\"]\n",
        "    inv_dict = {v: k for k, v in char_dict.items()}\n",
        "    NOSE = [1, 2, 98, 327]\n",
        "\n",
        "    REYE = [33, 7, 163, 144, 145, 153, 154, 155, 133, 246, 161, 160, 159, 158, 157, 173]\n",
        "    LEYE = [263, 249, 390, 373, 374, 380, 381, 382, 362, 466, 388, 387, 386, 385, 384, 398]\n",
        "\n",
        "    LHAND = list(range(468, 489))\n",
        "    RHAND = list(range(522, 543))\n",
        "\n",
        "    LNOSE = [98]\n",
        "    RNOSE = [327]\n",
        "\n",
        "    LLIP = [84, 181, 91, 146, 61, 185, 40, 39, 37, 87, 178, 88, 95, 78, 191, 80, 81, 82]\n",
        "    RLIP = [\n",
        "        314,\n",
        "        405,\n",
        "        321,\n",
        "        375,\n",
        "        291,\n",
        "        409,\n",
        "        270,\n",
        "        269,\n",
        "        267,\n",
        "        317,\n",
        "        402,\n",
        "        318,\n",
        "        324,\n",
        "        308,\n",
        "        415,\n",
        "        310,\n",
        "        311,\n",
        "        312,\n",
        "    ]\n",
        "    POSE = [500, 502, 504, 501, 503, 505, 512, 513]\n",
        "    LPOSE = [513, 505, 503, 501]\n",
        "    RPOSE = [512, 504, 502, 500]\n",
        "\n",
        "    POINT_LANDMARKS_PARTS = [LHAND, RHAND, LLIP, RLIP, LPOSE, RPOSE, NOSE, REYE, LEYE]\n",
        "    # POINT_LANDMARKS_PARTS = [LHAND, RHAND, NOSE]\n",
        "    POINT_LANDMARKS = [item for sublist in POINT_LANDMARKS_PARTS for item in sublist]\n",
        "    parts = {\n",
        "        \"LLIP\": LLIP,\n",
        "        \"RLIP\": RLIP,\n",
        "        \"LHAND\": LHAND,\n",
        "        \"RHAND\": RHAND,\n",
        "        \"LPOSE\": LPOSE,\n",
        "        \"RPOSE\": RPOSE,\n",
        "        \"LNOSE\": LNOSE,\n",
        "        \"RNOSE\": RNOSE,\n",
        "        \"REYE\": REYE,\n",
        "        \"LEYE\": LEYE,\n",
        "    }\n",
        "\n",
        "    LANDMARK_INDICES = {}  # type: ignore\n",
        "    for part in parts:\n",
        "        LANDMARK_INDICES[part] = []\n",
        "        for landmark in parts[part]:\n",
        "            if landmark in POINT_LANDMARKS:\n",
        "                LANDMARK_INDICES[part].append(POINT_LANDMARKS.index(landmark))\n",
        "\n",
        "    CENTER_LANDMARKS = LNOSE + RNOSE\n",
        "    CENTER_INDICES = LANDMARK_INDICES[\"LNOSE\"] + LANDMARK_INDICES[\"RNOSE\"]\n",
        "\n",
        "    NUM_NODES = len(POINT_LANDMARKS)\n",
        "    NUM_INPUT_FEATURES = 2 * NUM_NODES # (x,y)\n",
        "    CHANNELS = 6 * NUM_NODES #(x,y,dx,dy,dx2,dy2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMEA9LflYi14"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "P-kprZ1_Yi14"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Seed all random number generators\n",
        "def seed_everything(seed=42):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "def selected_columns(file_example):\n",
        "    df = pd.read_parquet(file_example)\n",
        "    selected_x = df.columns[[x + 1 for x in Constants.POINT_LANDMARKS]].tolist()\n",
        "    selected_y = [c.replace(\"x\", \"y\") for c in selected_x]\n",
        "    selected = []\n",
        "    for i in range(Constants.NUM_NODES):\n",
        "        selected.append(selected_x[i])\n",
        "        selected.append(selected_y[i])\n",
        "    return selected  # x1,y1,x2,y2,...\n",
        "\n",
        "\n",
        "\n",
        "def num_to_char_fn(y):\n",
        "    return [Constants.inv_dict.get(x, \"\") for x in y]\n",
        "\n",
        "\n",
        "# A callback class to output a few transcriptions during training\n",
        "class CallbackEval(tf.keras.callbacks.Callback):\n",
        "    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n",
        "\n",
        "    def __init__(self, model, dataset):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "        self.model = model\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs=None):\n",
        "        predictions = []\n",
        "        targets = []\n",
        "        for batch in self.dataset:\n",
        "            X, y = batch\n",
        "            batch_predictions = self.model(X)\n",
        "            batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "            predictions.extend(batch_predictions)\n",
        "            for label in y:\n",
        "                label = \"\".join(num_to_char_fn(label.numpy()))\n",
        "                targets.append(label)\n",
        "        print(\"-\" * 100)\n",
        "        # for i in np.random.randint(0, len(predictions), 2):\n",
        "        for i in range(10):\n",
        "            print(f\"Target    : {targets[i]}\")\n",
        "            print(f\"Prediction: {predictions[i]}, len: {len(predictions[i])}\")\n",
        "            print(\"-\" * 100)\n",
        "\n",
        "\n",
        "def decode_phrase(pred):\n",
        "    # decode cts prediction by prunning\n",
        "    # (T,CHAR_NUMS)\n",
        "    x = tf.argmax(pred, axis=1)\n",
        "    paddings = tf.constant(\n",
        "        [\n",
        "            [0, 1],\n",
        "        ]\n",
        "    )\n",
        "    x = tf.pad(x, paddings)\n",
        "    diff = tf.not_equal(x[:-1], x[1:])\n",
        "    adjacent_indices = tf.where(diff)[:, 0]\n",
        "    x = tf.gather(x, adjacent_indices)\n",
        "    mask = x != Constants.LABEL_PAD\n",
        "    x = tf.boolean_mask(x, mask, axis=0)\n",
        "    return x\n",
        "\n",
        "\n",
        "# A utility function to decode the output of the network\n",
        "def decode_batch_predictions(pred):\n",
        "    output_text = []\n",
        "    for result in pred:\n",
        "        result = \"\".join(num_to_char_fn(decode_phrase(result).numpy()))\n",
        "        output_text.append(result)\n",
        "    return output_text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def code_to_label(label_code):\n",
        "    label = [Constants.inv_dict[x] for x in label_code if Constants.inv_dict[x] != \"P\"]\n",
        "    label = \"\".join(label)\n",
        "    return label\n",
        "\n",
        "\n",
        "def convert_to_strings(batch_label_code):\n",
        "    output = []\n",
        "    for label_code in batch_label_code:\n",
        "        output.append(code_to_label(label_code))\n",
        "    return output\n",
        "\n",
        "\n",
        "def global_metric(val_ds, model):\n",
        "    global_N, global_D = 0, 0\n",
        "    count = 0\n",
        "    metric = LevDistanceMetric()\n",
        "    for batch in val_ds:\n",
        "        count += 1\n",
        "        print(count)\n",
        "        feature, label = batch\n",
        "        logits = model(feature)\n",
        "        _, _, D = batch_edit_distance(label, logits)\n",
        "        metric.update_state(label, logits)\n",
        "\n",
        "    result = metric.result().numpy()\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def sparse_from_dense_ignore_value(dense_tensor):\n",
        "    mask = tf.not_equal(dense_tensor, Constants.LABEL_PAD)\n",
        "    indices = tf.where(mask)\n",
        "    values = tf.boolean_mask(dense_tensor, mask)\n",
        "\n",
        "    return tf.SparseTensor(indices, values, tf.shape(dense_tensor, out_type=tf.int64))\n",
        "\n",
        "\n",
        "def batch_edit_distance(y_true, y_logits):\n",
        "    blank = Constants.LABEL_PAD\n",
        "    #y_true=tf.ensure_shape(y_true,(128,Constants.MAX_STRING_LEN))\n",
        "    #y_logits=tf.ensure_shape(y_logits,(128,128,60))\n",
        "    #tf.print(\"edit distance true shape\",tf.shape(y_true))\n",
        "    #tf.print(\"edit distance logits shape\",tf.shape(y_logits))\n",
        "\n",
        "    B = tf.shape(y_logits)[0]\n",
        "    seq_length = tf.shape(y_logits)[1]\n",
        "    to_decode = tf.transpose(y_logits, perm=[1, 0, 2])\n",
        "    sequence_length = tf.fill(dims=[B], value=seq_length)\n",
        "    hypothesis = tf.nn.ctc_greedy_decoder(\n",
        "        tf.cast(to_decode, tf.float32), sequence_length, blank_index=blank\n",
        "    )[0][\n",
        "        0\n",
        "    ]  # full is [B,...]\n",
        "    truth = sparse_from_dense_ignore_value(y_true)  # full is [B,...]\n",
        "    truth = tf.cast(truth, tf.int64)\n",
        "    edit_dist = tf.edit_distance(hypothesis, truth, normalize=False)\n",
        "\n",
        "    non_ignore_mask = tf.not_equal(y_true, blank)\n",
        "    N = tf.reduce_sum(tf.cast(non_ignore_mask, tf.float32))\n",
        "    D = tf.reduce_sum(edit_dist)\n",
        "    result = (N - D) / N\n",
        "    result = tf.clip_by_value(result, 0.0, 1.0)\n",
        "    return result, N, D\n",
        "\n",
        "\n",
        "class LevDistanceMetric(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name=\"Lev\", **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.distance = self.add_weight(name=\"dist\", initializer=\"zeros\")\n",
        "        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
        "\n",
        "    def update_state(self, y_true, y_logits, sample_weight=None):\n",
        "        # if using with keras compile, make sure the model outputs logits, not softmax probabilities\n",
        "        _, N, D = batch_edit_distance(y_true, y_logits)\n",
        "        self.distance.assign_add(D)\n",
        "        self.count.assign_add(N)\n",
        "\n",
        "    def result(self):\n",
        "        result = (self.count - self.distance) / self.count\n",
        "        result = tf.clip_by_value(result, 0.0, 1.0)\n",
        "        return result\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.count.assign(0.0)\n",
        "        self.distance.assign(0.0)\n",
        "\n",
        "\n",
        "\n",
        "class SWA(tf.keras.callbacks.Callback):\n",
        "    # Stochastic Weight Averaging\n",
        "    def __init__(\n",
        "        self,\n",
        "        save_name,\n",
        "        swa_epochs=[],\n",
        "        strategy=None,\n",
        "        train_ds=None,\n",
        "        valid_ds=None,\n",
        "        train_steps=1000,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.swa_epochs = swa_epochs\n",
        "        self.swa_weights = None\n",
        "        self.save_name = save_name\n",
        "        self.train_ds = train_ds\n",
        "        self.valid_ds = valid_ds\n",
        "        self.train_steps = train_steps\n",
        "        self.strategy = strategy\n",
        "\n",
        "    # @tf.function(jit_compile=True)\n",
        "    def train_step(self, iterator):\n",
        "        \"\"\"The step function for one training step.\"\"\"\n",
        "\n",
        "        def step_fn(inputs):\n",
        "            \"\"\"The computation to run on each device.\"\"\"\n",
        "            x, y = inputs\n",
        "            _ = self.model(x, training=True)\n",
        "\n",
        "        for x in iterator:\n",
        "            self.strategy.run(step_fn, args=(x,))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch in self.swa_epochs:\n",
        "            if self.swa_weights is None:\n",
        "                self.swa_weights = self.model.get_weights()\n",
        "            else:\n",
        "                w = self.model.get_weights()\n",
        "                for i in range(len(self.swa_weights)):\n",
        "                    self.swa_weights[i] += w[i]\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if len(self.swa_epochs):\n",
        "            print(\"applying SWA...\")\n",
        "            for i in range(len(self.swa_weights)):\n",
        "                self.swa_weights[i] = self.swa_weights[i] / len(self.swa_epochs)\n",
        "            self.model.set_weights(self.swa_weights)\n",
        "            if self.train_ds is not None:  # for the re-calculation of running mean and var\n",
        "                self.train_step(self.train_ds.take(self.train_steps))\n",
        "            print(f\"save SWA weights to {self.save_name}-SWA.h5\")\n",
        "            self.model.save_weights(f\"{self.save_name}-SWA.h5\")\n",
        "            if self.valid_ds is not None:\n",
        "                self.model.evaluate(self.valid_ds)\n",
        "\n",
        "\n",
        "class AWP(tf.keras.Model):\n",
        "    # Adversarial Weight Perturbation\n",
        "    def __init__(self, *args, delta=0.1, eps=1e-4, start_step=0, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.delta = delta\n",
        "        self.eps = eps\n",
        "        self.start_step = start_step\n",
        "\n",
        "    def train_step_awp(self, data):\n",
        "        # Unpack the data. Its structure depends on your model and\n",
        "        # on what you pass to `fit()`.\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "        params = self.trainable_variables\n",
        "        params_gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        for i in range(len(params_gradients)):\n",
        "            grad = tf.zeros_like(params[i]) + params_gradients[i]\n",
        "            delta = tf.math.divide_no_nan(\n",
        "                self.delta * grad, tf.math.sqrt(tf.reduce_sum(grad**2)) + self.eps\n",
        "            )\n",
        "            self.trainable_variables[i].assign_add(delta)\n",
        "        with tf.GradientTape() as tape2:\n",
        "            y_pred = self(x, training=True)\n",
        "            new_loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "            if hasattr(self.optimizer, \"get_scaled_loss\"):\n",
        "                new_loss = self.optimizer.get_scaled_loss(new_loss)\n",
        "\n",
        "        gradients = tape2.gradient(new_loss, self.trainable_variables)\n",
        "        if hasattr(self.optimizer, \"get_unscaled_gradients\"):\n",
        "            gradients = self.optimizer.get_unscaled_gradients(gradients)\n",
        "        for i in range(len(params_gradients)):\n",
        "            grad = tf.zeros_like(params[i]) + params_gradients[i]\n",
        "            delta = tf.math.divide_no_nan(\n",
        "                self.delta * grad, tf.math.sqrt(tf.reduce_sum(grad**2)) + self.eps\n",
        "            )\n",
        "            self.trainable_variables[i].assign_sub(delta)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        # self_loss.update_state(loss)\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def train_step(self, data):\n",
        "        return tf.cond(\n",
        "            self._train_counter < self.start_step,\n",
        "            lambda: super(AWP, self).train_step(data),\n",
        "            lambda: self.train_step_awp(data),\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQAPz56nYi15"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wnoZyLAcYi15"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CTCLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, pad_token_idx,batch_size,max_string_len,output_dim,output_steps,replicas):\n",
        "        self.pad_token_idx = pad_token_idx\n",
        "        self.batch_size=batch_size\n",
        "        self.max_string_len=max_string_len\n",
        "        self.output_steps=output_steps\n",
        "        self.output_dim=output_dim\n",
        "        self.replicas=replicas\n",
        "        super().__init__()\n",
        "\n",
        "    def call(self, labels, logits):\n",
        "\n",
        "        #logits=tf.ensure_shape(logits,(self.batch_size//self.replicas,self.output_steps,self.output_dim))\n",
        "        #labels=tf.ensure_shape(labels,(self.batch_size//self.replicas,self.max_string_len))\n",
        "        label_length = tf.reduce_sum(tf.cast(labels != self.pad_token_idx, tf.int32), axis=-1)\n",
        "        logit_length = tf.ones(tf.shape(logits)[0], dtype=tf.int32) * tf.shape(logits)[1]\n",
        "\n",
        "        ctc_loss = tf.nn.ctc_loss(\n",
        "            labels=labels,\n",
        "            logits=logits,\n",
        "            label_length=label_length,\n",
        "            logit_length=logit_length,\n",
        "            blank_index=self.pad_token_idx,\n",
        "            logits_time_major=False,\n",
        "        )\n",
        "\n",
        "        return ctc_loss\n",
        "\n",
        "\n",
        "class ECA(tf.keras.layers.Layer):\n",
        "    # Efficient Channel Attention\n",
        "    def __init__(self, kernel_size=5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.kernel_size = kernel_size\n",
        "        self.conv = tf.keras.layers.Conv1D(\n",
        "            1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n",
        "        nn = tf.expand_dims(nn, -1)\n",
        "        nn = self.conv(nn)\n",
        "        nn = tf.squeeze(nn, -1)\n",
        "        nn = tf.nn.sigmoid(nn)\n",
        "        nn = nn[:, None, :]\n",
        "        return inputs * nn\n",
        "\n",
        "\n",
        "class LateDropout(tf.keras.layers.Layer):\n",
        "    def __init__(self, rate, noise_shape=None, start_step=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.rate = rate\n",
        "        self.start_step = start_step\n",
        "        self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super().build(input_shape)\n",
        "        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n",
        "        self._train_counter = tf.Variable(0, dtype=\"int64\", aggregation=agg, trainable=False)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = tf.cond(\n",
        "            self._train_counter < self.start_step,\n",
        "            lambda: inputs,\n",
        "            lambda: self.dropout(inputs, training=training),\n",
        "        )\n",
        "        if training:\n",
        "            self._train_counter.assign_add(1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CausalDWConv1D(tf.keras.layers.Layer):\n",
        "    # Causal Depth Wise Convolution\n",
        "    def __init__(\n",
        "        self,\n",
        "        kernel_size=17,\n",
        "        dilation_rate=1,\n",
        "        use_bias=False,\n",
        "        depthwise_initializer=\"glorot_uniform\",\n",
        "        name=\"\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.causal_pad = tf.keras.layers.ZeroPadding1D(\n",
        "            (dilation_rate * (kernel_size - 1), 0), name=name + \"_pad\"\n",
        "        )\n",
        "        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n",
        "            kernel_size,\n",
        "            strides=1,\n",
        "            dilation_rate=dilation_rate,\n",
        "            padding=\"valid\",\n",
        "            use_bias=use_bias,\n",
        "            depthwise_initializer=depthwise_initializer,\n",
        "            name=name + \"_dwconv\",\n",
        "        )\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.causal_pad(inputs)\n",
        "        x = self.dw_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def Conv1DBlock(\n",
        "    channel_size,\n",
        "    kernel_size,\n",
        "    dilation_rate=1,\n",
        "    drop_rate=0.0,\n",
        "    expand_ratio=2,\n",
        "    # se_ratio=0.25,\n",
        "    activation=\"swish\",\n",
        "    name=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    efficient conv1d block, @hoyso48\n",
        "    \"\"\"\n",
        "    if name is None:\n",
        "        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n",
        "\n",
        "    # Expansion phase\n",
        "    def apply(inputs):\n",
        "        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n",
        "        channels_expand = channels_in * expand_ratio\n",
        "\n",
        "        skip = inputs\n",
        "\n",
        "        x = tf.keras.layers.Dense(\n",
        "            channels_expand, use_bias=True, activation=activation, name=name + \"_expand_conv\"\n",
        "        )(inputs)\n",
        "\n",
        "        # Depthwise Convolution\n",
        "        x = CausalDWConv1D(\n",
        "            kernel_size, dilation_rate=dilation_rate, use_bias=False, name=name + \"_dwconv\"\n",
        "        )(x)\n",
        "\n",
        "        #x = tf.keras.layers.LayerNormalization(name=name + \"_bn\")(x)\n",
        "        x = tf.keras.layers.BatchNormalization(name=name + \"_bn\")(x)\n",
        "\n",
        "        x = ECA()(x)  # efficient channel attention\n",
        "\n",
        "        x = tf.keras.layers.Dense(channel_size, use_bias=True, name=name + \"_project_conv\")(x)\n",
        "\n",
        "        if drop_rate > 0:\n",
        "            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1), name=name + \"_drop\")(x)\n",
        "\n",
        "        if channels_in == channel_size:\n",
        "            x = tf.keras.layers.add([x, skip], name=name + \"_add\")\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "\n",
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dim = dim\n",
        "        self.scale = self.dim**-0.5\n",
        "        self.num_heads = num_heads\n",
        "        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n",
        "        self.drop1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        qkv = self.qkv(inputs)\n",
        "        qkv = tf.keras.layers.Permute((2, 1, 3))(\n",
        "            tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv)\n",
        "        )\n",
        "        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n",
        "\n",
        "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask[:, None, None, :]\n",
        "\n",
        "        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n",
        "        attn = self.drop1(attn)\n",
        "\n",
        "        x = attn @ v\n",
        "        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def TransformerBlock(\n",
        "    dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation=\"swish\"\n",
        "):\n",
        "    def apply(inputs):\n",
        "        x = inputs\n",
        "        x = tf.keras.layers.LayerNormalization()(x)\n",
        "        x = MultiHeadSelfAttention(dim=dim, num_heads=num_heads, dropout=attn_dropout)(x)\n",
        "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1))(x)\n",
        "        x = tf.keras.layers.Add()([inputs, x])\n",
        "        attn_out = x\n",
        "\n",
        "        x = tf.keras.layers.LayerNormalization()(x)\n",
        "        x = tf.keras.layers.Dense(dim * expand, use_bias=False, activation=activation)(x)\n",
        "        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n",
        "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None, 1, 1))(x)\n",
        "        x = tf.keras.layers.Add()([attn_out, x])\n",
        "        return x\n",
        "\n",
        "    return apply\n",
        "\n",
        "def build_model1(\n",
        "    output_dim,\n",
        "    max_len=64,\n",
        "    dropout_step=0,\n",
        "    dim=192,\n",
        "    input_pad=-100,\n",
        "    with_transformer=False,\n",
        "    drop_rate=0.2,\n",
        "):\n",
        "    inp = tf.keras.Input(shape=(max_len, Constants.CHANNELS), dtype=tf.float32, name=\"inputs\")\n",
        "    x = tf.keras.layers.Masking(mask_value=input_pad, input_shape=(max_len, Constants.CHANNELS))(\n",
        "        inp\n",
        "    )\n",
        "    ksize = 17\n",
        "    x = tf.keras.layers.Dense(dim, use_bias=False, name=\"stem_conv\")(x)\n",
        "    #x = tf.keras.layers.LayerNormalization(name=\"stem_bn\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization(name=\"stem_bn\")(x)\n",
        "\n",
        "    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "    if with_transformer:\n",
        "        x = TransformerBlock(dim, expand=2)(x)\n",
        "\n",
        "    #x = tf.keras.layers.AvgPool1D(2, 2)(x)\n",
        "\n",
        "\n",
        "    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "    x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "    if with_transformer:\n",
        "        x = TransformerBlock(dim, expand=2)(x)\n",
        "\n",
        "    x = tf.keras.layers.AvgPool1D(2, 2)(x) # [B,T,dim]\n",
        "\n",
        "    lstm2 = tf.keras.layers.LSTM(units=dim//2, return_sequences=True,dtype=\"float32\") #[B,T,dim//2]\n",
        "    x2 = tf.keras.layers.Bidirectional(lstm2)(x) #[B,T,dim]\n",
        "\n",
        "    x2=tf.keras.layers.BatchNormalization()(x2)\n",
        "    x2=tf.keras.layers.Dense(output_dim)(x2)\n",
        "    soft=tf.keras.layers.Activation('softmax', dtype='float32')(x2)\n",
        "    logsoft=tf.keras.layers.Activation('log_softmax',dtype='float32',name=\"internal\")(x2)\n",
        "\n",
        "    x=tf.keras.layers.Dense(dim)(soft)+x\n",
        "    x=tf.keras.layers.BatchNormalization()(x)\n",
        "    if dim == 384:  # for the 4x sized model\n",
        "        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "        if with_transformer:\n",
        "            x = TransformerBlock(dim, expand=2)(x)\n",
        "\n",
        "        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "        x = Conv1DBlock(dim, ksize, drop_rate=drop_rate)(x)\n",
        "        if with_transformer:\n",
        "            x = TransformerBlock(dim, expand=2)(x)\n",
        "\n",
        "\n",
        "    lstm = tf.keras.layers.LSTM(units=dim//2, return_sequences=True,dtype=\"float32\")\n",
        "    x = tf.keras.layers.Bidirectional(lstm)(x)\n",
        "\n",
        "    x = LateDropout(0.6, start_step=dropout_step)(x)\n",
        "\n",
        "    # x = tf.keras.layers.LayerNormalization()(x)\n",
        "\n",
        "    #output = tf.keras.layers.Dense(output_dim, activation=\"log_softmax\",name=\"final_logsoft\")(x)  # logits\n",
        "    x=tf.keras.layers.Dense(output_dim)(x)\n",
        "    output = tf.keras.layers.Activation(\"log_softmax\",name=\"final\",dtype=\"float32\")(x)  # logits\n",
        "\n",
        "    model = tf.keras.Model(inp, outputs=[output,logsoft])\n",
        "    #model = tf.keras.Model(inp, outputs=output)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_model(output_dim, max_len, dim, input_pad,dropout_step,drop_rate):\n",
        "\n",
        "    model = build_model1(output_dim, max_len=max_len, input_pad=input_pad, dim=dim,  dropout_step=dropout_step,drop_rate=drop_rate)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piKWNcVHYi15"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7e1LGyFWYi16"
      },
      "outputs": [],
      "source": [
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def get_strategy():\n",
        "    logical_devices = tf.config.list_logical_devices()\n",
        "    # Check if TPU is available\n",
        "\n",
        "    gpu_available = any(\"GPU\" in device.name for device in logical_devices)\n",
        "    strategy = None\n",
        "    is_tpu = False\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        print(\"Running on TPU \", tpu.master())\n",
        "        is_tpu = True\n",
        "    except ValueError:\n",
        "        is_tpu = False\n",
        "\n",
        "    if is_tpu:\n",
        "        tf.config.experimental_connect_to_cluster(tpu)\n",
        "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "\n",
        "        print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "        strategy = tf.distribute.TPUStrategy(tpu)\n",
        "        #disable_eager_execution()  # LSTM layer can't use bfloat16 unless we do this.\n",
        "\n",
        "    else:\n",
        "        if gpu_available:\n",
        "\n",
        "            ngpu = len(gpus)\n",
        "            print(\"Num GPUs Available: \", ngpu)\n",
        "            if ngpu > 1:\n",
        "                strategy = tf.distribute.MirroredStrategy()\n",
        "            else:\n",
        "                strategy = tf.distribute.get_strategy()\n",
        "\n",
        "        else:\n",
        "            print(\"Runing on CPU\")\n",
        "            strategy = tf.distribute.get_strategy()\n",
        "    replicas = strategy.num_replicas_in_sync\n",
        "\n",
        "    print(f\"get strategy replicas: {replicas}\")\n",
        "\n",
        "    return strategy, replicas, is_tpu\n",
        "\n",
        "\n",
        "class CFG:\n",
        "    # These 3 variables are update dynamically later by calling update_config_with_strategy.\n",
        "    strategy = None  # type: ignore\n",
        "    replicas = 1\n",
        "    is_tpu = False\n",
        "\n",
        "    save_output = True\n",
        "    input_path = \"/kaggle/input\"\n",
        "    output_path = \"/kaggle/working\"\n",
        "\n",
        "    seed = 42\n",
        "    verbose = 1  # 0) silent 1) progress bar 2) one line per epoch\n",
        "\n",
        "    # max number of frames\n",
        "    #max_len = 256\n",
        "    max_len = 256\n",
        "    replicas = 1\n",
        "\n",
        "    lr = 3e-4   # 5e-4\n",
        "    weight_decay = 1e-4  # 4e-4\n",
        "    epochs = 300\n",
        "\n",
        "    batch_size=128\n",
        "\n",
        "    snapshot_epochs = []  # type: ignore\n",
        "    swa_epochs = list(range(3*(epochs//4),epochs+1))\n",
        "\n",
        "    fp16=False\n",
        "\n",
        "    awp = True\n",
        "    awp_lambda = 0.1\n",
        "    awp_start_epoch = 15\n",
        "    dropout_start_epoch = 15\n",
        "    resume = 0\n",
        "\n",
        "    dim = 384\n",
        "\n",
        "    comment = f\"model-{dim}-seed{seed}\"\n",
        "    output_dim = 60\n",
        "    num_eval = 6\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HCKh-AxTYi16"
      },
      "outputs": [],
      "source": [
        "\n",
        "def update_config_with_strategy(config):\n",
        "    # cfg is configuration instance\n",
        "    #strategy, replicas, is_tpu = get_strategy()\n",
        "    if tpu_strategy is not None:\n",
        "      strategy=tpu_strategy\n",
        "      replicas=8\n",
        "      is_tpu=True\n",
        "      config.input_path=config.input_path.replace(\"/kaggle\",\"gs://asl-bucket71\")\n",
        "      config.output_path = config.output_path.replace(\"/kaggle\",\"/content/drive/MyDrive/kaggle\")\n",
        "\n",
        "    else:\n",
        "      strategy,replicas,is_tpu=get_strategy()\n",
        "    print(\"Strategy\",strategy)\n",
        "\n",
        "    config.strategy = strategy\n",
        "    config.replicas = replicas\n",
        "    config.is_tpu = is_tpu\n",
        "    config.lr = config.lr * replicas\n",
        "    config.batch_size = config.batch_size * replicas\n",
        "    return config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5msRCmt0Yi16"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrUlKKykrs3c"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "j_tHWSs8Yi16"
      },
      "outputs": [],
      "source": [
        "\n",
        "def count_data_items(dataset):\n",
        "    dataset_size = 0\n",
        "    for _ in dataset:\n",
        "        dataset_size += 1\n",
        "    return dataset_size\n",
        "\n",
        "\n",
        "def interp1d_(x, target_len):\n",
        "    target_len = tf.maximum(1, target_len)\n",
        "    x = tf.image.resize(x, (target_len, tf.shape(x)[1]))\n",
        "    return x\n",
        "\n",
        "\n",
        "def tf_nan_mean(x, axis=0, keepdims=False):\n",
        "    return tf.reduce_sum(\n",
        "        tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims\n",
        "    ) / tf.reduce_sum(\n",
        "        tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims\n",
        "    )\n",
        "\n",
        "\n",
        "def tf_nan_std(x, center=None, axis=0, keepdims=False):\n",
        "    if center is None:\n",
        "        center = tf_nan_mean(x, axis=axis, keepdims=True)\n",
        "    d = x - center\n",
        "    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n",
        "\n",
        "\n",
        "def flip_lr(x):\n",
        "    if x.shape[1] == Constants.ROWS_PER_FRAME:\n",
        "        LHAND = Constants.LHAND\n",
        "        RHAND = Constants.RHAND\n",
        "        LLIP = Constants.LLIP\n",
        "        RLIP = Constants.RLIP\n",
        "        LEYE = Constants.LEYE\n",
        "        REYE = Constants.REYE\n",
        "        LNOSE = Constants.LNOSE\n",
        "        RNOSE = Constants.RNOSE\n",
        "        LPOSE = Constants.LPOSE\n",
        "        RPOSE = Constants.RPOSE\n",
        "    else:\n",
        "        LHAND = Constants.LANDMARK_INDICES[\"LHAND\"]\n",
        "        RHAND = Constants.LANDMARK_INDICES[\"RHAND\"]\n",
        "        LLIP = Constants.LANDMARK_INDICES[\"LLIP\"]\n",
        "        RLIP = Constants.LANDMARK_INDICES[\"RLIP\"]\n",
        "        LEYE = Constants.LANDMARK_INDICES[\"LEYE\"]\n",
        "        REYE = Constants.LANDMARK_INDICES[\"REYE\"]\n",
        "        LNOSE = Constants.LANDMARK_INDICES[\"LNOSE\"]\n",
        "        RNOSE = Constants.LANDMARK_INDICES[\"RNOSE\"]\n",
        "        LPOSE = Constants.LANDMARK_INDICES[\"LPOSE\"]\n",
        "        RPOSE = Constants.LANDMARK_INDICES[\"RPOSE\"]\n",
        "\n",
        "    x, y = tf.unstack(x, axis=-1)\n",
        "    x = 1 - x\n",
        "    new_x = tf.stack([x, y], -1)\n",
        "    new_x = tf.transpose(new_x, [1, 0, 2])\n",
        "    lhand = tf.gather(new_x, LHAND, axis=0)\n",
        "    rhand = tf.gather(new_x, RHAND, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LHAND)[..., None], rhand)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RHAND)[..., None], lhand)\n",
        "    llip = tf.gather(new_x, LLIP, axis=0)\n",
        "    rlip = tf.gather(new_x, RLIP, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LLIP)[..., None], rlip)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RLIP)[..., None], llip)\n",
        "    lpose = tf.gather(new_x, LPOSE, axis=0)\n",
        "    rpose = tf.gather(new_x, RPOSE, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LPOSE)[..., None], rpose)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RPOSE)[..., None], lpose)\n",
        "    leye = tf.gather(new_x, LEYE, axis=0)\n",
        "    reye = tf.gather(new_x, REYE, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LEYE)[..., None], reye)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(REYE)[..., None], leye)\n",
        "    lnose = tf.gather(new_x, LNOSE, axis=0)\n",
        "    rnose = tf.gather(new_x, RNOSE, axis=0)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(LNOSE)[..., None], rnose)\n",
        "    new_x = tf.tensor_scatter_nd_update(new_x, tf.constant(RNOSE)[..., None], lnose)\n",
        "    new_x = tf.transpose(new_x, [1, 0, 2])\n",
        "    return new_x\n",
        "\n",
        "\n",
        "def resample(x, rate=(0.8, 1.2)):\n",
        "    rate = tf.random.uniform((), rate[0], rate[1])\n",
        "    length = tf.shape(x)[0]\n",
        "    new_size = tf.cast(rate * tf.cast(length, tf.float32), tf.int32)\n",
        "    new_x = interp1d_(x, new_size)\n",
        "    return new_x\n",
        "\n",
        "\n",
        "def spatial_random_affine(\n",
        "    xyz,\n",
        "    scale=(0.8, 1.2),\n",
        "    shear=(-0.1, 0.1),\n",
        "    shift=(-0.1, 0.1),\n",
        "    degree=(-20, 20),\n",
        "):\n",
        "    center = tf.constant([0.5, 0.5])\n",
        "    if degree is not None:\n",
        "        xy = xyz[..., :2]\n",
        "        z = xyz[..., 2:]\n",
        "        xy -= center\n",
        "        degree = tf.random.uniform((), *degree)\n",
        "        radian = degree / 180 * np.pi\n",
        "        c = tf.math.cos(radian)\n",
        "        s = tf.math.sin(radian)\n",
        "        rotate_mat = tf.identity(\n",
        "            [\n",
        "                [c, s],\n",
        "                [-s, c],\n",
        "            ]\n",
        "        )\n",
        "        xy = xy @ rotate_mat\n",
        "        xy = xy + center\n",
        "        xyz = tf.concat([xy, z], axis=-1)\n",
        "\n",
        "    if scale is not None:\n",
        "        scale = tf.random.uniform((), *scale)\n",
        "        xyz = scale * xyz\n",
        "\n",
        "    if shear is not None:\n",
        "        xy = xyz[..., :2]\n",
        "        z = xyz[..., 2:]\n",
        "        shear_x = shear_y = tf.random.uniform((), *shear)\n",
        "        if tf.random.uniform(()) < 0.5:\n",
        "            shear_x = 0.0\n",
        "        else:\n",
        "            shear_y = 0.0\n",
        "        shear_mat = tf.identity([[1.0, shear_x], [shear_y, 1.0]])\n",
        "        xy = xy @ shear_mat\n",
        "        xyz = tf.concat([xy, z], axis=-1)\n",
        "\n",
        "    if shift is not None:\n",
        "        shift = tf.random.uniform((), *shift)\n",
        "        xyz = xyz + shift\n",
        "\n",
        "    return xyz\n",
        "\n",
        "\n",
        "def temporal_mask(x, size=[1, 15], mask_value=float(\"nan\")):\n",
        "    l0 = tf.shape(x)[0]\n",
        "    if size[1] > l0 // 8:\n",
        "        size[1] = l0 // 8\n",
        "        if size[1] <= 1:\n",
        "            size[1] = 2\n",
        "    mask_size = tf.random.uniform((), *size, dtype=tf.int32)\n",
        "    mask_offset = tf.random.uniform((), 0, tf.clip_by_value(l0 - mask_size, 1, l0), dtype=tf.int32)\n",
        "    x = tf.tensor_scatter_nd_update(\n",
        "        x,\n",
        "        tf.range(mask_offset, mask_offset + mask_size)[..., None],\n",
        "        tf.fill([mask_size, tf.shape(x)[1], 2], mask_value),\n",
        "    )\n",
        "    return x\n",
        "\n",
        "\n",
        "def spatial_mask(x, size=(0.05, 0.2), mask_value=float(\"nan\")):\n",
        "    mask_offset_y = tf.random.uniform(())\n",
        "    mask_offset_x = tf.random.uniform(())\n",
        "    mask_size = tf.random.uniform((), *size)\n",
        "    mask_x = (mask_offset_x < x[..., 0]) & (x[..., 0] < mask_offset_x + mask_size)\n",
        "    mask_y = (mask_offset_y < x[..., 1]) & (x[..., 1] < mask_offset_y + mask_size)\n",
        "    mask = mask_x & mask_y\n",
        "    x = tf.where(mask[..., None], mask_value, x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "def augment_fn(x):\n",
        "    # shape (T,F)\n",
        "    x = tf.reshape(x, (tf.shape(x)[0], -1, 2))\n",
        "    if tf.random.uniform(()) < 0.6:\n",
        "        x = resample(x, (0.5, 1.5))\n",
        "    if tf.random.uniform(()) < 0.6:\n",
        "        x = flip_lr(x)\n",
        "    if tf.random.uniform(()) < 0.6:\n",
        "        x = spatial_random_affine(x)\n",
        "    if tf.random.uniform(()) < 0.4:\n",
        "        x = temporal_mask(x)\n",
        "    if tf.random.uniform(()) < 0.4:\n",
        "        x = spatial_mask(x)\n",
        "    x = tf.reshape(x, (tf.shape(x)[0], -1))\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jYUGdwEDYi17"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Preprocess(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_len, normalize=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.max_len = max_len\n",
        "        self.center = Constants.CENTER_INDICES\n",
        "        self.normalize = normalize\n",
        "\n",
        "    # preprocess a batch of data\n",
        "    def call(self, x):\n",
        "        # rank is 3: [B,T,F]\n",
        "        # if your input is just [T,F], extend its dimesnion before calling.\n",
        "\n",
        "        x = tf.reshape(x, (tf.shape(x)[0], tf.shape(x)[1], Constants.NUM_NODES, 2))\n",
        "        # dimensions now are [B,T,F//2,2]\n",
        "\n",
        "        x_selected = x\n",
        "        if self.normalize:\n",
        "            mean = tf_nan_mean(tf.gather(x, self.center, axis=2), axis=[1, 2], keepdims=True)\n",
        "            mean = tf.where(tf.math.is_nan(mean), tf.constant(0.5, x.dtype), mean)\n",
        "            std = tf_nan_std(x_selected, center=mean, axis=[1, 2], keepdims=True)\n",
        "            x = (x_selected - mean) / std\n",
        "        else:\n",
        "            x = x_selected\n",
        "\n",
        "        dx = tf.cond(\n",
        "            tf.shape(x)[1] > 1,\n",
        "            lambda: tf.pad(x[:, 1:] - x[:, :-1], [[0, 0], [0, 1], [0, 0], [0, 0]]),\n",
        "            lambda: tf.zeros_like(x),\n",
        "        )\n",
        "\n",
        "        dx2 = tf.cond(\n",
        "            tf.shape(x)[1] > 2,\n",
        "            lambda: tf.pad(x[:, 2:] - x[:, :-2], [[0, 0], [0, 2], [0, 0], [0, 0]]),\n",
        "            lambda: tf.zeros_like(x),\n",
        "        )\n",
        "        length = tf.shape(x)[1]\n",
        "\n",
        "        x = tf.concat(\n",
        "            [\n",
        "                tf.reshape(x, (-1, length, 2 * Constants.NUM_NODES)),  # x1,y1,x2,y2,...\n",
        "                tf.reshape(dx, (-1, length, 2 * Constants.NUM_NODES)),\n",
        "                tf.reshape(dx2, (-1, length, 2 * Constants.NUM_NODES)),\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "\n",
        "        # x1,y1,x2,y2,...dx1,dy1,dx2,dy2,...\n",
        "        x = tf.where(tf.math.is_nan(x), tf.constant(0.0, x.dtype), x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def pad_if_short(x, max_len):\n",
        "    # shape (T,F)\n",
        "    pad_len = max_len - tf.shape(x)[0]\n",
        "    padding = tf.ones((pad_len, tf.shape(x)[1]), dtype=x.dtype) * Constants.INPUT_PAD\n",
        "    x = tf.concat([x, padding], axis=0)\n",
        "    return x\n",
        "\n",
        "\n",
        "def shrink_if_long(x, max_len):\n",
        "    # shape is [T,F]\n",
        "    if tf.shape(x)[0] > max_len:\n",
        "        # we need to extend the dimension to [T,F,channels]  for tf.image.resize\n",
        "        x = tf.image.resize(x[..., None], (max_len, tf.shape(x)[1]))\n",
        "        x = tf.squeeze(x, axis=2)\n",
        "\n",
        "    return x\n",
        "\n",
        "def preprocess(x, max_len, do_pad=True):\n",
        "    # shape (T,F)\n",
        "    x = shrink_if_long(x, max_len=max_len)\n",
        "    # Preprocess expects a batch, so we extend the dimension to (None,T,F), then reduce the output back to (T,F).\n",
        "    x = tf.cast(Preprocess(max_len=max_len)(x[None, ...])[0], tf.float32)\n",
        "\n",
        "    if do_pad:  # we can avoid this step if there is batch padding\n",
        "        x = pad_if_short(x, max_len=max_len)\n",
        "        #x=tf.ensure_shape(x,(max_len,Constants.CHANNELS))\n",
        "    else:\n",
        "        #x=tf.ensure_shape(x,(None,Constants.CHANNELS))\n",
        "        pad\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FdtpVVSbYi17"
      },
      "outputs": [],
      "source": [
        "\n",
        "def decode_tfrec(record_bytes):\n",
        "    features = tf.io.parse_single_example(\n",
        "        record_bytes,\n",
        "        {\n",
        "            \"coordinates\": tf.io.VarLenFeature(tf.float32),\n",
        "            \"label\": tf.io.VarLenFeature(tf.int64),\n",
        "        },\n",
        "    )\n",
        "    coords = tf.sparse.to_dense(features[\"coordinates\"])\n",
        "    coords = tf.reshape(coords, (-1, Constants.NUM_INPUT_FEATURES))\n",
        "    label = tf.sparse.to_dense(features[\"label\"])\n",
        "\n",
        "    #coords=tf.ensure_shape(coords,(None,Constants.NUM_INPUT_FEATURES))\n",
        "    #label=tf.ensure_shape(label,(None,))\n",
        "\n",
        "\n",
        "    return (coords, label)\n",
        "\n",
        "def ensure_shapes(x,y,batch_size,max_len):\n",
        "  x=tf.ensure_shape(x,(batch_size,max_len,Constants.CHANNELS))\n",
        "  y=tf.ensure_shape(y,(batch_size,Constants.MAX_STRING_LEN))\n",
        "  return x,y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ImAgehPoYi18"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_dataset(\n",
        "    filenames,\n",
        "    input_path,\n",
        "    max_len,\n",
        "    batch_size=64,\n",
        "    drop_remainder=False,\n",
        "    augment=False,\n",
        "    shuffle_buffer=None,\n",
        "    repeat=False,\n",
        "    use_tfrecords=True,\n",
        "):\n",
        "    ignore_order = tf.data.Options()\n",
        "    ignore_order.experimental_deterministic = False\n",
        "\n",
        "\n",
        "    ds = tf.data.TFRecordDataset(\n",
        "        filenames, num_parallel_reads=tf.data.AUTOTUNE, compression_type=\"GZIP\"\n",
        "    )\n",
        "    ds.with_options(ignore_order)\n",
        "    ds = ds.map(decode_tfrec, tf.data.AUTOTUNE)\n",
        "\n",
        "    if augment:\n",
        "        ds = ds.map(lambda x, y: (augment_fn(x), y), tf.data.AUTOTUNE)\n",
        "\n",
        "    ds = ds.map(lambda x, y: (preprocess(x, max_len=max_len, do_pad=False), y), tf.data.AUTOTUNE)\n",
        "    #if repeat:\n",
        "    #    ds = ds.repeat()\n",
        "\n",
        "    if shuffle_buffer is not None:\n",
        "        ds = ds.shuffle(shuffle_buffer)\n",
        "\n",
        "    ds = ds.padded_batch(\n",
        "        batch_size,\n",
        "        padding_values=(\n",
        "            tf.constant(Constants.INPUT_PAD, dtype=tf.float32),\n",
        "            tf.constant(Constants.LABEL_PAD, dtype=tf.int64),\n",
        "        ),\n",
        "        padded_shapes=([max_len, Constants.CHANNELS], [Constants.MAX_STRING_LEN]),\n",
        "        drop_remainder=drop_remainder,\n",
        "    )\n",
        "\n",
        "    #tf.data.experimental.assert_cardinality(len(labels) // BATCH_SIZE)\n",
        "    ds.map(lambda x,y: ensure_shapes(x,y,batch_size,max_len),tf.data.AUTOTUNE)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4HDYkwwgYi18"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_run(train_files, valid_files, config, num_train, num_valid,experiment_id=0, use_tfrecords=True,summary=False):\n",
        "    #gc.collect()\n",
        "    #tf.keras.backend.clear_session()\n",
        "\n",
        "\n",
        "    if config.fp16:\n",
        "        if config.is_tpu:\n",
        "            policy = \"mixed_bfloat16\"\n",
        "        else:\n",
        "            policy = \"mixed_float16\"\n",
        "    else:\n",
        "        policy = \"float32\"\n",
        "\n",
        "\n",
        "    tf.keras.mixed_precision.set_global_policy(policy)\n",
        "    print(f\"\\n... TWO IMPORTANT ASPECTS OF THE GLOBAL MIXED PRECISION POLICY:\")\n",
        "    print(f'\\t--> COMPUTE DTYPE  : {tf.keras.mixed_precision.global_policy().compute_dtype}')\n",
        "    print(f'\\t--> VARIABLE DTYPE : {tf.keras.mixed_precision.global_policy().variable_dtype}')\n",
        "    augment_train= True\n",
        "    repeat_train = True\n",
        "    if config.is_tpu:\n",
        "      shuffle_buffer = 16384 #4096\n",
        "    else:\n",
        "      shuffle_buffer=4096\n",
        "    print(\"shuffle_buffer\",shuffle_buffer)\n",
        "    train_ds = get_dataset(\n",
        "        train_files,\n",
        "        input_path=config.input_path,\n",
        "        max_len=config.max_len,\n",
        "        batch_size=config.batch_size,\n",
        "        drop_remainder=True,\n",
        "        augment=augment_train,\n",
        "        repeat=repeat_train,\n",
        "        shuffle_buffer=shuffle_buffer,\n",
        "        use_tfrecords=True,\n",
        "    )\n",
        "    if valid_files is not None:\n",
        "        valid_ds = get_dataset(\n",
        "            valid_files,\n",
        "            input_path=config.input_path,\n",
        "            max_len=config.max_len,\n",
        "            batch_size=config.batch_size,\n",
        "            use_tfrecords=True,\n",
        "            drop_remainder=True\n",
        "        )\n",
        "    else:\n",
        "        valid_ds = None\n",
        "        valid_files = []\n",
        "\n",
        "    # num_train = count_data_items(train_ds)\n",
        "    # num_valid = count_data_items(valid_ds)\n",
        "    # print(num_train, num_valid, config.batch_size)\n",
        "    # exit()\n",
        "\n",
        "    steps_per_epoch = num_train // config.batch_size\n",
        "    dropout_step = config.dropout_start_epoch * steps_per_epoch\n",
        "    strategy = config.strategy\n",
        "    with strategy.scope():\n",
        "        model = get_model(\n",
        "            max_len=config.max_len,\n",
        "            output_dim=config.output_dim,\n",
        "            input_pad=Constants.INPUT_PAD,\n",
        "            dim=config.dim,\n",
        "            dropout_step=dropout_step,\n",
        "            drop_rate=0.2\n",
        "        )\n",
        "\n",
        "        base_lr = config.lr\n",
        "        lr_schedule = CosineDecay(\n",
        "            initial_learning_rate=base_lr / 10,\n",
        "            decay_steps=int(0.95 * steps_per_epoch * config.epochs),\n",
        "            alpha=0.005,\n",
        "            name=None,\n",
        "            warmup_target=base_lr,\n",
        "            warmup_steps=int(0.05 * steps_per_epoch * config.epochs),\n",
        "        )\n",
        "\n",
        "        #opt = tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=config.weight_decay)\n",
        "        radam=tfa.optimizers.RectifiedAdam(learning_rate=lr_schedule,weight_decay=config.weight_decay)\n",
        "        ranger = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)\n",
        "        opt=ranger\n",
        "        awp_step = config.awp_start_epoch * steps_per_epoch\n",
        "        if config.awp:\n",
        "            model = AWP(model.input, model.output, delta=config.awp_lambda, eps=0., start_step=awp_step)\n",
        "            print(\"Using AWP\")\n",
        "\n",
        "        ctc_loss1 = CTCLoss(pad_token_idx=Constants.LABEL_PAD,batch_size=config.batch_size,\n",
        "                           max_string_len=Constants.MAX_STRING_LEN,\n",
        "                           output_dim=config.output_dim,\n",
        "                           output_steps=config.max_len//2,replicas=config.replicas)\n",
        "        ctc_loss2 = CTCLoss(pad_token_idx=Constants.LABEL_PAD,batch_size=config.batch_size,\n",
        "                           max_string_len=Constants.MAX_STRING_LEN,\n",
        "                           output_dim=config.output_dim,\n",
        "                           output_steps=config.max_len//2,replicas=config.replicas)\n",
        "\n",
        "        if not config.is_tpu:\n",
        "          metrics=metrics= [LevDistanceMetric(),]\n",
        "        else:\n",
        "          metrics=None\n",
        "        model.compile(\n",
        "          optimizer=opt,\n",
        "          loss=[ctc_loss1,ctc_loss2],\n",
        "          loss_weights=[0.5,0.5],\n",
        "          metrics= metrics,\n",
        "          #steps_per_execution=16\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    if summary:\n",
        "        print()\n",
        "        model.summary()\n",
        "        print()\n",
        "        print(train_ds, valid_ds)\n",
        "        print()\n",
        "    print(f\"---------experiment {experiment_id}---------\")\n",
        "    print(f\"train:{num_train} \")\n",
        "    print()\n",
        "\n",
        "    if config.resume:\n",
        "        print(f\"resume from epoch{config.resume}\")\n",
        "        model.load_weights(f\"{config.output_path}/{config.comment}-exp{experiment_id}-last.h5\")\n",
        "        if train_ds is not None:\n",
        "            model.evaluate(train_ds.take(steps_per_epoch))\n",
        "        if valid_ds is not None:\n",
        "            model.evaluate(valid_ds)\n",
        "\n",
        "    tb_logger = tf.keras.callbacks.TensorBoard(\n",
        "        log_dir=config.output_path,\n",
        "    )\n",
        "    sv_loss = tf.keras.callbacks.ModelCheckpoint(\n",
        "        f\"{config.output_path}/{config.comment}-exp{experiment_id}-best.h5\",\n",
        "        monitor=\"val_final_loss\",\n",
        "        verbose=1,\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        mode=\"min\",\n",
        "        save_freq=\"epoch\",\n",
        "    )\n",
        "\n",
        "    # Callback function to check transcription on the val set.\n",
        "    # validation_callback = CallbackEval(model, valid_ds)\n",
        "    memory_usage = MemoryUsageCallbackExtended()\n",
        "    swa = SWA(\n",
        "        f\"{config.output_path}/{config.comment}-exp{experiment_id}\",\n",
        "        config.swa_epochs,\n",
        "        strategy=strategy,\n",
        "        train_ds=train_ds,\n",
        "        valid_ds=valid_ds,\n",
        "    )\n",
        "    callbacks = []\n",
        "    if config.save_output:\n",
        "        #callbacks.append(tb_logger)\n",
        "        callbacks.append(swa)\n",
        "        callbacks.append(sv_loss)\n",
        "    #callbacks.append(memory_usage)\n",
        "        callbacks.append(tf.keras.callbacks.TerminateOnNaN())\n",
        "    # callbacks.append(validation_callback)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds,\n",
        "        epochs=config.epochs - config.resume,\n",
        "        #steps_per_epoch=steps_per_epoch,\n",
        "        #validation_steps=num_valid // config.batch_size,\n",
        "        callbacks=callbacks,\n",
        "        validation_data=valid_ds,\n",
        "        verbose=config.verbose,\n",
        "    )\n",
        "\n",
        "    if config.save_output:  # reload the saved best weights checkpoint\n",
        "        saved_based_model = f\"{config.output_path}/{config.comment}-exp{experiment_id}-best.h5\"\n",
        "        if os.path.exists(saved_based_model):\n",
        "            model.load_weights(saved_based_model)\n",
        "        else:\n",
        "            print(f\"Warning: could not find {saved_based_model}\")\n",
        "    if valid_ds is not None:\n",
        "        cv = model.evaluate(valid_ds, verbose=config.verbose)\n",
        "    else:\n",
        "        cv = None\n",
        "    return model, cv, history\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YpxT9BTFYi18"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(config, experiment_id=0, use_supplemental=True):\n",
        "    #tf.keras.backend.clear_session()\n",
        "    if config.strategy is None:\n",
        "      update_config_with_strategy(config)\n",
        "    print(f\"using {config.replicas} replicas\")\n",
        "    print(f\"batch size {config.batch_size}\")\n",
        "    print(f\"learning rate {config.lr}\")\n",
        "    print(f\"fp16={config.fp16}\")\n",
        "    seed_everything(config.seed)\n",
        "\n",
        "\n",
        "    all_filenames = sorted(tf.io.gfile.glob(config.input_path+\"/asl-preprocessing/records/*.tfrecord\"))\n",
        "    regular = [x for x in all_filenames if \"supp\" not in x]\n",
        "    supp = [x for x in all_filenames if \"supp\" in x]\n",
        "\n",
        "    data_filenames = regular\n",
        "    if use_supplemental:\n",
        "        data_filenames += supp\n",
        "    print(\"Using TFRECORDS\")\n",
        "\n",
        "\n",
        "    valid_files = data_filenames[: config.num_eval]  # first part in list\n",
        "    train_files = data_filenames[config.num_eval :]\n",
        "    random.shuffle(train_files)\n",
        "\n",
        "\n",
        "    df1 = pd.read_csv(config.input_path + \"/asl-fingerspelling/train.csv\")\n",
        "    df2 = pd.read_csv(config.input_path + \"/asl-fingerspelling/supplemental_metadata.csv\")\n",
        "    df_info = pd.concat([df1, df2])\n",
        "\n",
        "    #ds = get_dataset(train_files, CFG.input_path,max_len=CFG.max_len, augment=False, batch_size=64)\n",
        "    #print(ds)\n",
        "    #for x,y in ds:\n",
        "    #    print(x,y)\n",
        "    #raise\n",
        "\n",
        "    if use_supplemental:\n",
        "        num_train = 3567 * 32  # with supplemental\n",
        "    else:\n",
        "        num_train = 1912 * 32  # without supplemental\n",
        "\n",
        "    num_valid=config.num_eval*1000\n",
        "\n",
        "    train_run(\n",
        "        train_files,\n",
        "        valid_files,\n",
        "        config,\n",
        "        num_train,\n",
        "        num_valid,\n",
        "        summary=False,\n",
        "        experiment_id=experiment_id,\n",
        "        use_tfrecords=True,\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lfC8z_wLYi19"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVIhC2jeYi19"
      },
      "source": [
        "# Train It!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5UdMh9V7Yi19",
        "outputId": "d51989b7-b5b4-45e1-c2c9-1541942fd4c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Strategy <tensorflow.python.distribute.tpu_strategy.TPUStrategyV2 object at 0x7efc2be05f90>\n",
            "using 8 replicas\n",
            "batch size 1024\n",
            "learning rate 0.0024\n",
            "fp16=False\n",
            "Using TFRECORDS\n",
            "\n",
            "... TWO IMPORTANT ASPECTS OF THE GLOBAL MIXED PRECISION POLICY:\n",
            "\t--> COMPUTE DTYPE  : float32\n",
            "\t--> VARIABLE DTYPE : float32\n",
            "shuffle_buffer 16384\n",
            "Using AWP\n",
            "---------experiment 0---------\n",
            "train:114144 \n",
            "\n",
            "Epoch 1/300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/ctc_ops.py:1512: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/ctc_ops.py:1495: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    111/Unknown - 350s 294ms/step - loss: 208.1652 - final_loss: 210.0327 - internal_loss: 206.2978\n",
            "Epoch 1: val_final_loss improved from inf to 305.97214, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 371s 482ms/step - loss: 208.1652 - final_loss: 210.0327 - internal_loss: 206.2978 - val_loss: 280.5361 - val_final_loss: 305.9721 - val_internal_loss: 255.1002\n",
            "Epoch 2/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 119.2374 - final_loss: 98.9104 - internal_loss: 139.5643\n",
            "Epoch 2: val_final_loss improved from 305.97214 to 103.70723, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 44s 340ms/step - loss: 119.2374 - final_loss: 98.9104 - internal_loss: 139.5643 - val_loss: 103.7690 - val_final_loss: 103.7072 - val_internal_loss: 103.8307\n",
            "Epoch 3/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 102.2137 - final_loss: 80.0482 - internal_loss: 124.3792\n",
            "Epoch 3: val_final_loss improved from 103.70723 to 72.69484, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 44s 340ms/step - loss: 102.2137 - final_loss: 80.0482 - internal_loss: 124.3792 - val_loss: 77.2298 - val_final_loss: 72.6948 - val_internal_loss: 81.7648\n",
            "Epoch 4/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 93.9342 - final_loss: 78.6945 - internal_loss: 109.1740\n",
            "Epoch 4: val_final_loss improved from 72.69484 to 69.64259, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 44s 342ms/step - loss: 93.9342 - final_loss: 78.6945 - internal_loss: 109.1740 - val_loss: 75.0998 - val_final_loss: 69.6426 - val_internal_loss: 80.5570\n",
            "Epoch 5/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 88.4614 - final_loss: 76.9024 - internal_loss: 100.0203\n",
            "Epoch 5: val_final_loss improved from 69.64259 to 66.50000, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 47s 362ms/step - loss: 88.4614 - final_loss: 76.9024 - internal_loss: 100.0203 - val_loss: 70.5654 - val_final_loss: 66.5000 - val_internal_loss: 74.6307\n",
            "Epoch 6/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 84.5141 - final_loss: 75.2224 - internal_loss: 93.8058\n",
            "Epoch 6: val_final_loss improved from 66.50000 to 65.92251, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 44s 340ms/step - loss: 84.5141 - final_loss: 75.2224 - internal_loss: 93.8058 - val_loss: 75.9723 - val_final_loss: 65.9225 - val_internal_loss: 86.0222\n",
            "Epoch 7/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 82.2541 - final_loss: 74.2611 - internal_loss: 90.2471\n",
            "Epoch 7: val_final_loss improved from 65.92251 to 62.16613, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 44s 339ms/step - loss: 82.2541 - final_loss: 74.2611 - internal_loss: 90.2471 - val_loss: 71.4559 - val_final_loss: 62.1661 - val_internal_loss: 80.7458\n",
            "Epoch 8/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 80.5030 - final_loss: 73.2363 - internal_loss: 87.7698\n",
            "Epoch 8: val_final_loss improved from 62.16613 to 59.43755, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 44s 343ms/step - loss: 80.5030 - final_loss: 73.2363 - internal_loss: 87.7698 - val_loss: 68.1145 - val_final_loss: 59.4376 - val_internal_loss: 76.7915\n",
            "Epoch 9/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 92.7908 - final_loss: 74.5698 - internal_loss: 111.0117\n",
            "Epoch 9: val_final_loss did not improve from 59.43755\n",
            "111/111 [==============================] - 50s 360ms/step - loss: 92.7908 - final_loss: 74.5698 - internal_loss: 111.0117 - val_loss: 86.5835 - val_final_loss: 62.4453 - val_internal_loss: 110.7218\n",
            "Epoch 10/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 78.0769 - final_loss: 72.7222 - internal_loss: 83.4316\n",
            "Epoch 10: val_final_loss did not improve from 59.43755\n",
            "111/111 [==============================] - 45s 348ms/step - loss: 78.0769 - final_loss: 72.7222 - internal_loss: 83.4316 - val_loss: 76.5175 - val_final_loss: 60.6138 - val_internal_loss: 92.4212\n",
            "Epoch 11/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 73.6494 - final_loss: 71.2879 - internal_loss: 76.0108\n",
            "Epoch 11: val_final_loss improved from 59.43755 to 59.40684, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 44s 341ms/step - loss: 73.6493 - final_loss: 71.2879 - internal_loss: 76.0108 - val_loss: 66.2093 - val_final_loss: 59.4068 - val_internal_loss: 73.0118\n",
            "Epoch 12/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 70.4505 - final_loss: 69.5807 - internal_loss: 71.3202\n",
            "Epoch 12: val_final_loss did not improve from 59.40684\n",
            "111/111 [==============================] - 42s 324ms/step - loss: 70.4505 - final_loss: 69.5807 - internal_loss: 71.3202 - val_loss: 61.6492 - val_final_loss: 60.7094 - val_internal_loss: 62.5890\n",
            "Epoch 13/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 66.9084 - final_loss: 67.6897 - internal_loss: 66.1272\n",
            "Epoch 13: val_final_loss improved from 59.40684 to 56.29704, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 47s 370ms/step - loss: 66.9084 - final_loss: 67.6897 - internal_loss: 66.1271 - val_loss: 55.8224 - val_final_loss: 56.2970 - val_internal_loss: 55.3477\n",
            "Epoch 14/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 59.5962 - final_loss: 61.8561 - internal_loss: 57.3364\n",
            "Epoch 14: val_final_loss improved from 56.29704 to 51.24184, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 44s 339ms/step - loss: 59.5962 - final_loss: 61.8561 - internal_loss: 57.3364 - val_loss: 51.7692 - val_final_loss: 51.2418 - val_internal_loss: 52.2965\n",
            "Epoch 15/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 49.7137 - final_loss: 49.9464 - internal_loss: 49.4809\n",
            "Epoch 15: val_final_loss improved from 51.24184 to 38.69213, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 44s 341ms/step - loss: 49.7137 - final_loss: 49.9464 - internal_loss: 49.4809 - val_loss: 39.1145 - val_final_loss: 38.6921 - val_internal_loss: 39.5368\n",
            "Epoch 16/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 79.2148 - final_loss: 84.3997 - internal_loss: 74.0299\n",
            "Epoch 16: val_final_loss did not improve from 38.69213\n",
            "111/111 [==============================] - 70s 568ms/step - loss: 79.2148 - final_loss: 84.3997 - internal_loss: 74.0299 - val_loss: 56.3729 - val_final_loss: 64.5844 - val_internal_loss: 48.1614\n",
            "Epoch 17/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 65.9251 - final_loss: 76.8226 - internal_loss: 55.0276\n",
            "Epoch 17: val_final_loss did not improve from 38.69213\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 65.9251 - final_loss: 76.8226 - internal_loss: 55.0276 - val_loss: 49.3606 - val_final_loss: 60.5464 - val_internal_loss: 38.1748\n",
            "Epoch 18/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 60.7861 - final_loss: 70.6331 - internal_loss: 50.9391\n",
            "Epoch 18: val_final_loss did not improve from 38.69213\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 60.7861 - final_loss: 70.6331 - internal_loss: 50.9391 - val_loss: 41.1550 - val_final_loss: 46.7995 - val_internal_loss: 35.5105\n",
            "Epoch 19/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 51.1505 - final_loss: 53.9141 - internal_loss: 48.3869\n",
            "Epoch 19: val_final_loss improved from 38.69213 to 34.69791, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 72s 590ms/step - loss: 51.1505 - final_loss: 53.9141 - internal_loss: 48.3869 - val_loss: 33.8859 - val_final_loss: 34.6979 - val_internal_loss: 33.0739\n",
            "Epoch 20/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 47.5135 - final_loss: 48.7634 - internal_loss: 46.2635\n",
            "Epoch 20: val_final_loss improved from 34.69791 to 31.64738, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 604ms/step - loss: 47.5135 - final_loss: 48.7634 - internal_loss: 46.2635 - val_loss: 31.5077 - val_final_loss: 31.6474 - val_internal_loss: 31.3679\n",
            "Epoch 21/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 45.2943 - final_loss: 45.9812 - internal_loss: 44.6076\n",
            "Epoch 21: val_final_loss improved from 31.64738 to 30.74742, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 585ms/step - loss: 45.2943 - final_loss: 45.9812 - internal_loss: 44.6075 - val_loss: 30.8318 - val_final_loss: 30.7474 - val_internal_loss: 30.9161\n",
            "Epoch 22/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 43.7601 - final_loss: 44.1849 - internal_loss: 43.3352\n",
            "Epoch 22: val_final_loss improved from 30.74742 to 29.21316, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 73s 583ms/step - loss: 43.7601 - final_loss: 44.1849 - internal_loss: 43.3352 - val_loss: 29.3493 - val_final_loss: 29.2132 - val_internal_loss: 29.4854\n",
            "Epoch 23/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 42.3633 - final_loss: 42.5483 - internal_loss: 42.1785\n",
            "Epoch 23: val_final_loss improved from 29.21316 to 28.59372, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 582ms/step - loss: 42.3633 - final_loss: 42.5483 - internal_loss: 42.1785 - val_loss: 28.8261 - val_final_loss: 28.5937 - val_internal_loss: 29.0585\n",
            "Epoch 24/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 41.2070 - final_loss: 41.2374 - internal_loss: 41.1765\n",
            "Epoch 24: val_final_loss improved from 28.59372 to 27.37506, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 584ms/step - loss: 41.2070 - final_loss: 41.2374 - internal_loss: 41.1765 - val_loss: 27.7329 - val_final_loss: 27.3751 - val_internal_loss: 28.0907\n",
            "Epoch 25/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 40.2248 - final_loss: 40.1241 - internal_loss: 40.3255\n",
            "Epoch 25: val_final_loss improved from 27.37506 to 27.15214, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 580ms/step - loss: 40.2248 - final_loss: 40.1241 - internal_loss: 40.3255 - val_loss: 27.6483 - val_final_loss: 27.1521 - val_internal_loss: 28.1444\n",
            "Epoch 26/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 39.4734 - final_loss: 39.2886 - internal_loss: 39.6582\n",
            "Epoch 26: val_final_loss improved from 27.15214 to 26.12724, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 583ms/step - loss: 39.4734 - final_loss: 39.2886 - internal_loss: 39.6582 - val_loss: 26.6837 - val_final_loss: 26.1272 - val_internal_loss: 27.2402\n",
            "Epoch 27/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 38.7413 - final_loss: 38.4925 - internal_loss: 38.9900\n",
            "Epoch 27: val_final_loss improved from 26.12724 to 26.11981, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 582ms/step - loss: 38.7412 - final_loss: 38.4925 - internal_loss: 38.9900 - val_loss: 26.6921 - val_final_loss: 26.1198 - val_internal_loss: 27.2645\n",
            "Epoch 28/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 38.0016 - final_loss: 37.6428 - internal_loss: 38.3604\n",
            "Epoch 28: val_final_loss improved from 26.11981 to 24.88452, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 583ms/step - loss: 38.0016 - final_loss: 37.6428 - internal_loss: 38.3604 - val_loss: 25.4562 - val_final_loss: 24.8845 - val_internal_loss: 26.0279\n",
            "Epoch 29/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 37.3323 - final_loss: 36.9377 - internal_loss: 37.7269\n",
            "Epoch 29: val_final_loss did not improve from 24.88452\n",
            "111/111 [==============================] - 69s 568ms/step - loss: 37.3323 - final_loss: 36.9377 - internal_loss: 37.7269 - val_loss: 25.9254 - val_final_loss: 25.2511 - val_internal_loss: 26.5996\n",
            "Epoch 30/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 36.7650 - final_loss: 36.3052 - internal_loss: 37.2247\n",
            "Epoch 30: val_final_loss improved from 24.88452 to 24.15335, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 586ms/step - loss: 36.7650 - final_loss: 36.3052 - internal_loss: 37.2247 - val_loss: 24.7955 - val_final_loss: 24.1533 - val_internal_loss: 25.4376\n",
            "Epoch 31/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 36.2335 - final_loss: 35.7998 - internal_loss: 36.6671\n",
            "Epoch 31: val_final_loss did not improve from 24.15335\n",
            "111/111 [==============================] - 73s 569ms/step - loss: 36.2335 - final_loss: 35.7998 - internal_loss: 36.6671 - val_loss: 25.3353 - val_final_loss: 24.4221 - val_internal_loss: 26.2484\n",
            "Epoch 32/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 35.6968 - final_loss: 35.1274 - internal_loss: 36.2662\n",
            "Epoch 32: val_final_loss improved from 24.15335 to 23.25403, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 584ms/step - loss: 35.6968 - final_loss: 35.1274 - internal_loss: 36.2662 - val_loss: 23.9787 - val_final_loss: 23.2540 - val_internal_loss: 24.7033\n",
            "Epoch 33/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 35.1471 - final_loss: 34.5627 - internal_loss: 35.7314\n",
            "Epoch 33: val_final_loss did not improve from 23.25403\n",
            "111/111 [==============================] - 69s 568ms/step - loss: 35.1471 - final_loss: 34.5627 - internal_loss: 35.7314 - val_loss: 24.4334 - val_final_loss: 23.5506 - val_internal_loss: 25.3162\n",
            "Epoch 34/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 34.7453 - final_loss: 34.1213 - internal_loss: 35.3694\n",
            "Epoch 34: val_final_loss improved from 23.25403 to 22.46301, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 74s 585ms/step - loss: 34.7453 - final_loss: 34.1213 - internal_loss: 35.3694 - val_loss: 23.2445 - val_final_loss: 22.4630 - val_internal_loss: 24.0261\n",
            "Epoch 35/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 34.3078 - final_loss: 33.7095 - internal_loss: 34.9062\n",
            "Epoch 35: val_final_loss did not improve from 22.46301\n",
            "111/111 [==============================] - 69s 568ms/step - loss: 34.3078 - final_loss: 33.7095 - internal_loss: 34.9062 - val_loss: 23.7141 - val_final_loss: 22.8634 - val_internal_loss: 24.5648\n",
            "Epoch 36/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 33.9029 - final_loss: 33.2116 - internal_loss: 34.5941\n",
            "Epoch 36: val_final_loss improved from 22.46301 to 22.01139, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 580ms/step - loss: 33.9029 - final_loss: 33.2116 - internal_loss: 34.5941 - val_loss: 22.8633 - val_final_loss: 22.0114 - val_internal_loss: 23.7152\n",
            "Epoch 37/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 33.5009 - final_loss: 32.8117 - internal_loss: 34.1902\n",
            "Epoch 37: val_final_loss did not improve from 22.01139\n",
            "111/111 [==============================] - 68s 567ms/step - loss: 33.5009 - final_loss: 32.8117 - internal_loss: 34.1902 - val_loss: 23.2838 - val_final_loss: 22.4493 - val_internal_loss: 24.1183\n",
            "Epoch 38/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 33.0754 - final_loss: 32.3158 - internal_loss: 33.8350\n",
            "Epoch 38: val_final_loss improved from 22.01139 to 21.64667, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 581ms/step - loss: 33.0754 - final_loss: 32.3158 - internal_loss: 33.8350 - val_loss: 22.4135 - val_final_loss: 21.6467 - val_internal_loss: 23.1803\n",
            "Epoch 39/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 32.6880 - final_loss: 31.9354 - internal_loss: 33.4406\n",
            "Epoch 39: val_final_loss improved from 21.64667 to 21.61129, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 587ms/step - loss: 32.6880 - final_loss: 31.9354 - internal_loss: 33.4406 - val_loss: 22.4636 - val_final_loss: 21.6113 - val_internal_loss: 23.3159\n",
            "Epoch 40/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 32.3576 - final_loss: 31.5697 - internal_loss: 33.1454\n",
            "Epoch 40: val_final_loss improved from 21.61129 to 20.97647, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 75s 587ms/step - loss: 32.3576 - final_loss: 31.5697 - internal_loss: 33.1454 - val_loss: 21.8073 - val_final_loss: 20.9765 - val_internal_loss: 22.6381\n",
            "Epoch 41/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 31.9526 - final_loss: 31.1543 - internal_loss: 32.7509\n",
            "Epoch 41: val_final_loss did not improve from 20.97647\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 31.9526 - final_loss: 31.1543 - internal_loss: 32.7509 - val_loss: 22.2256 - val_final_loss: 21.3881 - val_internal_loss: 23.0631\n",
            "Epoch 42/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 31.7116 - final_loss: 30.8755 - internal_loss: 32.5478\n",
            "Epoch 42: val_final_loss improved from 20.97647 to 20.71019, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 74s 607ms/step - loss: 31.7116 - final_loss: 30.8755 - internal_loss: 32.5478 - val_loss: 21.5370 - val_final_loss: 20.7102 - val_internal_loss: 22.3637\n",
            "Epoch 43/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 31.3284 - final_loss: 30.4805 - internal_loss: 32.1763\n",
            "Epoch 43: val_final_loss did not improve from 20.71019\n",
            "111/111 [==============================] - 70s 571ms/step - loss: 31.3284 - final_loss: 30.4805 - internal_loss: 32.1763 - val_loss: 21.8588 - val_final_loss: 20.9555 - val_internal_loss: 22.7621\n",
            "Epoch 44/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 31.0779 - final_loss: 30.1953 - internal_loss: 31.9605\n",
            "Epoch 44: val_final_loss improved from 20.71019 to 20.26687, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 588ms/step - loss: 31.0779 - final_loss: 30.1953 - internal_loss: 31.9605 - val_loss: 21.1270 - val_final_loss: 20.2669 - val_internal_loss: 21.9871\n",
            "Epoch 45/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 30.7733 - final_loss: 29.9133 - internal_loss: 31.6334\n",
            "Epoch 45: val_final_loss did not improve from 20.26687\n",
            "111/111 [==============================] - 70s 570ms/step - loss: 30.7733 - final_loss: 29.9133 - internal_loss: 31.6334 - val_loss: 21.5836 - val_final_loss: 20.7051 - val_internal_loss: 22.4622\n",
            "Epoch 46/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 30.4944 - final_loss: 29.5231 - internal_loss: 31.4657\n",
            "Epoch 46: val_final_loss improved from 20.26687 to 19.76987, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 584ms/step - loss: 30.4944 - final_loss: 29.5231 - internal_loss: 31.4657 - val_loss: 20.6593 - val_final_loss: 19.7699 - val_internal_loss: 21.5487\n",
            "Epoch 47/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 30.1638 - final_loss: 29.2289 - internal_loss: 31.0986\n",
            "Epoch 47: val_final_loss did not improve from 19.76987\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 30.1638 - final_loss: 29.2289 - internal_loss: 31.0986 - val_loss: 21.1255 - val_final_loss: 20.1977 - val_internal_loss: 22.0534\n",
            "Epoch 48/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 29.8771 - final_loss: 28.9320 - internal_loss: 30.8222\n",
            "Epoch 48: val_final_loss improved from 19.76987 to 19.48456, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 72s 585ms/step - loss: 29.8771 - final_loss: 28.9320 - internal_loss: 30.8222 - val_loss: 20.3659 - val_final_loss: 19.4846 - val_internal_loss: 21.2473\n",
            "Epoch 49/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 29.6812 - final_loss: 28.6992 - internal_loss: 30.6631\n",
            "Epoch 49: val_final_loss did not improve from 19.48456\n",
            "111/111 [==============================] - 70s 571ms/step - loss: 29.6812 - final_loss: 28.6992 - internal_loss: 30.6631 - val_loss: 20.9915 - val_final_loss: 20.0036 - val_internal_loss: 21.9794\n",
            "Epoch 50/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 29.4126 - final_loss: 28.3941 - internal_loss: 30.4310\n",
            "Epoch 50: val_final_loss improved from 19.48456 to 19.15716, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 587ms/step - loss: 29.4126 - final_loss: 28.3941 - internal_loss: 30.4310 - val_loss: 20.0604 - val_final_loss: 19.1572 - val_internal_loss: 20.9636\n",
            "Epoch 51/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 29.1461 - final_loss: 28.1201 - internal_loss: 30.1721\n",
            "Epoch 51: val_final_loss did not improve from 19.15716\n",
            "111/111 [==============================] - 86s 678ms/step - loss: 29.1461 - final_loss: 28.1201 - internal_loss: 30.1721 - val_loss: 20.6461 - val_final_loss: 19.6863 - val_internal_loss: 21.6058\n",
            "Epoch 52/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 28.9528 - final_loss: 27.9027 - internal_loss: 30.0028\n",
            "Epoch 52: val_final_loss improved from 19.15716 to 18.79205, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 586ms/step - loss: 28.9528 - final_loss: 27.9027 - internal_loss: 30.0028 - val_loss: 19.7125 - val_final_loss: 18.7920 - val_internal_loss: 20.6329\n",
            "Epoch 53/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 28.6876 - final_loss: 27.6167 - internal_loss: 29.7585\n",
            "Epoch 53: val_final_loss did not improve from 18.79205\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 28.6876 - final_loss: 27.6167 - internal_loss: 29.7585 - val_loss: 20.5826 - val_final_loss: 19.6676 - val_internal_loss: 21.4977\n",
            "Epoch 54/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 28.5096 - final_loss: 27.4193 - internal_loss: 29.5998\n",
            "Epoch 54: val_final_loss improved from 18.79205 to 18.60254, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 76s 584ms/step - loss: 28.5096 - final_loss: 27.4193 - internal_loss: 29.5998 - val_loss: 19.4904 - val_final_loss: 18.6025 - val_internal_loss: 20.3783\n",
            "Epoch 55/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 28.2597 - final_loss: 27.1668 - internal_loss: 29.3527\n",
            "Epoch 55: val_final_loss did not improve from 18.60254\n",
            "111/111 [==============================] - 69s 566ms/step - loss: 28.2597 - final_loss: 27.1668 - internal_loss: 29.3527 - val_loss: 19.9521 - val_final_loss: 19.0294 - val_internal_loss: 20.8748\n",
            "Epoch 56/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 28.0839 - final_loss: 26.9435 - internal_loss: 29.2243\n",
            "Epoch 56: val_final_loss improved from 18.60254 to 18.36753, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 586ms/step - loss: 28.0839 - final_loss: 26.9435 - internal_loss: 29.2243 - val_loss: 19.2145 - val_final_loss: 18.3675 - val_internal_loss: 20.0614\n",
            "Epoch 57/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 27.8950 - final_loss: 26.7706 - internal_loss: 29.0193\n",
            "Epoch 57: val_final_loss did not improve from 18.36753\n",
            "111/111 [==============================] - 70s 571ms/step - loss: 27.8950 - final_loss: 26.7706 - internal_loss: 29.0193 - val_loss: 20.3544 - val_final_loss: 19.5421 - val_internal_loss: 21.1668\n",
            "Epoch 58/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 27.7324 - final_loss: 26.5772 - internal_loss: 28.8876\n",
            "Epoch 58: val_final_loss improved from 18.36753 to 18.36116, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 72s 587ms/step - loss: 27.7324 - final_loss: 26.5772 - internal_loss: 28.8876 - val_loss: 19.2348 - val_final_loss: 18.3612 - val_internal_loss: 20.1085\n",
            "Epoch 59/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 27.5159 - final_loss: 26.3445 - internal_loss: 28.6874\n",
            "Epoch 59: val_final_loss did not improve from 18.36116\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 27.5159 - final_loss: 26.3445 - internal_loss: 28.6874 - val_loss: 20.1499 - val_final_loss: 19.1945 - val_internal_loss: 21.1053\n",
            "Epoch 60/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 27.3237 - final_loss: 26.1445 - internal_loss: 28.5029\n",
            "Epoch 60: val_final_loss improved from 18.36116 to 18.15644, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 584ms/step - loss: 27.3237 - final_loss: 26.1445 - internal_loss: 28.5029 - val_loss: 19.0724 - val_final_loss: 18.1564 - val_internal_loss: 19.9884\n",
            "Epoch 61/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 27.1075 - final_loss: 25.8906 - internal_loss: 28.3244\n",
            "Epoch 61: val_final_loss did not improve from 18.15644\n",
            "111/111 [==============================] - 70s 570ms/step - loss: 27.1075 - final_loss: 25.8906 - internal_loss: 28.3244 - val_loss: 19.5232 - val_final_loss: 18.5218 - val_internal_loss: 20.5246\n",
            "Epoch 62/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 27.0211 - final_loss: 25.7760 - internal_loss: 28.2661\n",
            "Epoch 62: val_final_loss improved from 18.15644 to 17.72626, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 584ms/step - loss: 27.0211 - final_loss: 25.7760 - internal_loss: 28.2661 - val_loss: 18.6605 - val_final_loss: 17.7263 - val_internal_loss: 19.5946\n",
            "Epoch 63/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 26.7020 - final_loss: 25.4418 - internal_loss: 27.9623\n",
            "Epoch 63: val_final_loss did not improve from 17.72626\n",
            "111/111 [==============================] - 70s 568ms/step - loss: 26.7020 - final_loss: 25.4418 - internal_loss: 27.9623 - val_loss: 19.8277 - val_final_loss: 18.8433 - val_internal_loss: 20.8121\n",
            "Epoch 64/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 26.6433 - final_loss: 25.3699 - internal_loss: 27.9167\n",
            "Epoch 64: val_final_loss improved from 17.72626 to 17.66010, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 75s 621ms/step - loss: 26.6433 - final_loss: 25.3699 - internal_loss: 27.9167 - val_loss: 18.5382 - val_final_loss: 17.6601 - val_internal_loss: 19.4162\n",
            "Epoch 65/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 26.3061 - final_loss: 25.0075 - internal_loss: 27.6046\n",
            "Epoch 65: val_final_loss did not improve from 17.66010\n",
            "111/111 [==============================] - 77s 595ms/step - loss: 26.3061 - final_loss: 25.0075 - internal_loss: 27.6046 - val_loss: 19.3789 - val_final_loss: 18.4839 - val_internal_loss: 20.2739\n",
            "Epoch 66/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 26.1938 - final_loss: 24.8880 - internal_loss: 27.4995\n",
            "Epoch 66: val_final_loss improved from 17.66010 to 17.48241, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 73s 585ms/step - loss: 26.1938 - final_loss: 24.8880 - internal_loss: 27.4995 - val_loss: 18.3977 - val_final_loss: 17.4824 - val_internal_loss: 19.3131\n",
            "Epoch 67/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 26.1167 - final_loss: 24.7832 - internal_loss: 27.4503\n",
            "Epoch 67: val_final_loss did not improve from 17.48241\n",
            "111/111 [==============================] - 69s 567ms/step - loss: 26.1167 - final_loss: 24.7832 - internal_loss: 27.4503 - val_loss: 19.0038 - val_final_loss: 18.0888 - val_internal_loss: 19.9187\n",
            "Epoch 68/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 25.9063 - final_loss: 24.5205 - internal_loss: 27.2921\n",
            "Epoch 68: val_final_loss improved from 17.48241 to 17.33463, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 77s 584ms/step - loss: 25.9063 - final_loss: 24.5205 - internal_loss: 27.2921 - val_loss: 18.2132 - val_final_loss: 17.3346 - val_internal_loss: 19.0917\n",
            "Epoch 69/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 25.8474 - final_loss: 24.5082 - internal_loss: 27.1865\n",
            "Epoch 69: val_final_loss did not improve from 17.33463\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 25.8474 - final_loss: 24.5082 - internal_loss: 27.1865 - val_loss: 18.7483 - val_final_loss: 17.8192 - val_internal_loss: 19.6775\n",
            "Epoch 70/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 25.6860 - final_loss: 24.2814 - internal_loss: 27.0905\n",
            "Epoch 70: val_final_loss improved from 17.33463 to 17.27408, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 72s 589ms/step - loss: 25.6860 - final_loss: 24.2814 - internal_loss: 27.0905 - val_loss: 18.2239 - val_final_loss: 17.2741 - val_internal_loss: 19.1737\n",
            "Epoch 71/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 25.4992 - final_loss: 24.1066 - internal_loss: 26.8918\n",
            "Epoch 71: val_final_loss did not improve from 17.27408\n",
            "111/111 [==============================] - 70s 571ms/step - loss: 25.4992 - final_loss: 24.1066 - internal_loss: 26.8918 - val_loss: 19.3419 - val_final_loss: 18.3376 - val_internal_loss: 20.3463\n",
            "Epoch 72/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 25.4317 - final_loss: 24.0404 - internal_loss: 26.8230\n",
            "Epoch 72: val_final_loss did not improve from 17.27408\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 25.4317 - final_loss: 24.0404 - internal_loss: 26.8230 - val_loss: 18.1744 - val_final_loss: 17.2763 - val_internal_loss: 19.0725\n",
            "Epoch 73/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 25.2073 - final_loss: 23.7844 - internal_loss: 26.6302\n",
            "Epoch 73: val_final_loss did not improve from 17.27408\n",
            "111/111 [==============================] - 70s 570ms/step - loss: 25.2073 - final_loss: 23.7844 - internal_loss: 26.6302 - val_loss: 18.6135 - val_final_loss: 17.7219 - val_internal_loss: 19.5050\n",
            "Epoch 74/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 25.0876 - final_loss: 23.6444 - internal_loss: 26.5308\n",
            "Epoch 74: val_final_loss improved from 17.27408 to 17.06701, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 583ms/step - loss: 25.0876 - final_loss: 23.6444 - internal_loss: 26.5308 - val_loss: 17.9346 - val_final_loss: 17.0670 - val_internal_loss: 18.8022\n",
            "Epoch 75/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 25.0778 - final_loss: 23.6033 - internal_loss: 26.5522\n",
            "Epoch 75: val_final_loss did not improve from 17.06701\n",
            "111/111 [==============================] - 69s 567ms/step - loss: 25.0778 - final_loss: 23.6033 - internal_loss: 26.5522 - val_loss: 19.1698 - val_final_loss: 18.1865 - val_internal_loss: 20.1531\n",
            "Epoch 76/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 24.8432 - final_loss: 23.3815 - internal_loss: 26.3050\n",
            "Epoch 76: val_final_loss improved from 17.06701 to 16.95028, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 585ms/step - loss: 24.8432 - final_loss: 23.3815 - internal_loss: 26.3049 - val_loss: 17.7926 - val_final_loss: 16.9503 - val_internal_loss: 18.6350\n",
            "Epoch 77/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 24.6323 - final_loss: 23.1322 - internal_loss: 26.1324\n",
            "Epoch 77: val_final_loss did not improve from 16.95028\n",
            "111/111 [==============================] - 70s 569ms/step - loss: 24.6323 - final_loss: 23.1322 - internal_loss: 26.1324 - val_loss: 18.8650 - val_final_loss: 17.8147 - val_internal_loss: 19.9152\n",
            "Epoch 78/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 24.5874 - final_loss: 23.0780 - internal_loss: 26.0967\n",
            "Epoch 78: val_final_loss improved from 16.95028 to 16.87037, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 585ms/step - loss: 24.5874 - final_loss: 23.0780 - internal_loss: 26.0967 - val_loss: 17.7257 - val_final_loss: 16.8704 - val_internal_loss: 18.5810\n",
            "Epoch 79/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 24.4080 - final_loss: 22.8963 - internal_loss: 25.9197\n",
            "Epoch 79: val_final_loss did not improve from 16.87037\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 24.4080 - final_loss: 22.8963 - internal_loss: 25.9197 - val_loss: 18.4406 - val_final_loss: 17.5457 - val_internal_loss: 19.3355\n",
            "Epoch 80/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 24.4035 - final_loss: 22.8573 - internal_loss: 25.9497\n",
            "Epoch 80: val_final_loss improved from 16.87037 to 16.75625, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 72s 587ms/step - loss: 24.4035 - final_loss: 22.8573 - internal_loss: 25.9497 - val_loss: 17.6304 - val_final_loss: 16.7563 - val_internal_loss: 18.5045\n",
            "Epoch 81/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 24.5572 - final_loss: 22.9632 - internal_loss: 26.1512\n",
            "Epoch 81: val_final_loss did not improve from 16.75625\n",
            "111/111 [==============================] - 69s 568ms/step - loss: 24.5572 - final_loss: 22.9632 - internal_loss: 26.1512 - val_loss: 18.5824 - val_final_loss: 17.6131 - val_internal_loss: 19.5517\n",
            "Epoch 82/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 24.2303 - final_loss: 22.6625 - internal_loss: 25.7981\n",
            "Epoch 82: val_final_loss improved from 16.75625 to 16.54990, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 583ms/step - loss: 24.2303 - final_loss: 22.6625 - internal_loss: 25.7981 - val_loss: 17.4146 - val_final_loss: 16.5499 - val_internal_loss: 18.2792\n",
            "Epoch 83/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 24.0318 - final_loss: 22.4692 - internal_loss: 25.5944\n",
            "Epoch 83: val_final_loss did not improve from 16.54990\n",
            "111/111 [==============================] - 69s 566ms/step - loss: 24.0318 - final_loss: 22.4692 - internal_loss: 25.5944 - val_loss: 18.2536 - val_final_loss: 17.3187 - val_internal_loss: 19.1884\n",
            "Epoch 84/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 23.9263 - final_loss: 22.3243 - internal_loss: 25.5283\n",
            "Epoch 84: val_final_loss improved from 16.54990 to 16.53819, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 583ms/step - loss: 23.9263 - final_loss: 22.3243 - internal_loss: 25.5283 - val_loss: 17.4111 - val_final_loss: 16.5382 - val_internal_loss: 18.2841\n",
            "Epoch 85/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 23.7767 - final_loss: 22.1892 - internal_loss: 25.3641\n",
            "Epoch 85: val_final_loss did not improve from 16.53819\n",
            "111/111 [==============================] - 68s 567ms/step - loss: 23.7767 - final_loss: 22.1892 - internal_loss: 25.3641 - val_loss: 17.8380 - val_final_loss: 16.9369 - val_internal_loss: 18.7391\n",
            "Epoch 86/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 23.7227 - final_loss: 22.0974 - internal_loss: 25.3481\n",
            "Epoch 86: val_final_loss improved from 16.53819 to 16.39137, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 584ms/step - loss: 23.7227 - final_loss: 22.0974 - internal_loss: 25.3481 - val_loss: 17.2087 - val_final_loss: 16.3914 - val_internal_loss: 18.0259\n",
            "Epoch 87/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 28.5225 - final_loss: 27.4724 - internal_loss: 29.5725\n",
            "Epoch 87: val_final_loss did not improve from 16.39137\n",
            "111/111 [==============================] - 69s 568ms/step - loss: 28.5225 - final_loss: 27.4724 - internal_loss: 29.5725 - val_loss: 20.2860 - val_final_loss: 20.4250 - val_internal_loss: 20.1471\n",
            "Epoch 88/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 24.8317 - final_loss: 23.3107 - internal_loss: 26.3526\n",
            "Epoch 88: val_final_loss did not improve from 16.39137\n",
            "111/111 [==============================] - 68s 566ms/step - loss: 24.8317 - final_loss: 23.3107 - internal_loss: 26.3526 - val_loss: 17.7209 - val_final_loss: 16.8964 - val_internal_loss: 18.5454\n",
            "Epoch 89/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 24.3610 - final_loss: 22.7628 - internal_loss: 25.9591\n",
            "Epoch 89: val_final_loss did not improve from 16.39137\n",
            "111/111 [==============================] - 69s 566ms/step - loss: 24.3610 - final_loss: 22.7628 - internal_loss: 25.9591 - val_loss: 17.8287 - val_final_loss: 16.9086 - val_internal_loss: 18.7487\n",
            "Epoch 90/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 23.7945 - final_loss: 22.1721 - internal_loss: 25.4168\n",
            "Epoch 90: val_final_loss did not improve from 16.39137\n",
            "111/111 [==============================] - 69s 566ms/step - loss: 23.7945 - final_loss: 22.1721 - internal_loss: 25.4168 - val_loss: 17.3934 - val_final_loss: 16.5113 - val_internal_loss: 18.2756\n",
            "Epoch 91/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 23.5854 - final_loss: 21.9397 - internal_loss: 25.2311\n",
            "Epoch 91: val_final_loss did not improve from 16.39137\n",
            "111/111 [==============================] - 70s 570ms/step - loss: 23.5854 - final_loss: 21.9397 - internal_loss: 25.2311 - val_loss: 17.9642 - val_final_loss: 17.0173 - val_internal_loss: 18.9111\n",
            "Epoch 92/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 23.4200 - final_loss: 21.7732 - internal_loss: 25.0669\n",
            "Epoch 92: val_final_loss improved from 16.39137 to 16.34396, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 585ms/step - loss: 23.4200 - final_loss: 21.7732 - internal_loss: 25.0669 - val_loss: 17.1833 - val_final_loss: 16.3440 - val_internal_loss: 18.0226\n",
            "Epoch 93/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 23.2306 - final_loss: 21.5674 - internal_loss: 24.8939\n",
            "Epoch 93: val_final_loss did not improve from 16.34396\n",
            "111/111 [==============================] - 69s 571ms/step - loss: 23.2306 - final_loss: 21.5674 - internal_loss: 24.8939 - val_loss: 17.5799 - val_final_loss: 16.7263 - val_internal_loss: 18.4336\n",
            "Epoch 94/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 23.1381 - final_loss: 21.4537 - internal_loss: 24.8225\n",
            "Epoch 94: val_final_loss did not improve from 16.34396\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 23.1381 - final_loss: 21.4537 - internal_loss: 24.8225 - val_loss: 17.2448 - val_final_loss: 16.3650 - val_internal_loss: 18.1246\n",
            "Epoch 95/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 23.0688 - final_loss: 21.3659 - internal_loss: 24.7717\n",
            "Epoch 95: val_final_loss did not improve from 16.34396\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 23.0688 - final_loss: 21.3659 - internal_loss: 24.7717 - val_loss: 17.8927 - val_final_loss: 16.9659 - val_internal_loss: 18.8195\n",
            "Epoch 96/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 22.9337 - final_loss: 21.2381 - internal_loss: 24.6294\n",
            "Epoch 96: val_final_loss improved from 16.34396 to 16.13748, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 588ms/step - loss: 22.9337 - final_loss: 21.2381 - internal_loss: 24.6294 - val_loss: 16.9367 - val_final_loss: 16.1375 - val_internal_loss: 17.7359\n",
            "Epoch 97/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 22.8198 - final_loss: 21.0945 - internal_loss: 24.5450\n",
            "Epoch 97: val_final_loss did not improve from 16.13748\n",
            "111/111 [==============================] - 70s 571ms/step - loss: 22.8198 - final_loss: 21.0945 - internal_loss: 24.5450 - val_loss: 17.8489 - val_final_loss: 16.9365 - val_internal_loss: 18.7613\n",
            "Epoch 98/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 22.7438 - final_loss: 21.0220 - internal_loss: 24.4657\n",
            "Epoch 98: val_final_loss improved from 16.13748 to 16.09330, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 588ms/step - loss: 22.7438 - final_loss: 21.0219 - internal_loss: 24.4657 - val_loss: 16.9211 - val_final_loss: 16.0933 - val_internal_loss: 17.7489\n",
            "Epoch 99/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 22.6178 - final_loss: 20.8874 - internal_loss: 24.3483\n",
            "Epoch 99: val_final_loss did not improve from 16.09330\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 22.6178 - final_loss: 20.8874 - internal_loss: 24.3483 - val_loss: 17.6656 - val_final_loss: 16.8172 - val_internal_loss: 18.5140\n",
            "Epoch 100/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 22.5848 - final_loss: 20.8386 - internal_loss: 24.3310\n",
            "Epoch 100: val_final_loss did not improve from 16.09330\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 22.5848 - final_loss: 20.8386 - internal_loss: 24.3310 - val_loss: 16.9890 - val_final_loss: 16.1114 - val_internal_loss: 17.8666\n",
            "Epoch 101/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 22.5230 - final_loss: 20.7688 - internal_loss: 24.2772\n",
            "Epoch 101: val_final_loss did not improve from 16.09330\n",
            "111/111 [==============================] - 70s 569ms/step - loss: 22.5230 - final_loss: 20.7688 - internal_loss: 24.2772 - val_loss: 18.3843 - val_final_loss: 17.3923 - val_internal_loss: 19.3764\n",
            "Epoch 102/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 22.4237 - final_loss: 20.6575 - internal_loss: 24.1900\n",
            "Epoch 102: val_final_loss did not improve from 16.09330\n",
            "111/111 [==============================] - 69s 567ms/step - loss: 22.4237 - final_loss: 20.6575 - internal_loss: 24.1900 - val_loss: 16.9114 - val_final_loss: 16.1112 - val_internal_loss: 17.7115\n",
            "Epoch 103/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 22.3339 - final_loss: 20.5650 - internal_loss: 24.1027\n",
            "Epoch 103: val_final_loss did not improve from 16.09330\n",
            "111/111 [==============================] - 70s 571ms/step - loss: 22.3339 - final_loss: 20.5650 - internal_loss: 24.1027 - val_loss: 17.7810 - val_final_loss: 16.8028 - val_internal_loss: 18.7591\n",
            "Epoch 104/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 22.3357 - final_loss: 20.5425 - internal_loss: 24.1290\n",
            "Epoch 104: val_final_loss improved from 16.09330 to 15.94520, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 581ms/step - loss: 22.3357 - final_loss: 20.5425 - internal_loss: 24.1290 - val_loss: 16.7632 - val_final_loss: 15.9452 - val_internal_loss: 17.5812\n",
            "Epoch 105/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 22.1918 - final_loss: 20.4079 - internal_loss: 23.9756\n",
            "Epoch 105: val_final_loss did not improve from 15.94520\n",
            "111/111 [==============================] - 69s 567ms/step - loss: 22.1918 - final_loss: 20.4079 - internal_loss: 23.9756 - val_loss: 17.8839 - val_final_loss: 17.0315 - val_internal_loss: 18.7363\n",
            "Epoch 106/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 22.1528 - final_loss: 20.3505 - internal_loss: 23.9551\n",
            "Epoch 106: val_final_loss improved from 15.94520 to 15.81992, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 582ms/step - loss: 22.1528 - final_loss: 20.3505 - internal_loss: 23.9551 - val_loss: 16.7003 - val_final_loss: 15.8199 - val_internal_loss: 17.5808\n",
            "Epoch 107/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 22.0348 - final_loss: 20.2389 - internal_loss: 23.8306\n",
            "Epoch 107: val_final_loss did not improve from 15.81992\n",
            "111/111 [==============================] - 71s 588ms/step - loss: 22.0348 - final_loss: 20.2389 - internal_loss: 23.8306 - val_loss: 17.3945 - val_final_loss: 16.4922 - val_internal_loss: 18.2968\n",
            "Epoch 108/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 22.0866 - final_loss: 20.2608 - internal_loss: 23.9123\n",
            "Epoch 108: val_final_loss did not improve from 15.81992\n",
            "111/111 [==============================] - 69s 568ms/step - loss: 22.0866 - final_loss: 20.2608 - internal_loss: 23.9123 - val_loss: 16.7394 - val_final_loss: 15.8458 - val_internal_loss: 17.6329\n",
            "Epoch 109/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 21.9709 - final_loss: 20.1491 - internal_loss: 23.7926\n",
            "Epoch 109: val_final_loss did not improve from 15.81992\n",
            "111/111 [==============================] - 69s 566ms/step - loss: 21.9709 - final_loss: 20.1491 - internal_loss: 23.7926 - val_loss: 17.0894 - val_final_loss: 16.2414 - val_internal_loss: 17.9374\n",
            "Epoch 110/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 21.8553 - final_loss: 20.0127 - internal_loss: 23.6979\n",
            "Epoch 110: val_final_loss improved from 15.81992 to 15.75693, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 69s 585ms/step - loss: 21.8553 - final_loss: 20.0127 - internal_loss: 23.6979 - val_loss: 16.6021 - val_final_loss: 15.7569 - val_internal_loss: 17.4473\n",
            "Epoch 111/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 21.7710 - final_loss: 19.9360 - internal_loss: 23.6060\n",
            "Epoch 111: val_final_loss did not improve from 15.75693\n",
            "111/111 [==============================] - 70s 570ms/step - loss: 21.7710 - final_loss: 19.9360 - internal_loss: 23.6060 - val_loss: 17.1621 - val_final_loss: 16.4010 - val_internal_loss: 17.9232\n",
            "Epoch 112/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 21.7259 - final_loss: 19.8890 - internal_loss: 23.5628\n",
            "Epoch 112: val_final_loss did not improve from 15.75693\n",
            "111/111 [==============================] - 69s 571ms/step - loss: 21.7259 - final_loss: 19.8890 - internal_loss: 23.5628 - val_loss: 16.6504 - val_final_loss: 15.7927 - val_internal_loss: 17.5081\n",
            "Epoch 113/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 21.7020 - final_loss: 19.8487 - internal_loss: 23.5553\n",
            "Epoch 113: val_final_loss did not improve from 15.75693\n",
            "111/111 [==============================] - 70s 572ms/step - loss: 21.7020 - final_loss: 19.8487 - internal_loss: 23.5553 - val_loss: 17.1614 - val_final_loss: 16.3278 - val_internal_loss: 17.9951\n",
            "Epoch 114/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 21.5420 - final_loss: 19.7067 - internal_loss: 23.3772\n",
            "Epoch 114: val_final_loss improved from 15.75693 to 15.61675, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 588ms/step - loss: 21.5420 - final_loss: 19.7067 - internal_loss: 23.3772 - val_loss: 16.4442 - val_final_loss: 15.6168 - val_internal_loss: 17.2717\n",
            "Epoch 115/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 21.5577 - final_loss: 19.6932 - internal_loss: 23.4221\n",
            "Epoch 115: val_final_loss did not improve from 15.61675\n",
            "111/111 [==============================] - 70s 570ms/step - loss: 21.5577 - final_loss: 19.6932 - internal_loss: 23.4221 - val_loss: 17.0869 - val_final_loss: 16.2458 - val_internal_loss: 17.9280\n",
            "Epoch 116/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 21.4457 - final_loss: 19.5954 - internal_loss: 23.2961\n",
            "Epoch 116: val_final_loss did not improve from 15.61675\n",
            "111/111 [==============================] - 70s 569ms/step - loss: 21.4457 - final_loss: 19.5954 - internal_loss: 23.2961 - val_loss: 16.5544 - val_final_loss: 15.7534 - val_internal_loss: 17.3555\n",
            "Epoch 117/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 21.3587 - final_loss: 19.4763 - internal_loss: 23.2411\n",
            "Epoch 117: val_final_loss did not improve from 15.61675\n",
            "111/111 [==============================] - 69s 568ms/step - loss: 21.3587 - final_loss: 19.4763 - internal_loss: 23.2411 - val_loss: 17.3448 - val_final_loss: 16.4917 - val_internal_loss: 18.1980\n",
            "Epoch 118/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 21.2875 - final_loss: 19.3979 - internal_loss: 23.1771\n",
            "Epoch 118: val_final_loss did not improve from 15.61675\n",
            "111/111 [==============================] - 69s 567ms/step - loss: 21.2875 - final_loss: 19.3979 - internal_loss: 23.1771 - val_loss: 16.4931 - val_final_loss: 15.6791 - val_internal_loss: 17.3071\n",
            "Epoch 119/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 21.2463 - final_loss: 19.3603 - internal_loss: 23.1324\n",
            "Epoch 119: val_final_loss did not improve from 15.61675\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 21.2463 - final_loss: 19.3603 - internal_loss: 23.1324 - val_loss: 17.1976 - val_final_loss: 16.3067 - val_internal_loss: 18.0886\n",
            "Epoch 120/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 21.1944 - final_loss: 19.2957 - internal_loss: 23.0932\n",
            "Epoch 120: val_final_loss did not improve from 15.61675\n",
            "111/111 [==============================] - 69s 568ms/step - loss: 21.1944 - final_loss: 19.2957 - internal_loss: 23.0932 - val_loss: 16.4928 - val_final_loss: 15.6587 - val_internal_loss: 17.3268\n",
            "Epoch 121/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 21.1317 - final_loss: 19.2294 - internal_loss: 23.0340\n",
            "Epoch 121: val_final_loss did not improve from 15.61675\n",
            "111/111 [==============================] - 69s 567ms/step - loss: 21.1317 - final_loss: 19.2294 - internal_loss: 23.0340 - val_loss: 16.6205 - val_final_loss: 15.8167 - val_internal_loss: 17.4243\n",
            "Epoch 122/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 21.0016 - final_loss: 19.0861 - internal_loss: 22.9170\n",
            "Epoch 122: val_final_loss improved from 15.61675 to 15.51355, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 583ms/step - loss: 21.0016 - final_loss: 19.0861 - internal_loss: 22.9170 - val_loss: 16.3513 - val_final_loss: 15.5136 - val_internal_loss: 17.1891\n",
            "Epoch 123/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 20.9848 - final_loss: 19.0563 - internal_loss: 22.9132\n",
            "Epoch 123: val_final_loss did not improve from 15.51355\n",
            "111/111 [==============================] - 68s 569ms/step - loss: 20.9848 - final_loss: 19.0563 - internal_loss: 22.9132 - val_loss: 17.0862 - val_final_loss: 16.2247 - val_internal_loss: 17.9476\n",
            "Epoch 124/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 20.9332 - final_loss: 18.9971 - internal_loss: 22.8694\n",
            "Epoch 124: val_final_loss did not improve from 15.51355\n",
            "111/111 [==============================] - 69s 567ms/step - loss: 20.9332 - final_loss: 18.9971 - internal_loss: 22.8694 - val_loss: 16.3896 - val_final_loss: 15.5711 - val_internal_loss: 17.2080\n",
            "Epoch 125/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 20.7466 - final_loss: 18.8054 - internal_loss: 22.6879\n",
            "Epoch 125: val_final_loss did not improve from 15.51355\n",
            "111/111 [==============================] - 68s 571ms/step - loss: 20.7466 - final_loss: 18.8054 - internal_loss: 22.6879 - val_loss: 16.7950 - val_final_loss: 15.9078 - val_internal_loss: 17.6822\n",
            "Epoch 126/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 20.7600 - final_loss: 18.8161 - internal_loss: 22.7040\n",
            "Epoch 126: val_final_loss improved from 15.51355 to 15.31937, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 584ms/step - loss: 20.7600 - final_loss: 18.8161 - internal_loss: 22.7040 - val_loss: 16.1632 - val_final_loss: 15.3194 - val_internal_loss: 17.0070\n",
            "Epoch 127/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 20.7087 - final_loss: 18.7704 - internal_loss: 22.6471\n",
            "Epoch 127: val_final_loss did not improve from 15.31937\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 20.7087 - final_loss: 18.7704 - internal_loss: 22.6471 - val_loss: 16.8194 - val_final_loss: 15.9641 - val_internal_loss: 17.6746\n",
            "Epoch 128/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 20.6550 - final_loss: 18.7123 - internal_loss: 22.5977\n",
            "Epoch 128: val_final_loss did not improve from 15.31937\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 20.6550 - final_loss: 18.7123 - internal_loss: 22.5977 - val_loss: 16.2506 - val_final_loss: 15.4433 - val_internal_loss: 17.0579\n",
            "Epoch 129/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 20.5669 - final_loss: 18.6120 - internal_loss: 22.5218\n",
            "Epoch 129: val_final_loss did not improve from 15.31937\n",
            "111/111 [==============================] - 72s 593ms/step - loss: 20.5669 - final_loss: 18.6120 - internal_loss: 22.5218 - val_loss: 16.9115 - val_final_loss: 16.0654 - val_internal_loss: 17.7576\n",
            "Epoch 130/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 20.5645 - final_loss: 18.6095 - internal_loss: 22.5194\n",
            "Epoch 130: val_final_loss did not improve from 15.31937\n",
            "111/111 [==============================] - 69s 571ms/step - loss: 20.5645 - final_loss: 18.6095 - internal_loss: 22.5194 - val_loss: 16.1284 - val_final_loss: 15.3428 - val_internal_loss: 16.9140\n",
            "Epoch 131/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 20.5552 - final_loss: 18.5887 - internal_loss: 22.5217\n",
            "Epoch 131: val_final_loss did not improve from 15.31937\n",
            "111/111 [==============================] - 69s 566ms/step - loss: 20.5552 - final_loss: 18.5887 - internal_loss: 22.5217 - val_loss: 16.6858 - val_final_loss: 15.8427 - val_internal_loss: 17.5290\n",
            "Epoch 132/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 20.4867 - final_loss: 18.5157 - internal_loss: 22.4578\n",
            "Epoch 132: val_final_loss did not improve from 15.31937\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 20.4867 - final_loss: 18.5157 - internal_loss: 22.4578 - val_loss: 16.1091 - val_final_loss: 15.3277 - val_internal_loss: 16.8906\n",
            "Epoch 133/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 20.4190 - final_loss: 18.4379 - internal_loss: 22.4001\n",
            "Epoch 133: val_final_loss did not improve from 15.31937\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 20.4190 - final_loss: 18.4379 - internal_loss: 22.4000 - val_loss: 16.8022 - val_final_loss: 15.9472 - val_internal_loss: 17.6571\n",
            "Epoch 134/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 20.3518 - final_loss: 18.3558 - internal_loss: 22.3478\n",
            "Epoch 134: val_final_loss did not improve from 15.31937\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 20.3518 - final_loss: 18.3558 - internal_loss: 22.3478 - val_loss: 16.2709 - val_final_loss: 15.4788 - val_internal_loss: 17.0631\n",
            "Epoch 135/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 20.2818 - final_loss: 18.2971 - internal_loss: 22.2665\n",
            "Epoch 135: val_final_loss did not improve from 15.31937\n",
            "111/111 [==============================] - 70s 572ms/step - loss: 20.2818 - final_loss: 18.2971 - internal_loss: 22.2665 - val_loss: 16.4657 - val_final_loss: 15.6646 - val_internal_loss: 17.2669\n",
            "Epoch 136/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 20.2148 - final_loss: 18.2201 - internal_loss: 22.2096\n",
            "Epoch 136: val_final_loss improved from 15.31937 to 15.31526, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 585ms/step - loss: 20.2148 - final_loss: 18.2201 - internal_loss: 22.2096 - val_loss: 16.0878 - val_final_loss: 15.3153 - val_internal_loss: 16.8604\n",
            "Epoch 137/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 20.1229 - final_loss: 18.1149 - internal_loss: 22.1310\n",
            "Epoch 137: val_final_loss did not improve from 15.31526\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 20.1229 - final_loss: 18.1149 - internal_loss: 22.1310 - val_loss: 16.5691 - val_final_loss: 15.7921 - val_internal_loss: 17.3461\n",
            "Epoch 138/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 20.0904 - final_loss: 18.0825 - internal_loss: 22.0982\n",
            "Epoch 138: val_final_loss did not improve from 15.31526\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 20.0904 - final_loss: 18.0825 - internal_loss: 22.0982 - val_loss: 16.0821 - val_final_loss: 15.3190 - val_internal_loss: 16.8451\n",
            "Epoch 139/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 20.0914 - final_loss: 18.0896 - internal_loss: 22.0931\n",
            "Epoch 139: val_final_loss did not improve from 15.31526\n",
            "111/111 [==============================] - 70s 572ms/step - loss: 20.0914 - final_loss: 18.0896 - internal_loss: 22.0931 - val_loss: 16.6275 - val_final_loss: 15.8835 - val_internal_loss: 17.3715\n",
            "Epoch 140/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.9712 - final_loss: 17.9502 - internal_loss: 21.9922\n",
            "Epoch 140: val_final_loss did not improve from 15.31526\n",
            "111/111 [==============================] - 68s 571ms/step - loss: 19.9712 - final_loss: 17.9502 - internal_loss: 21.9922 - val_loss: 16.1936 - val_final_loss: 15.4037 - val_internal_loss: 16.9836\n",
            "Epoch 141/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 20.0239 - final_loss: 17.9964 - internal_loss: 22.0514\n",
            "Epoch 141: val_final_loss did not improve from 15.31526\n",
            "111/111 [==============================] - 69s 567ms/step - loss: 20.0239 - final_loss: 17.9964 - internal_loss: 22.0514 - val_loss: 16.8243 - val_final_loss: 16.0080 - val_internal_loss: 17.6406\n",
            "Epoch 142/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.9390 - final_loss: 17.8993 - internal_loss: 21.9786\n",
            "Epoch 142: val_final_loss improved from 15.31526 to 15.27519, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 582ms/step - loss: 19.9390 - final_loss: 17.8993 - internal_loss: 21.9786 - val_loss: 15.9952 - val_final_loss: 15.2752 - val_internal_loss: 16.7152\n",
            "Epoch 143/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.9927 - final_loss: 17.9450 - internal_loss: 22.0405\n",
            "Epoch 143: val_final_loss did not improve from 15.27519\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 19.9927 - final_loss: 17.9450 - internal_loss: 22.0405 - val_loss: 16.6116 - val_final_loss: 15.7875 - val_internal_loss: 17.4358\n",
            "Epoch 144/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.9284 - final_loss: 17.8739 - internal_loss: 21.9830\n",
            "Epoch 144: val_final_loss did not improve from 15.27519\n",
            "111/111 [==============================] - 70s 570ms/step - loss: 19.9284 - final_loss: 17.8739 - internal_loss: 21.9830 - val_loss: 16.0729 - val_final_loss: 15.3147 - val_internal_loss: 16.8312\n",
            "Epoch 145/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.8097 - final_loss: 17.7582 - internal_loss: 21.8613\n",
            "Epoch 145: val_final_loss did not improve from 15.27519\n",
            "111/111 [==============================] - 70s 572ms/step - loss: 19.8097 - final_loss: 17.7582 - internal_loss: 21.8613 - val_loss: 16.2749 - val_final_loss: 15.4595 - val_internal_loss: 17.0902\n",
            "Epoch 146/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.7259 - final_loss: 17.6786 - internal_loss: 21.7732\n",
            "Epoch 146: val_final_loss improved from 15.27519 to 15.16700, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 584ms/step - loss: 19.7259 - final_loss: 17.6786 - internal_loss: 21.7732 - val_loss: 15.9970 - val_final_loss: 15.1670 - val_internal_loss: 16.8270\n",
            "Epoch 147/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.6695 - final_loss: 17.6114 - internal_loss: 21.7275\n",
            "Epoch 147: val_final_loss did not improve from 15.16700\n",
            "111/111 [==============================] - 69s 568ms/step - loss: 19.6695 - final_loss: 17.6114 - internal_loss: 21.7275 - val_loss: 16.4423 - val_final_loss: 15.6026 - val_internal_loss: 17.2821\n",
            "Epoch 148/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.6134 - final_loss: 17.5608 - internal_loss: 21.6660\n",
            "Epoch 148: val_final_loss improved from 15.16700 to 15.03598, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 584ms/step - loss: 19.6134 - final_loss: 17.5608 - internal_loss: 21.6660 - val_loss: 15.8293 - val_final_loss: 15.0360 - val_internal_loss: 16.6225\n",
            "Epoch 149/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.5578 - final_loss: 17.5062 - internal_loss: 21.6094\n",
            "Epoch 149: val_final_loss did not improve from 15.03598\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 19.5578 - final_loss: 17.5062 - internal_loss: 21.6094 - val_loss: 16.0400 - val_final_loss: 15.2581 - val_internal_loss: 16.8219\n",
            "Epoch 150/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.5117 - final_loss: 17.4377 - internal_loss: 21.5858\n",
            "Epoch 150: val_final_loss did not improve from 15.03598\n",
            "111/111 [==============================] - 72s 591ms/step - loss: 19.5117 - final_loss: 17.4377 - internal_loss: 21.5858 - val_loss: 15.9037 - val_final_loss: 15.1317 - val_internal_loss: 16.6757\n",
            "Epoch 151/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.4210 - final_loss: 17.3357 - internal_loss: 21.5063\n",
            "Epoch 151: val_final_loss did not improve from 15.03598\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 19.4210 - final_loss: 17.3357 - internal_loss: 21.5063 - val_loss: 16.4716 - val_final_loss: 15.6689 - val_internal_loss: 17.2743\n",
            "Epoch 152/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.3722 - final_loss: 17.2979 - internal_loss: 21.4466\n",
            "Epoch 152: val_final_loss did not improve from 15.03598\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 19.3722 - final_loss: 17.2979 - internal_loss: 21.4466 - val_loss: 15.8158 - val_final_loss: 15.0681 - val_internal_loss: 16.5635\n",
            "Epoch 153/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.3709 - final_loss: 17.2841 - internal_loss: 21.4577\n",
            "Epoch 153: val_final_loss did not improve from 15.03598\n",
            "111/111 [==============================] - 70s 570ms/step - loss: 19.3709 - final_loss: 17.2841 - internal_loss: 21.4577 - val_loss: 16.4112 - val_final_loss: 15.6045 - val_internal_loss: 17.2180\n",
            "Epoch 154/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.3421 - final_loss: 17.2576 - internal_loss: 21.4265\n",
            "Epoch 154: val_final_loss improved from 15.03598 to 15.03361, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 72s 588ms/step - loss: 19.3421 - final_loss: 17.2576 - internal_loss: 21.4265 - val_loss: 15.8165 - val_final_loss: 15.0336 - val_internal_loss: 16.5993\n",
            "Epoch 155/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.2665 - final_loss: 17.1703 - internal_loss: 21.3627\n",
            "Epoch 155: val_final_loss did not improve from 15.03361\n",
            "111/111 [==============================] - 70s 570ms/step - loss: 19.2665 - final_loss: 17.1703 - internal_loss: 21.3627 - val_loss: 16.4316 - val_final_loss: 15.5749 - val_internal_loss: 17.2884\n",
            "Epoch 156/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.2472 - final_loss: 17.1657 - internal_loss: 21.3288\n",
            "Epoch 156: val_final_loss did not improve from 15.03361\n",
            "111/111 [==============================] - 70s 571ms/step - loss: 19.2472 - final_loss: 17.1657 - internal_loss: 21.3288 - val_loss: 15.8414 - val_final_loss: 15.0616 - val_internal_loss: 16.6212\n",
            "Epoch 157/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.2254 - final_loss: 17.1291 - internal_loss: 21.3216\n",
            "Epoch 157: val_final_loss did not improve from 15.03361\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 19.2254 - final_loss: 17.1291 - internal_loss: 21.3216 - val_loss: 16.0386 - val_final_loss: 15.3197 - val_internal_loss: 16.7575\n",
            "Epoch 158/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.1228 - final_loss: 17.0184 - internal_loss: 21.2271\n",
            "Epoch 158: val_final_loss did not improve from 15.03361\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 19.1228 - final_loss: 17.0184 - internal_loss: 21.2271 - val_loss: 15.8198 - val_final_loss: 15.0645 - val_internal_loss: 16.5751\n",
            "Epoch 159/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.0878 - final_loss: 16.9751 - internal_loss: 21.2004\n",
            "Epoch 159: val_final_loss did not improve from 15.03361\n",
            "111/111 [==============================] - 70s 571ms/step - loss: 19.0878 - final_loss: 16.9751 - internal_loss: 21.2004 - val_loss: 16.1086 - val_final_loss: 15.3870 - val_internal_loss: 16.8302\n",
            "Epoch 160/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.0475 - final_loss: 16.9331 - internal_loss: 21.1619\n",
            "Epoch 160: val_final_loss did not improve from 15.03361\n",
            "111/111 [==============================] - 71s 580ms/step - loss: 19.0475 - final_loss: 16.9331 - internal_loss: 21.1619 - val_loss: 15.8120 - val_final_loss: 15.0435 - val_internal_loss: 16.5805\n",
            "Epoch 161/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 19.0220 - final_loss: 16.9182 - internal_loss: 21.1257\n",
            "Epoch 161: val_final_loss did not improve from 15.03361\n",
            "111/111 [==============================] - 70s 581ms/step - loss: 19.0220 - final_loss: 16.9182 - internal_loss: 21.1257 - val_loss: 16.0120 - val_final_loss: 15.2397 - val_internal_loss: 16.7842\n",
            "Epoch 162/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.9773 - final_loss: 16.8621 - internal_loss: 21.0925\n",
            "Epoch 162: val_final_loss improved from 15.03361 to 14.91316, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 587ms/step - loss: 18.9773 - final_loss: 16.8621 - internal_loss: 21.0925 - val_loss: 15.6715 - val_final_loss: 14.9132 - val_internal_loss: 16.4298\n",
            "Epoch 163/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.9152 - final_loss: 16.7891 - internal_loss: 21.0413\n",
            "Epoch 163: val_final_loss did not improve from 14.91316\n",
            "111/111 [==============================] - 69s 573ms/step - loss: 18.9152 - final_loss: 16.7891 - internal_loss: 21.0413 - val_loss: 15.9657 - val_final_loss: 15.1932 - val_internal_loss: 16.7382\n",
            "Epoch 164/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.8826 - final_loss: 16.7616 - internal_loss: 21.0036\n",
            "Epoch 164: val_final_loss did not improve from 14.91316\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 18.8826 - final_loss: 16.7616 - internal_loss: 21.0036 - val_loss: 15.7547 - val_final_loss: 14.9958 - val_internal_loss: 16.5137\n",
            "Epoch 165/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.8918 - final_loss: 16.7615 - internal_loss: 21.0221\n",
            "Epoch 165: val_final_loss did not improve from 14.91316\n",
            "111/111 [==============================] - 67s 569ms/step - loss: 18.8918 - final_loss: 16.7615 - internal_loss: 21.0221 - val_loss: 16.0497 - val_final_loss: 15.2372 - val_internal_loss: 16.8622\n",
            "Epoch 166/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.8157 - final_loss: 16.6756 - internal_loss: 20.9557\n",
            "Epoch 166: val_final_loss improved from 14.91316 to 14.90978, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 588ms/step - loss: 18.8157 - final_loss: 16.6756 - internal_loss: 20.9557 - val_loss: 15.6701 - val_final_loss: 14.9098 - val_internal_loss: 16.4305\n",
            "Epoch 167/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.7592 - final_loss: 16.6204 - internal_loss: 20.8980\n",
            "Epoch 167: val_final_loss did not improve from 14.90978\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 18.7592 - final_loss: 16.6204 - internal_loss: 20.8980 - val_loss: 15.8901 - val_final_loss: 15.0997 - val_internal_loss: 16.6805\n",
            "Epoch 168/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.6885 - final_loss: 16.5412 - internal_loss: 20.8358\n",
            "Epoch 168: val_final_loss did not improve from 14.90978\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 18.6885 - final_loss: 16.5412 - internal_loss: 20.8358 - val_loss: 15.7086 - val_final_loss: 14.9644 - val_internal_loss: 16.4529\n",
            "Epoch 169/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.6663 - final_loss: 16.5225 - internal_loss: 20.8102\n",
            "Epoch 169: val_final_loss did not improve from 14.90978\n",
            "111/111 [==============================] - 69s 568ms/step - loss: 18.6663 - final_loss: 16.5225 - internal_loss: 20.8102 - val_loss: 15.9759 - val_final_loss: 15.2178 - val_internal_loss: 16.7341\n",
            "Epoch 170/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.6321 - final_loss: 16.4850 - internal_loss: 20.7791\n",
            "Epoch 170: val_final_loss did not improve from 14.90978\n",
            "111/111 [==============================] - 69s 566ms/step - loss: 18.6321 - final_loss: 16.4850 - internal_loss: 20.7791 - val_loss: 15.7457 - val_final_loss: 15.0292 - val_internal_loss: 16.4622\n",
            "Epoch 171/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.6274 - final_loss: 16.4603 - internal_loss: 20.7946\n",
            "Epoch 171: val_final_loss did not improve from 14.90978\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 18.6274 - final_loss: 16.4603 - internal_loss: 20.7946 - val_loss: 15.8711 - val_final_loss: 15.0589 - val_internal_loss: 16.6833\n",
            "Epoch 172/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.5448 - final_loss: 16.3800 - internal_loss: 20.7096\n",
            "Epoch 172: val_final_loss did not improve from 14.90978\n",
            "111/111 [==============================] - 70s 571ms/step - loss: 18.5448 - final_loss: 16.3800 - internal_loss: 20.7096 - val_loss: 15.7311 - val_final_loss: 15.0106 - val_internal_loss: 16.4515\n",
            "Epoch 173/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.5210 - final_loss: 16.3698 - internal_loss: 20.6722\n",
            "Epoch 173: val_final_loss did not improve from 14.90978\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 18.5210 - final_loss: 16.3698 - internal_loss: 20.6722 - val_loss: 16.0125 - val_final_loss: 15.2273 - val_internal_loss: 16.7976\n",
            "Epoch 174/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.5407 - final_loss: 16.3810 - internal_loss: 20.7004\n",
            "Epoch 174: val_final_loss improved from 14.90978 to 14.81109, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 585ms/step - loss: 18.5407 - final_loss: 16.3810 - internal_loss: 20.7004 - val_loss: 15.5623 - val_final_loss: 14.8111 - val_internal_loss: 16.3135\n",
            "Epoch 175/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.4587 - final_loss: 16.2805 - internal_loss: 20.6368\n",
            "Epoch 175: val_final_loss did not improve from 14.81109\n",
            "111/111 [==============================] - 69s 568ms/step - loss: 18.4587 - final_loss: 16.2805 - internal_loss: 20.6368 - val_loss: 15.9058 - val_final_loss: 15.1224 - val_internal_loss: 16.6893\n",
            "Epoch 176/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.3858 - final_loss: 16.2109 - internal_loss: 20.5608\n",
            "Epoch 176: val_final_loss improved from 14.81109 to 14.78123, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 584ms/step - loss: 18.3858 - final_loss: 16.2109 - internal_loss: 20.5608 - val_loss: 15.5600 - val_final_loss: 14.7812 - val_internal_loss: 16.3388\n",
            "Epoch 177/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.2748 - final_loss: 16.0963 - internal_loss: 20.4532\n",
            "Epoch 177: val_final_loss did not improve from 14.78123\n",
            "111/111 [==============================] - 70s 570ms/step - loss: 18.2748 - final_loss: 16.0963 - internal_loss: 20.4532 - val_loss: 16.0290 - val_final_loss: 15.2075 - val_internal_loss: 16.8505\n",
            "Epoch 178/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.3386 - final_loss: 16.1651 - internal_loss: 20.5120\n",
            "Epoch 178: val_final_loss did not improve from 14.78123\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 18.3386 - final_loss: 16.1651 - internal_loss: 20.5120 - val_loss: 15.6361 - val_final_loss: 14.9342 - val_internal_loss: 16.3381\n",
            "Epoch 179/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.2547 - final_loss: 16.0761 - internal_loss: 20.4332\n",
            "Epoch 179: val_final_loss did not improve from 14.78123\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 18.2547 - final_loss: 16.0761 - internal_loss: 20.4332 - val_loss: 15.8337 - val_final_loss: 15.0785 - val_internal_loss: 16.5888\n",
            "Epoch 180/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.2597 - final_loss: 16.0942 - internal_loss: 20.4252\n",
            "Epoch 180: val_final_loss did not improve from 14.78123\n",
            "111/111 [==============================] - 70s 572ms/step - loss: 18.2597 - final_loss: 16.0942 - internal_loss: 20.4252 - val_loss: 15.5774 - val_final_loss: 14.8119 - val_internal_loss: 16.3429\n",
            "Epoch 181/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.1522 - final_loss: 15.9662 - internal_loss: 20.3383\n",
            "Epoch 181: val_final_loss did not improve from 14.78123\n",
            "111/111 [==============================] - 69s 574ms/step - loss: 18.1522 - final_loss: 15.9662 - internal_loss: 20.3383 - val_loss: 15.8724 - val_final_loss: 15.2279 - val_internal_loss: 16.5168\n",
            "Epoch 182/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.1647 - final_loss: 15.9844 - internal_loss: 20.3450\n",
            "Epoch 182: val_final_loss did not improve from 14.78123\n",
            "111/111 [==============================] - 67s 569ms/step - loss: 18.1647 - final_loss: 15.9844 - internal_loss: 20.3450 - val_loss: 15.5520 - val_final_loss: 14.8194 - val_internal_loss: 16.2846\n",
            "Epoch 183/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.1619 - final_loss: 15.9761 - internal_loss: 20.3477\n",
            "Epoch 183: val_final_loss did not improve from 14.78123\n",
            "111/111 [==============================] - 70s 573ms/step - loss: 18.1619 - final_loss: 15.9761 - internal_loss: 20.3477 - val_loss: 15.9633 - val_final_loss: 15.2229 - val_internal_loss: 16.7037\n",
            "Epoch 184/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.0943 - final_loss: 15.8936 - internal_loss: 20.2951\n",
            "Epoch 184: val_final_loss improved from 14.78123 to 14.77108, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 586ms/step - loss: 18.0943 - final_loss: 15.8936 - internal_loss: 20.2951 - val_loss: 15.4971 - val_final_loss: 14.7711 - val_internal_loss: 16.2231\n",
            "Epoch 185/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.1094 - final_loss: 15.9194 - internal_loss: 20.2994\n",
            "Epoch 185: val_final_loss did not improve from 14.77108\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 18.1094 - final_loss: 15.9194 - internal_loss: 20.2994 - val_loss: 15.9026 - val_final_loss: 15.1536 - val_internal_loss: 16.6516\n",
            "Epoch 186/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.0415 - final_loss: 15.8538 - internal_loss: 20.2292\n",
            "Epoch 186: val_final_loss improved from 14.77108 to 14.75418, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 584ms/step - loss: 18.0415 - final_loss: 15.8538 - internal_loss: 20.2292 - val_loss: 15.5229 - val_final_loss: 14.7542 - val_internal_loss: 16.2916\n",
            "Epoch 187/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.0137 - final_loss: 15.8069 - internal_loss: 20.2204\n",
            "Epoch 187: val_final_loss did not improve from 14.75418\n",
            "111/111 [==============================] - 69s 571ms/step - loss: 18.0137 - final_loss: 15.8069 - internal_loss: 20.2204 - val_loss: 15.6921 - val_final_loss: 14.9407 - val_internal_loss: 16.4434\n",
            "Epoch 188/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.9814 - final_loss: 15.7709 - internal_loss: 20.1918\n",
            "Epoch 188: val_final_loss improved from 14.75418 to 14.72488, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 583ms/step - loss: 17.9814 - final_loss: 15.7709 - internal_loss: 20.1918 - val_loss: 15.4810 - val_final_loss: 14.7249 - val_internal_loss: 16.2371\n",
            "Epoch 189/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.9480 - final_loss: 15.7404 - internal_loss: 20.1556\n",
            "Epoch 189: val_final_loss did not improve from 14.72488\n",
            "111/111 [==============================] - 70s 570ms/step - loss: 17.9480 - final_loss: 15.7404 - internal_loss: 20.1556 - val_loss: 15.8693 - val_final_loss: 15.1403 - val_internal_loss: 16.5983\n",
            "Epoch 190/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 18.1620 - final_loss: 15.9110 - internal_loss: 20.4130\n",
            "Epoch 190: val_final_loss did not improve from 14.72488\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 18.1620 - final_loss: 15.9110 - internal_loss: 20.4130 - val_loss: 15.5844 - val_final_loss: 14.8512 - val_internal_loss: 16.3176\n",
            "Epoch 191/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.9258 - final_loss: 15.6993 - internal_loss: 20.1524\n",
            "Epoch 191: val_final_loss did not improve from 14.72488\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 17.9258 - final_loss: 15.6993 - internal_loss: 20.1524 - val_loss: 15.7331 - val_final_loss: 14.9954 - val_internal_loss: 16.4708\n",
            "Epoch 192/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.8755 - final_loss: 15.6646 - internal_loss: 20.0863\n",
            "Epoch 192: val_final_loss did not improve from 14.72488\n",
            "111/111 [==============================] - 70s 573ms/step - loss: 17.8755 - final_loss: 15.6646 - internal_loss: 20.0863 - val_loss: 15.5489 - val_final_loss: 14.8127 - val_internal_loss: 16.2851\n",
            "Epoch 193/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.7975 - final_loss: 15.5751 - internal_loss: 20.0199\n",
            "Epoch 193: val_final_loss did not improve from 14.72488\n",
            "111/111 [==============================] - 72s 594ms/step - loss: 17.7975 - final_loss: 15.5751 - internal_loss: 20.0199 - val_loss: 15.6414 - val_final_loss: 14.8968 - val_internal_loss: 16.3859\n",
            "Epoch 194/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.7874 - final_loss: 15.5631 - internal_loss: 20.0118\n",
            "Epoch 194: val_final_loss did not improve from 14.72488\n",
            "111/111 [==============================] - 70s 573ms/step - loss: 17.7874 - final_loss: 15.5631 - internal_loss: 20.0118 - val_loss: 15.5340 - val_final_loss: 14.8425 - val_internal_loss: 16.2255\n",
            "Epoch 195/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.7695 - final_loss: 15.5454 - internal_loss: 19.9936\n",
            "Epoch 195: val_final_loss did not improve from 14.72488\n",
            "111/111 [==============================] - 70s 572ms/step - loss: 17.7695 - final_loss: 15.5454 - internal_loss: 19.9936 - val_loss: 15.6139 - val_final_loss: 14.8849 - val_internal_loss: 16.3429\n",
            "Epoch 196/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.7508 - final_loss: 15.5185 - internal_loss: 19.9832\n",
            "Epoch 196: val_final_loss did not improve from 14.72488\n",
            "111/111 [==============================] - 69s 571ms/step - loss: 17.7508 - final_loss: 15.5185 - internal_loss: 19.9832 - val_loss: 15.4972 - val_final_loss: 14.7468 - val_internal_loss: 16.2476\n",
            "Epoch 197/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.7504 - final_loss: 15.5218 - internal_loss: 19.9790\n",
            "Epoch 197: val_final_loss did not improve from 14.72488\n",
            "111/111 [==============================] - 70s 570ms/step - loss: 17.7504 - final_loss: 15.5218 - internal_loss: 19.9790 - val_loss: 15.5287 - val_final_loss: 14.8567 - val_internal_loss: 16.2007\n",
            "Epoch 198/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.6747 - final_loss: 15.4359 - internal_loss: 19.9136\n",
            "Epoch 198: val_final_loss did not improve from 14.72488\n",
            "111/111 [==============================] - 70s 570ms/step - loss: 17.6747 - final_loss: 15.4359 - internal_loss: 19.9136 - val_loss: 15.4968 - val_final_loss: 14.7843 - val_internal_loss: 16.2093\n",
            "Epoch 199/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.6194 - final_loss: 15.3802 - internal_loss: 19.8586\n",
            "Epoch 199: val_final_loss did not improve from 14.72488\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 17.6194 - final_loss: 15.3802 - internal_loss: 19.8586 - val_loss: 15.4986 - val_final_loss: 14.7403 - val_internal_loss: 16.2568\n",
            "Epoch 200/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.6182 - final_loss: 15.3907 - internal_loss: 19.8457\n",
            "Epoch 200: val_final_loss did not improve from 14.72488\n",
            "111/111 [==============================] - 70s 569ms/step - loss: 17.6182 - final_loss: 15.3907 - internal_loss: 19.8457 - val_loss: 15.4753 - val_final_loss: 14.7676 - val_internal_loss: 16.1829\n",
            "Epoch 201/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.5738 - final_loss: 15.3384 - internal_loss: 19.8091\n",
            "Epoch 201: val_final_loss did not improve from 14.72488\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 17.5738 - final_loss: 15.3384 - internal_loss: 19.8091 - val_loss: 15.6113 - val_final_loss: 14.9560 - val_internal_loss: 16.2665\n",
            "Epoch 202/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.5467 - final_loss: 15.3088 - internal_loss: 19.7845\n",
            "Epoch 202: val_final_loss did not improve from 14.72488\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 17.5467 - final_loss: 15.3088 - internal_loss: 19.7845 - val_loss: 15.4752 - val_final_loss: 14.7638 - val_internal_loss: 16.1866\n",
            "Epoch 203/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.5000 - final_loss: 15.2558 - internal_loss: 19.7442\n",
            "Epoch 203: val_final_loss did not improve from 14.72488\n",
            "111/111 [==============================] - 68s 570ms/step - loss: 17.5000 - final_loss: 15.2558 - internal_loss: 19.7442 - val_loss: 15.5486 - val_final_loss: 14.8207 - val_internal_loss: 16.2765\n",
            "Epoch 204/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.4435 - final_loss: 15.1906 - internal_loss: 19.6963\n",
            "Epoch 204: val_final_loss improved from 14.72488 to 14.69529, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 588ms/step - loss: 17.4435 - final_loss: 15.1906 - internal_loss: 19.6963 - val_loss: 15.3874 - val_final_loss: 14.6953 - val_internal_loss: 16.0795\n",
            "Epoch 205/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.4286 - final_loss: 15.1899 - internal_loss: 19.6673\n",
            "Epoch 205: val_final_loss did not improve from 14.69529\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 17.4286 - final_loss: 15.1899 - internal_loss: 19.6673 - val_loss: 15.5889 - val_final_loss: 14.8835 - val_internal_loss: 16.2943\n",
            "Epoch 206/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.4467 - final_loss: 15.1973 - internal_loss: 19.6961\n",
            "Epoch 206: val_final_loss improved from 14.69529 to 14.63687, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 587ms/step - loss: 17.4467 - final_loss: 15.1973 - internal_loss: 19.6961 - val_loss: 15.3524 - val_final_loss: 14.6369 - val_internal_loss: 16.0680\n",
            "Epoch 207/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.3791 - final_loss: 15.1127 - internal_loss: 19.6455\n",
            "Epoch 207: val_final_loss did not improve from 14.63687\n",
            "111/111 [==============================] - 69s 571ms/step - loss: 17.3791 - final_loss: 15.1127 - internal_loss: 19.6455 - val_loss: 15.4826 - val_final_loss: 14.6853 - val_internal_loss: 16.2798\n",
            "Epoch 208/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.3929 - final_loss: 15.1448 - internal_loss: 19.6411\n",
            "Epoch 208: val_final_loss did not improve from 14.63687\n",
            "111/111 [==============================] - 70s 574ms/step - loss: 17.3929 - final_loss: 15.1448 - internal_loss: 19.6411 - val_loss: 15.4213 - val_final_loss: 14.7141 - val_internal_loss: 16.1286\n",
            "Epoch 209/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.3401 - final_loss: 15.0867 - internal_loss: 19.5934\n",
            "Epoch 209: val_final_loss did not improve from 14.63687\n",
            "111/111 [==============================] - 70s 574ms/step - loss: 17.3401 - final_loss: 15.0867 - internal_loss: 19.5934 - val_loss: 15.5227 - val_final_loss: 14.7732 - val_internal_loss: 16.2721\n",
            "Epoch 210/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.3261 - final_loss: 15.0781 - internal_loss: 19.5740\n",
            "Epoch 210: val_final_loss did not improve from 14.63687\n",
            "111/111 [==============================] - 69s 571ms/step - loss: 17.3261 - final_loss: 15.0781 - internal_loss: 19.5740 - val_loss: 15.3690 - val_final_loss: 14.6672 - val_internal_loss: 16.0708\n",
            "Epoch 211/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.2484 - final_loss: 15.0046 - internal_loss: 19.4923\n",
            "Epoch 211: val_final_loss did not improve from 14.63687\n",
            "111/111 [==============================] - 69s 571ms/step - loss: 17.2484 - final_loss: 15.0046 - internal_loss: 19.4923 - val_loss: 15.3643 - val_final_loss: 14.6434 - val_internal_loss: 16.0852\n",
            "Epoch 212/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.1806 - final_loss: 14.9286 - internal_loss: 19.4327\n",
            "Epoch 212: val_final_loss did not improve from 14.63687\n",
            "111/111 [==============================] - 73s 612ms/step - loss: 17.1806 - final_loss: 14.9286 - internal_loss: 19.4327 - val_loss: 15.3737 - val_final_loss: 14.6804 - val_internal_loss: 16.0670\n",
            "Epoch 213/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.2462 - final_loss: 14.9899 - internal_loss: 19.5025\n",
            "Epoch 213: val_final_loss did not improve from 14.63687\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 17.2462 - final_loss: 14.9899 - internal_loss: 19.5025 - val_loss: 15.5815 - val_final_loss: 14.8647 - val_internal_loss: 16.2983\n",
            "Epoch 214/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.1985 - final_loss: 14.9351 - internal_loss: 19.4619\n",
            "Epoch 214: val_final_loss did not improve from 14.63687\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 17.1985 - final_loss: 14.9351 - internal_loss: 19.4619 - val_loss: 15.4420 - val_final_loss: 14.7037 - val_internal_loss: 16.1803\n",
            "Epoch 215/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.1883 - final_loss: 14.9240 - internal_loss: 19.4526\n",
            "Epoch 215: val_final_loss did not improve from 14.63687\n",
            "111/111 [==============================] - 72s 592ms/step - loss: 17.1883 - final_loss: 14.9240 - internal_loss: 19.4526 - val_loss: 15.4609 - val_final_loss: 14.7635 - val_internal_loss: 16.1583\n",
            "Epoch 216/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.1679 - final_loss: 14.9044 - internal_loss: 19.4314\n",
            "Epoch 216: val_final_loss did not improve from 14.63687\n",
            "111/111 [==============================] - 69s 570ms/step - loss: 17.1679 - final_loss: 14.9044 - internal_loss: 19.4314 - val_loss: 15.3824 - val_final_loss: 14.6747 - val_internal_loss: 16.0902\n",
            "Epoch 217/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.1538 - final_loss: 14.8883 - internal_loss: 19.4192\n",
            "Epoch 217: val_final_loss did not improve from 14.63687\n",
            "111/111 [==============================] - 69s 568ms/step - loss: 17.1538 - final_loss: 14.8883 - internal_loss: 19.4192 - val_loss: 15.3993 - val_final_loss: 14.6767 - val_internal_loss: 16.1218\n",
            "Epoch 218/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.1078 - final_loss: 14.8426 - internal_loss: 19.3731\n",
            "Epoch 218: val_final_loss improved from 14.63687 to 14.61733, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 586ms/step - loss: 17.1078 - final_loss: 14.8426 - internal_loss: 19.3731 - val_loss: 15.3409 - val_final_loss: 14.6173 - val_internal_loss: 16.0645\n",
            "Epoch 219/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.0805 - final_loss: 14.8080 - internal_loss: 19.3530\n",
            "Epoch 219: val_final_loss did not improve from 14.61733\n",
            "111/111 [==============================] - 69s 569ms/step - loss: 17.0805 - final_loss: 14.8080 - internal_loss: 19.3530 - val_loss: 15.3772 - val_final_loss: 14.6432 - val_internal_loss: 16.1111\n",
            "Epoch 220/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.1005 - final_loss: 14.8244 - internal_loss: 19.3766\n",
            "Epoch 220: val_final_loss did not improve from 14.61733\n",
            "111/111 [==============================] - 70s 569ms/step - loss: 17.1005 - final_loss: 14.8244 - internal_loss: 19.3766 - val_loss: 15.3283 - val_final_loss: 14.6282 - val_internal_loss: 16.0284\n",
            "Epoch 221/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.0143 - final_loss: 14.7374 - internal_loss: 19.2912\n",
            "Epoch 221: val_final_loss did not improve from 14.61733\n",
            "111/111 [==============================] - 69s 568ms/step - loss: 17.0143 - final_loss: 14.7374 - internal_loss: 19.2912 - val_loss: 15.5228 - val_final_loss: 14.8329 - val_internal_loss: 16.2127\n",
            "Epoch 222/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 17.0448 - final_loss: 14.7786 - internal_loss: 19.3109\n",
            "Epoch 222: val_final_loss did not improve from 14.61733\n",
            "111/111 [==============================] - 68s 570ms/step - loss: 17.0448 - final_loss: 14.7786 - internal_loss: 19.3109 - val_loss: 15.3308 - val_final_loss: 14.6274 - val_internal_loss: 16.0342\n",
            "Epoch 223/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.9793 - final_loss: 14.7101 - internal_loss: 19.2484\n",
            "Epoch 223: val_final_loss did not improve from 14.61733\n",
            "111/111 [==============================] - 69s 571ms/step - loss: 16.9793 - final_loss: 14.7101 - internal_loss: 19.2484 - val_loss: 15.5408 - val_final_loss: 14.8506 - val_internal_loss: 16.2309\n",
            "Epoch 224/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.9704 - final_loss: 14.6892 - internal_loss: 19.2515\n",
            "Epoch 224: val_final_loss improved from 14.61733 to 14.56337, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 587ms/step - loss: 16.9704 - final_loss: 14.6892 - internal_loss: 19.2515 - val_loss: 15.2893 - val_final_loss: 14.5634 - val_internal_loss: 16.0153\n",
            "Epoch 225/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.9752 - final_loss: 14.7001 - internal_loss: 19.2504\n",
            "Epoch 225: val_final_loss did not improve from 14.56337\n",
            "111/111 [==============================] - 69s 567ms/step - loss: 16.9752 - final_loss: 14.7001 - internal_loss: 19.2504 - val_loss: 15.3818 - val_final_loss: 14.6305 - val_internal_loss: 16.1332\n",
            "Epoch 226/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.9280 - final_loss: 14.6553 - internal_loss: 19.2006\n",
            "Epoch 226: val_final_loss did not improve from 14.56337\n",
            "111/111 [==============================] - 70s 578ms/step - loss: 16.9280 - final_loss: 14.6553 - internal_loss: 19.2006 - val_loss: 15.3327 - val_final_loss: 14.5855 - val_internal_loss: 16.0799\n",
            "Epoch 227/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.9218 - final_loss: 14.6519 - internal_loss: 19.1917\n",
            "Epoch 227: val_final_loss did not improve from 14.56337\n",
            "111/111 [==============================] - 70s 576ms/step - loss: 16.9218 - final_loss: 14.6519 - internal_loss: 19.1917 - val_loss: 15.3015 - val_final_loss: 14.6037 - val_internal_loss: 15.9992\n",
            "Epoch 228/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.9230 - final_loss: 14.6346 - internal_loss: 19.2114\n",
            "Epoch 228: val_final_loss did not improve from 14.56337\n",
            "111/111 [==============================] - 70s 575ms/step - loss: 16.9230 - final_loss: 14.6346 - internal_loss: 19.2114 - val_loss: 15.3162 - val_final_loss: 14.5872 - val_internal_loss: 16.0451\n",
            "Epoch 229/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.8958 - final_loss: 14.6095 - internal_loss: 19.1821\n",
            "Epoch 229: val_final_loss did not improve from 14.56337\n",
            "111/111 [==============================] - 70s 576ms/step - loss: 16.8958 - final_loss: 14.6095 - internal_loss: 19.1821 - val_loss: 15.3760 - val_final_loss: 14.6653 - val_internal_loss: 16.0867\n",
            "Epoch 230/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.8021 - final_loss: 14.5059 - internal_loss: 19.0983\n",
            "Epoch 230: val_final_loss did not improve from 14.56337\n",
            "111/111 [==============================] - 70s 584ms/step - loss: 16.8021 - final_loss: 14.5059 - internal_loss: 19.0983 - val_loss: 15.2980 - val_final_loss: 14.5700 - val_internal_loss: 16.0260\n",
            "Epoch 231/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.8544 - final_loss: 14.5664 - internal_loss: 19.1423\n",
            "Epoch 231: val_final_loss did not improve from 14.56337\n",
            "111/111 [==============================] - 70s 576ms/step - loss: 16.8544 - final_loss: 14.5664 - internal_loss: 19.1423 - val_loss: 15.3573 - val_final_loss: 14.6506 - val_internal_loss: 16.0639\n",
            "Epoch 232/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.8646 - final_loss: 14.5787 - internal_loss: 19.1504\n",
            "Epoch 232: val_final_loss improved from 14.56337 to 14.56165, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 72s 591ms/step - loss: 16.8646 - final_loss: 14.5787 - internal_loss: 19.1504 - val_loss: 15.2569 - val_final_loss: 14.5616 - val_internal_loss: 15.9521\n",
            "Epoch 233/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.8075 - final_loss: 14.5112 - internal_loss: 19.1038\n",
            "Epoch 233: val_final_loss improved from 14.56165 to 14.55041, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 72s 592ms/step - loss: 16.8075 - final_loss: 14.5112 - internal_loss: 19.1038 - val_loss: 15.2693 - val_final_loss: 14.5504 - val_internal_loss: 15.9881\n",
            "Epoch 234/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.8060 - final_loss: 14.5324 - internal_loss: 19.0795\n",
            "Epoch 234: val_final_loss did not improve from 14.55041\n",
            "111/111 [==============================] - 70s 576ms/step - loss: 16.8060 - final_loss: 14.5324 - internal_loss: 19.0795 - val_loss: 15.2668 - val_final_loss: 14.5592 - val_internal_loss: 15.9744\n",
            "Epoch 235/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.7765 - final_loss: 14.4843 - internal_loss: 19.0686\n",
            "Epoch 235: val_final_loss did not improve from 14.55041\n",
            "111/111 [==============================] - 70s 576ms/step - loss: 16.7765 - final_loss: 14.4843 - internal_loss: 19.0686 - val_loss: 15.3204 - val_final_loss: 14.6261 - val_internal_loss: 16.0147\n",
            "Epoch 236/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.7076 - final_loss: 14.4038 - internal_loss: 19.0114\n",
            "Epoch 236: val_final_loss did not improve from 14.55041\n",
            "111/111 [==============================] - 69s 577ms/step - loss: 16.7076 - final_loss: 14.4038 - internal_loss: 19.0114 - val_loss: 15.2901 - val_final_loss: 14.5682 - val_internal_loss: 16.0120\n",
            "Epoch 237/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.7072 - final_loss: 14.4197 - internal_loss: 18.9947\n",
            "Epoch 237: val_final_loss did not improve from 14.55041\n",
            "111/111 [==============================] - 73s 603ms/step - loss: 16.7072 - final_loss: 14.4197 - internal_loss: 18.9947 - val_loss: 15.2870 - val_final_loss: 14.5667 - val_internal_loss: 16.0073\n",
            "Epoch 238/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.7624 - final_loss: 14.4771 - internal_loss: 19.0476\n",
            "Epoch 238: val_final_loss did not improve from 14.55041\n",
            "111/111 [==============================] - 71s 579ms/step - loss: 16.7624 - final_loss: 14.4771 - internal_loss: 19.0476 - val_loss: 15.2763 - val_final_loss: 14.5711 - val_internal_loss: 15.9816\n",
            "Epoch 239/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.7293 - final_loss: 14.4384 - internal_loss: 19.0203\n",
            "Epoch 239: val_final_loss did not improve from 14.55041\n",
            "111/111 [==============================] - 70s 577ms/step - loss: 16.7293 - final_loss: 14.4384 - internal_loss: 19.0203 - val_loss: 15.2785 - val_final_loss: 14.5769 - val_internal_loss: 15.9802\n",
            "Epoch 240/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.6072 - final_loss: 14.3048 - internal_loss: 18.9097\n",
            "Epoch 240: val_final_loss improved from 14.55041 to 14.53795, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 70s 590ms/step - loss: 16.6072 - final_loss: 14.3048 - internal_loss: 18.9097 - val_loss: 15.2581 - val_final_loss: 14.5380 - val_internal_loss: 15.9782\n",
            "Epoch 241/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.6229 - final_loss: 14.3441 - internal_loss: 18.9017\n",
            "Epoch 241: val_final_loss did not improve from 14.53795\n",
            "111/111 [==============================] - 70s 577ms/step - loss: 16.6229 - final_loss: 14.3441 - internal_loss: 18.9017 - val_loss: 15.3003 - val_final_loss: 14.5935 - val_internal_loss: 16.0070\n",
            "Epoch 242/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.6539 - final_loss: 14.3547 - internal_loss: 18.9530\n",
            "Epoch 242: val_final_loss improved from 14.53795 to 14.52654, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 588ms/step - loss: 16.6539 - final_loss: 14.3547 - internal_loss: 18.9530 - val_loss: 15.2262 - val_final_loss: 14.5265 - val_internal_loss: 15.9259\n",
            "Epoch 243/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.6607 - final_loss: 14.3582 - internal_loss: 18.9632\n",
            "Epoch 243: val_final_loss did not improve from 14.52654\n",
            "111/111 [==============================] - 70s 576ms/step - loss: 16.6607 - final_loss: 14.3582 - internal_loss: 18.9632 - val_loss: 15.2612 - val_final_loss: 14.5505 - val_internal_loss: 15.9719\n",
            "Epoch 244/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.6032 - final_loss: 14.3060 - internal_loss: 18.9003\n",
            "Epoch 244: val_final_loss did not improve from 14.52654\n",
            "111/111 [==============================] - 70s 578ms/step - loss: 16.6032 - final_loss: 14.3060 - internal_loss: 18.9003 - val_loss: 15.2495 - val_final_loss: 14.5266 - val_internal_loss: 15.9723\n",
            "Epoch 245/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.5518 - final_loss: 14.2575 - internal_loss: 18.8462\n",
            "Epoch 245: val_final_loss did not improve from 14.52654\n",
            "111/111 [==============================] - 69s 574ms/step - loss: 16.5518 - final_loss: 14.2575 - internal_loss: 18.8462 - val_loss: 15.2870 - val_final_loss: 14.6145 - val_internal_loss: 15.9594\n",
            "Epoch 246/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.5502 - final_loss: 14.2505 - internal_loss: 18.8500\n",
            "Epoch 246: val_final_loss did not improve from 14.52654\n",
            "111/111 [==============================] - 69s 575ms/step - loss: 16.5502 - final_loss: 14.2505 - internal_loss: 18.8500 - val_loss: 15.2672 - val_final_loss: 14.5775 - val_internal_loss: 15.9569\n",
            "Epoch 247/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.6161 - final_loss: 14.3125 - internal_loss: 18.9198\n",
            "Epoch 247: val_final_loss did not improve from 14.52654\n",
            "111/111 [==============================] - 70s 574ms/step - loss: 16.6161 - final_loss: 14.3125 - internal_loss: 18.9198 - val_loss: 15.2939 - val_final_loss: 14.5979 - val_internal_loss: 15.9900\n",
            "Epoch 248/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.5338 - final_loss: 14.2352 - internal_loss: 18.8324\n",
            "Epoch 248: val_final_loss improved from 14.52654 to 14.51861, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 71s 594ms/step - loss: 16.5338 - final_loss: 14.2352 - internal_loss: 18.8324 - val_loss: 15.2355 - val_final_loss: 14.5186 - val_internal_loss: 15.9525\n",
            "Epoch 249/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.5444 - final_loss: 14.2353 - internal_loss: 18.8535\n",
            "Epoch 249: val_final_loss did not improve from 14.51861\n",
            "111/111 [==============================] - 70s 579ms/step - loss: 16.5444 - final_loss: 14.2353 - internal_loss: 18.8535 - val_loss: 15.2635 - val_final_loss: 14.5643 - val_internal_loss: 15.9626\n",
            "Epoch 250/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.5568 - final_loss: 14.2586 - internal_loss: 18.8549\n",
            "Epoch 250: val_final_loss did not improve from 14.51861\n",
            "111/111 [==============================] - 70s 578ms/step - loss: 16.5568 - final_loss: 14.2586 - internal_loss: 18.8549 - val_loss: 15.2379 - val_final_loss: 14.5480 - val_internal_loss: 15.9278\n",
            "Epoch 251/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.5822 - final_loss: 14.2923 - internal_loss: 18.8720\n",
            "Epoch 251: val_final_loss improved from 14.51861 to 14.50324, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 72s 594ms/step - loss: 16.5822 - final_loss: 14.2923 - internal_loss: 18.8720 - val_loss: 15.2157 - val_final_loss: 14.5032 - val_internal_loss: 15.9282\n",
            "Epoch 252/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.4917 - final_loss: 14.1922 - internal_loss: 18.7911\n",
            "Epoch 252: val_final_loss did not improve from 14.50324\n",
            "111/111 [==============================] - 70s 577ms/step - loss: 16.4917 - final_loss: 14.1922 - internal_loss: 18.7911 - val_loss: 15.2300 - val_final_loss: 14.5370 - val_internal_loss: 15.9230\n",
            "Epoch 253/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.4295 - final_loss: 14.1266 - internal_loss: 18.7325\n",
            "Epoch 253: val_final_loss did not improve from 14.50324\n",
            "111/111 [==============================] - 69s 578ms/step - loss: 16.4295 - final_loss: 14.1266 - internal_loss: 18.7325 - val_loss: 15.2273 - val_final_loss: 14.5232 - val_internal_loss: 15.9313\n",
            "Epoch 254/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.4848 - final_loss: 14.1847 - internal_loss: 18.7849\n",
            "Epoch 254: val_final_loss improved from 14.50324 to 14.48467, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 72s 595ms/step - loss: 16.4848 - final_loss: 14.1847 - internal_loss: 18.7849 - val_loss: 15.1871 - val_final_loss: 14.4847 - val_internal_loss: 15.8895\n",
            "Epoch 255/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.4865 - final_loss: 14.1748 - internal_loss: 18.7982\n",
            "Epoch 255: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 577ms/step - loss: 16.4865 - final_loss: 14.1748 - internal_loss: 18.7982 - val_loss: 15.2330 - val_final_loss: 14.5476 - val_internal_loss: 15.9184\n",
            "Epoch 256/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.4533 - final_loss: 14.1416 - internal_loss: 18.7650\n",
            "Epoch 256: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 580ms/step - loss: 16.4533 - final_loss: 14.1416 - internal_loss: 18.7650 - val_loss: 15.2194 - val_final_loss: 14.5299 - val_internal_loss: 15.9090\n",
            "Epoch 257/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.4161 - final_loss: 14.1078 - internal_loss: 18.7244\n",
            "Epoch 257: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 577ms/step - loss: 16.4161 - final_loss: 14.1078 - internal_loss: 18.7244 - val_loss: 15.2394 - val_final_loss: 14.5316 - val_internal_loss: 15.9472\n",
            "Epoch 258/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.4760 - final_loss: 14.1692 - internal_loss: 18.7827\n",
            "Epoch 258: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 575ms/step - loss: 16.4760 - final_loss: 14.1692 - internal_loss: 18.7827 - val_loss: 15.2129 - val_final_loss: 14.5152 - val_internal_loss: 15.9107\n",
            "Epoch 259/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.4551 - final_loss: 14.1526 - internal_loss: 18.7576\n",
            "Epoch 259: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 72s 597ms/step - loss: 16.4551 - final_loss: 14.1526 - internal_loss: 18.7576 - val_loss: 15.2159 - val_final_loss: 14.5263 - val_internal_loss: 15.9055\n",
            "Epoch 260/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.4269 - final_loss: 14.1210 - internal_loss: 18.7328\n",
            "Epoch 260: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 579ms/step - loss: 16.4269 - final_loss: 14.1210 - internal_loss: 18.7328 - val_loss: 15.2309 - val_final_loss: 14.5357 - val_internal_loss: 15.9261\n",
            "Epoch 261/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.4774 - final_loss: 14.1875 - internal_loss: 18.7673\n",
            "Epoch 261: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 578ms/step - loss: 16.4774 - final_loss: 14.1875 - internal_loss: 18.7673 - val_loss: 15.2334 - val_final_loss: 14.5407 - val_internal_loss: 15.9262\n",
            "Epoch 262/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.4116 - final_loss: 14.1019 - internal_loss: 18.7212\n",
            "Epoch 262: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 577ms/step - loss: 16.4116 - final_loss: 14.1019 - internal_loss: 18.7212 - val_loss: 15.2198 - val_final_loss: 14.5349 - val_internal_loss: 15.9046\n",
            "Epoch 263/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.3943 - final_loss: 14.0852 - internal_loss: 18.7034\n",
            "Epoch 263: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 579ms/step - loss: 16.3943 - final_loss: 14.0852 - internal_loss: 18.7034 - val_loss: 15.2274 - val_final_loss: 14.5221 - val_internal_loss: 15.9328\n",
            "Epoch 264/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.4227 - final_loss: 14.1034 - internal_loss: 18.7421\n",
            "Epoch 264: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 577ms/step - loss: 16.4227 - final_loss: 14.1034 - internal_loss: 18.7421 - val_loss: 15.2018 - val_final_loss: 14.5172 - val_internal_loss: 15.8864\n",
            "Epoch 265/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.3842 - final_loss: 14.0651 - internal_loss: 18.7033\n",
            "Epoch 265: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 575ms/step - loss: 16.3842 - final_loss: 14.0651 - internal_loss: 18.7033 - val_loss: 15.1792 - val_final_loss: 14.4925 - val_internal_loss: 15.8659\n",
            "Epoch 266/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.3630 - final_loss: 14.0554 - internal_loss: 18.6706\n",
            "Epoch 266: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 576ms/step - loss: 16.3630 - final_loss: 14.0554 - internal_loss: 18.6706 - val_loss: 15.2029 - val_final_loss: 14.5139 - val_internal_loss: 15.8919\n",
            "Epoch 267/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.3794 - final_loss: 14.0641 - internal_loss: 18.6947\n",
            "Epoch 267: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 574ms/step - loss: 16.3794 - final_loss: 14.0641 - internal_loss: 18.6947 - val_loss: 15.2113 - val_final_loss: 14.5314 - val_internal_loss: 15.8912\n",
            "Epoch 268/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.3587 - final_loss: 14.0428 - internal_loss: 18.6746\n",
            "Epoch 268: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 578ms/step - loss: 16.3587 - final_loss: 14.0428 - internal_loss: 18.6746 - val_loss: 15.2004 - val_final_loss: 14.5194 - val_internal_loss: 15.8814\n",
            "Epoch 269/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.3310 - final_loss: 14.0191 - internal_loss: 18.6430\n",
            "Epoch 269: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 575ms/step - loss: 16.3310 - final_loss: 14.0191 - internal_loss: 18.6430 - val_loss: 15.1854 - val_final_loss: 14.4933 - val_internal_loss: 15.8774\n",
            "Epoch 270/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.3457 - final_loss: 14.0261 - internal_loss: 18.6653\n",
            "Epoch 270: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 574ms/step - loss: 16.3457 - final_loss: 14.0261 - internal_loss: 18.6653 - val_loss: 15.2123 - val_final_loss: 14.5248 - val_internal_loss: 15.8998\n",
            "Epoch 271/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.3236 - final_loss: 14.0119 - internal_loss: 18.6353\n",
            "Epoch 271: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 576ms/step - loss: 16.3236 - final_loss: 14.0119 - internal_loss: 18.6353 - val_loss: 15.2067 - val_final_loss: 14.5193 - val_internal_loss: 15.8940\n",
            "Epoch 272/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.3141 - final_loss: 13.9976 - internal_loss: 18.6305\n",
            "Epoch 272: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 575ms/step - loss: 16.3141 - final_loss: 13.9976 - internal_loss: 18.6305 - val_loss: 15.2004 - val_final_loss: 14.5167 - val_internal_loss: 15.8842\n",
            "Epoch 273/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.3285 - final_loss: 14.0177 - internal_loss: 18.6394\n",
            "Epoch 273: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 71s 578ms/step - loss: 16.3285 - final_loss: 14.0177 - internal_loss: 18.6394 - val_loss: 15.1780 - val_final_loss: 14.4883 - val_internal_loss: 15.8678\n",
            "Epoch 274/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.3219 - final_loss: 14.0090 - internal_loss: 18.6349\n",
            "Epoch 274: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 577ms/step - loss: 16.3219 - final_loss: 14.0090 - internal_loss: 18.6349 - val_loss: 15.2032 - val_final_loss: 14.5128 - val_internal_loss: 15.8936\n",
            "Epoch 275/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.3228 - final_loss: 14.0053 - internal_loss: 18.6403\n",
            "Epoch 275: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 579ms/step - loss: 16.3228 - final_loss: 14.0053 - internal_loss: 18.6403 - val_loss: 15.1837 - val_final_loss: 14.4887 - val_internal_loss: 15.8787\n",
            "Epoch 276/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2999 - final_loss: 13.9844 - internal_loss: 18.6154\n",
            "Epoch 276: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 580ms/step - loss: 16.2999 - final_loss: 13.9844 - internal_loss: 18.6154 - val_loss: 15.1898 - val_final_loss: 14.5048 - val_internal_loss: 15.8748\n",
            "Epoch 277/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2846 - final_loss: 13.9628 - internal_loss: 18.6064\n",
            "Epoch 277: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 579ms/step - loss: 16.2846 - final_loss: 13.9628 - internal_loss: 18.6064 - val_loss: 15.1788 - val_final_loss: 14.4889 - val_internal_loss: 15.8687\n",
            "Epoch 278/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2713 - final_loss: 13.9475 - internal_loss: 18.5952\n",
            "Epoch 278: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 70s 579ms/step - loss: 16.2713 - final_loss: 13.9475 - internal_loss: 18.5952 - val_loss: 15.1764 - val_final_loss: 14.4851 - val_internal_loss: 15.8677\n",
            "Epoch 279/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.3354 - final_loss: 14.0210 - internal_loss: 18.6497\n",
            "Epoch 279: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 71s 579ms/step - loss: 16.3354 - final_loss: 14.0210 - internal_loss: 18.6497 - val_loss: 15.1810 - val_final_loss: 14.4945 - val_internal_loss: 15.8676\n",
            "Epoch 280/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2277 - final_loss: 13.9260 - internal_loss: 18.5294\n",
            "Epoch 280: val_final_loss did not improve from 14.48467\n",
            "111/111 [==============================] - 69s 576ms/step - loss: 16.2277 - final_loss: 13.9260 - internal_loss: 18.5294 - val_loss: 15.1747 - val_final_loss: 14.4910 - val_internal_loss: 15.8584\n",
            "Epoch 281/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2834 - final_loss: 13.9650 - internal_loss: 18.6017\n",
            "Epoch 281: val_final_loss improved from 14.48467 to 14.48221, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 74s 614ms/step - loss: 16.2834 - final_loss: 13.9650 - internal_loss: 18.6017 - val_loss: 15.1666 - val_final_loss: 14.4822 - val_internal_loss: 15.8510\n",
            "Epoch 282/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.3111 - final_loss: 14.0052 - internal_loss: 18.6170\n",
            "Epoch 282: val_final_loss did not improve from 14.48221\n",
            "111/111 [==============================] - 70s 579ms/step - loss: 16.3111 - final_loss: 14.0051 - internal_loss: 18.6170 - val_loss: 15.1756 - val_final_loss: 14.4866 - val_internal_loss: 15.8646\n",
            "Epoch 283/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2803 - final_loss: 13.9611 - internal_loss: 18.5996\n",
            "Epoch 283: val_final_loss did not improve from 14.48221\n",
            "111/111 [==============================] - 71s 582ms/step - loss: 16.2803 - final_loss: 13.9611 - internal_loss: 18.5996 - val_loss: 15.1760 - val_final_loss: 14.4864 - val_internal_loss: 15.8657\n",
            "Epoch 284/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2752 - final_loss: 13.9627 - internal_loss: 18.5877\n",
            "Epoch 284: val_final_loss improved from 14.48221 to 14.47604, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 72s 593ms/step - loss: 16.2752 - final_loss: 13.9627 - internal_loss: 18.5877 - val_loss: 15.1669 - val_final_loss: 14.4760 - val_internal_loss: 15.8577\n",
            "Epoch 285/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2968 - final_loss: 13.9826 - internal_loss: 18.6110\n",
            "Epoch 285: val_final_loss improved from 14.47604 to 14.47083, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 72s 596ms/step - loss: 16.2968 - final_loss: 13.9826 - internal_loss: 18.6110 - val_loss: 15.1642 - val_final_loss: 14.4708 - val_internal_loss: 15.8576\n",
            "Epoch 286/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2698 - final_loss: 13.9631 - internal_loss: 18.5765\n",
            "Epoch 286: val_final_loss improved from 14.47083 to 14.44079, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 72s 593ms/step - loss: 16.2698 - final_loss: 13.9631 - internal_loss: 18.5765 - val_loss: 15.1359 - val_final_loss: 14.4408 - val_internal_loss: 15.8311\n",
            "Epoch 287/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.3041 - final_loss: 13.9920 - internal_loss: 18.6163\n",
            "Epoch 287: val_final_loss did not improve from 14.44079\n",
            "111/111 [==============================] - 70s 578ms/step - loss: 16.3041 - final_loss: 13.9920 - internal_loss: 18.6163 - val_loss: 15.1562 - val_final_loss: 14.4683 - val_internal_loss: 15.8441\n",
            "Epoch 288/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2756 - final_loss: 13.9543 - internal_loss: 18.5969\n",
            "Epoch 288: val_final_loss did not improve from 14.44079\n",
            "111/111 [==============================] - 70s 580ms/step - loss: 16.2756 - final_loss: 13.9543 - internal_loss: 18.5969 - val_loss: 15.1600 - val_final_loss: 14.4682 - val_internal_loss: 15.8518\n",
            "Epoch 289/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2297 - final_loss: 13.9089 - internal_loss: 18.5505\n",
            "Epoch 289: val_final_loss did not improve from 14.44079\n",
            "111/111 [==============================] - 70s 578ms/step - loss: 16.2297 - final_loss: 13.9089 - internal_loss: 18.5505 - val_loss: 15.1411 - val_final_loss: 14.4479 - val_internal_loss: 15.8343\n",
            "Epoch 290/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2590 - final_loss: 13.9427 - internal_loss: 18.5753\n",
            "Epoch 290: val_final_loss did not improve from 14.44079\n",
            "111/111 [==============================] - 70s 580ms/step - loss: 16.2590 - final_loss: 13.9427 - internal_loss: 18.5753 - val_loss: 15.1409 - val_final_loss: 14.4484 - val_internal_loss: 15.8334\n",
            "Epoch 291/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2934 - final_loss: 13.9811 - internal_loss: 18.6058\n",
            "Epoch 291: val_final_loss did not improve from 14.44079\n",
            "111/111 [==============================] - 70s 579ms/step - loss: 16.2934 - final_loss: 13.9811 - internal_loss: 18.6058 - val_loss: 15.1396 - val_final_loss: 14.4444 - val_internal_loss: 15.8348\n",
            "Epoch 292/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.3052 - final_loss: 13.9975 - internal_loss: 18.6128\n",
            "Epoch 292: val_final_loss did not improve from 14.44079\n",
            "111/111 [==============================] - 70s 576ms/step - loss: 16.3052 - final_loss: 13.9975 - internal_loss: 18.6128 - val_loss: 15.1404 - val_final_loss: 14.4487 - val_internal_loss: 15.8322\n",
            "Epoch 293/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2674 - final_loss: 13.9466 - internal_loss: 18.5883\n",
            "Epoch 293: val_final_loss did not improve from 14.44079\n",
            "111/111 [==============================] - 70s 575ms/step - loss: 16.2674 - final_loss: 13.9466 - internal_loss: 18.5883 - val_loss: 15.1486 - val_final_loss: 14.4614 - val_internal_loss: 15.8358\n",
            "Epoch 294/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2489 - final_loss: 13.9307 - internal_loss: 18.5671\n",
            "Epoch 294: val_final_loss did not improve from 14.44079\n",
            "111/111 [==============================] - 70s 577ms/step - loss: 16.2489 - final_loss: 13.9307 - internal_loss: 18.5671 - val_loss: 15.1402 - val_final_loss: 14.4447 - val_internal_loss: 15.8357\n",
            "Epoch 295/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2901 - final_loss: 13.9741 - internal_loss: 18.6061\n",
            "Epoch 295: val_final_loss did not improve from 14.44079\n",
            "111/111 [==============================] - 70s 576ms/step - loss: 16.2901 - final_loss: 13.9741 - internal_loss: 18.6061 - val_loss: 15.1513 - val_final_loss: 14.4615 - val_internal_loss: 15.8410\n",
            "Epoch 296/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.1259 - final_loss: 13.8048 - internal_loss: 18.4470\n",
            "Epoch 296: val_final_loss did not improve from 14.44079\n",
            "111/111 [==============================] - 70s 575ms/step - loss: 16.1259 - final_loss: 13.8048 - internal_loss: 18.4470 - val_loss: 15.1529 - val_final_loss: 14.4750 - val_internal_loss: 15.8309\n",
            "Epoch 297/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2593 - final_loss: 13.9396 - internal_loss: 18.5791\n",
            "Epoch 297: val_final_loss did not improve from 14.44079\n",
            "111/111 [==============================] - 70s 575ms/step - loss: 16.2593 - final_loss: 13.9396 - internal_loss: 18.5791 - val_loss: 15.1456 - val_final_loss: 14.4567 - val_internal_loss: 15.8345\n",
            "Epoch 298/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2522 - final_loss: 13.9380 - internal_loss: 18.5663\n",
            "Epoch 298: val_final_loss did not improve from 14.44079\n",
            "111/111 [==============================] - 70s 575ms/step - loss: 16.2522 - final_loss: 13.9380 - internal_loss: 18.5663 - val_loss: 15.1367 - val_final_loss: 14.4418 - val_internal_loss: 15.8317\n",
            "Epoch 299/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2894 - final_loss: 13.9769 - internal_loss: 18.6019\n",
            "Epoch 299: val_final_loss improved from 14.44079 to 14.43286, saving model to /content/drive/MyDrive/kaggle/working/model-384-seed42-exp0-best.h5\n",
            "111/111 [==============================] - 72s 592ms/step - loss: 16.2894 - final_loss: 13.9769 - internal_loss: 18.6019 - val_loss: 15.1302 - val_final_loss: 14.4329 - val_internal_loss: 15.8275\n",
            "Epoch 300/300\n",
            "111/111 [==============================] - ETA: 0s - loss: 16.2544 - final_loss: 13.9348 - internal_loss: 18.5741\n",
            "Epoch 300: val_final_loss did not improve from 14.43286\n",
            "111/111 [==============================] - 70s 575ms/step - loss: 16.2544 - final_loss: 13.9348 - internal_loss: 18.5741 - val_loss: 15.1402 - val_final_loss: 14.4514 - val_internal_loss: 15.8290\n",
            "applying SWA...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-8d2a58b7961c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_traceback_filtering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_supplemental\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-d3aab9c2a5e1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config, experiment_id, use_supplemental)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mnum_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_eval\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     train_run(\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mtrain_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mvalid_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-4c0062ec80de>\u001b[0m in \u001b[0;36mtrain_run\u001b[0;34m(train_files, valid_files, config, num_train, num_valid, experiment_id, use_tfrecords, summary)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;31m# callbacks.append(validation_callback)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_traceback_filtering_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1758\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_eval_data_handler\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_data_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1760\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1761\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-8aea6e98f6fb>\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswa_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for the re-calculation of running mean and var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"save SWA weights to {self.save_name}-SWA.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.save_name}-SWA.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-8aea6e98f6fb>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/tpu_strategy.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fn, args, kwargs, options)\u001b[0m\n\u001b[1;32m    418\u001b[0m       \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrunning\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0mreplica\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \"\"\"\n\u001b[0;32m--> 420\u001b[0;31m     \u001b[0mvalidate_run_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_partial_apply_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/tpu_strategy.py\u001b[0m in \u001b[0;36mvalidate_run_function\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConcreteFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdef_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;34m\"TPUStrategy.run(fn, ...) does not support pure eager \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;34m\"execution. please make sure the function passed into \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: TPUStrategy.run(fn, ...) does not support pure eager execution. please make sure the function passed into `strategy.run` is a `tf.function` or `strategy.run` is called inside a `tf.function` if eager behavior is enabled."
          ]
        }
      ],
      "source": [
        "if 'config' not in globals():\n",
        "  config=CFG()\n",
        "tf.debugging.disable_traceback_filtering()\n",
        "train(config,use_supplemental=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdtx5fW3-DPm"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uAqcPmSYi1-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWJM1IC5Yi1-"
      },
      "outputs": [],
      "source": [
        "class InferModel(tf.Module):\n",
        "    def __init__(self, model,config=CFG):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.max_len=config.max_len\n",
        "\n",
        "    @tf.function(\n",
        "        input_signature=[tf.TensorSpec(shape=(None,Constants.NUM_INPUT_FEATURES), dtype=tf.float32, name=\"inputs\")]\n",
        "    )\n",
        "    def __call__(self, inputs):\n",
        "        \"\"\"\n",
        "        Applies the feature generation model and main model to the input tensor.\n",
        "\n",
        "        Args:\n",
        "            inputs: Input tensor with shape (T, F).\n",
        "\n",
        "        Returns:\n",
        "            A dictionary with a single key 'outputs' and corresponding output tensor.\n",
        "        \"\"\"\n",
        "        x=tf.cast(inputs,tf.float32)\n",
        "        x = x[None] # trick to deal with empty frames\n",
        "        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, Constants.NUM_INPUT_FEATURES)), lambda: tf.identity(x))\n",
        "        x = x[0]\n",
        "        x = preprocess(x,max_len=self.max_len)\n",
        "\n",
        "        x = self.model(x[None],training=False)[0][0]\n",
        "\n",
        "        x=decode_phrase(x)\n",
        "        x = tf.cond(tf.shape(x)[0] == 0, lambda: tf.zeros(1, tf.int64), lambda: tf.identity(x))\n",
        "\n",
        "        outputs=tf.one_hot(x,depth=59,dtype=tf.float32)\n",
        "        return {\"outputs\": outputs}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xpye2JQYi1_"
      },
      "outputs": [],
      "source": [
        "\n",
        "config=CFG\n",
        "\n",
        "model = get_model(\n",
        "    max_len=config.max_len,\n",
        "    output_dim=config.output_dim,\n",
        "    dim=config.dim,\n",
        "    input_pad=Constants.INPUT_PAD,\n",
        ")\n",
        "experiment_id=0\n",
        "\n",
        "saved_based_model = f\"{config.input_path}/weights/{config.comment}-exp{experiment_id}-best.h5\"\n",
        "model.load_weights(saved_based_model)\n",
        "print(f\"model with weights {saved_based_model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwxBqovnYi1_"
      },
      "outputs": [],
      "source": [
        "# Sanity Check\n",
        "import json\n",
        "with open (config.input_path+\"/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
        "    character_map = json.load(f)\n",
        "rev_character_map = {j:i for i,j in character_map.items()}\n",
        "\n",
        "infer_keras_model=InferModel(model)\n",
        "\n",
        "main_dir = config.input_path+'/asl-fingerspelling'\n",
        "path = f'{main_dir}/train_landmarks/5414471.parquet'\n",
        "cols=selected_columns(path)\n",
        "df = pd.read_parquet(path, engine = 'auto', columns = cols)\n",
        "seq_id=1816796431\n",
        "seq=df.loc[seq_id]\n",
        "data = seq[cols].to_numpy()\n",
        "print(f'input shape: {data.shape}, dtype: {data.dtype}')\n",
        "output = infer_keras_model(data)[\"outputs\"]\n",
        "prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output, axis=1)])\n",
        "\n",
        "print(prediction_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQM7GMtfYi1_"
      },
      "outputs": [],
      "source": [
        "SAVED_MODEL_PATH=config.input_path+\"/infer_model\"\n",
        "\n",
        "tf.saved_model.save(infer_keras_model,SAVED_MODEL_PATH)\n",
        "keras_model_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_PATH)\n",
        "keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "keras_model_converter.target_spec.supported_types = [tf.float16]\n",
        "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "#converter.allow_custom_ops=True\n",
        "tflite_model = keras_model_converter.convert()\n",
        "TFLITE_FILE_PATH=config.output_path+\"/model.tflite\"\n",
        "with open(TFLITE_FILE_PATH, \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "with open(config.output_path+'/inference_args.json', 'w') as f:\n",
        "     json.dump({ 'selected_columns': cols }, f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KNUGXjBYi2A"
      },
      "outputs": [],
      "source": [
        "interpreter = tf.lite.Interpreter(TFLITE_FILE_PATH)\n",
        "REQUIRED_SIGNATURE = \"serving_default\"\n",
        "REQUIRED_OUTPUT = \"outputs\"\n",
        "found_signatures = list(interpreter.get_signature_list().keys())\n",
        "if REQUIRED_SIGNATURE not in found_signatures:\n",
        "    print(\"Required input signature not found.\")\n",
        "\n",
        "prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
        "output = prediction_fn(inputs=data)\n",
        "prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)])\n",
        "print(prediction_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTeDBunuYi2A"
      },
      "outputs": [],
      "source": [
        "!zip submission.zip config.output_path+\"/model.tflite\" config.output_path+\"/inference_args.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWcEOwClYi2A"
      },
      "outputs": [],
      "source": [
        "#!pip install /kaggle/input/tflite-wheels-2140/tflite_runtime_nightly-2.14.0.dev20230508-cp310-cp310-manylinux2014_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6W11OFc7Yi2A"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "import json\n",
        "import pandas as pd\n",
        "import tflite_runtime.interpreter as tflite\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import Levenshtein as Lev\n",
        "import glob\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoac5lq4Yi2B"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "SEL_FEATURES = json.load(open('/kaggle/working/inference_args.json'))['selected_columns']\n",
        "\n",
        "def load_relevant_data_subset(pq_path):\n",
        "        return pd.read_parquet(pq_path, columns=SEL_FEATURES) #selected_columns)\n",
        "\n",
        "with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
        "    character_map = json.load(f)\n",
        "rev_character_map = {j:i for i,j in character_map.items()}\n",
        "\n",
        "\n",
        "df = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\n",
        "\n",
        "idx = 0\n",
        "sample = df.loc[idx]\n",
        "loaded = load_relevant_data_subset('/kaggle/input/asl-fingerspelling/' + sample['path'])\n",
        "loaded = loaded[loaded.index==sample['sequence_id']].values\n",
        "print(loaded.shape)\n",
        "frames = loaded\n",
        "\n",
        "def wer__(s1, s2):\n",
        "    w1 = len(s1.split())\n",
        "    lvd = Lev.distance(s1, s2)\n",
        "    return lvd / w1\n",
        "\n",
        "interpreter = tflite.Interpreter('model.tflite')\n",
        "found_signatures = list(interpreter.get_signature_list().keys())\n",
        "\n",
        "REQUIRED_SIGNATURE = 'serving_default'\n",
        "REQUIRED_OUTPUT = 'outputs'\n",
        "if REQUIRED_SIGNATURE not in found_signatures:\n",
        "    raise KernelEvalException('Required input signature not found.')\n",
        "\n",
        "prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
        "output_lite = prediction_fn(inputs=frames)\n",
        "prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output_lite[REQUIRED_OUTPUT], axis=1)])\n",
        "print(prediction_str)\n",
        "\n",
        "\n",
        "st = time.time()\n",
        "count=0\n",
        "model_time = 0\n",
        "\n",
        "levs = []\n",
        "\n",
        "files=glob.glob('/kaggle/input/asl-fingerspelling/train_landmarks/*.parquet')\n",
        "for f in files:\n",
        "    df = load_relevant_data_subset(f)\n",
        "    seq=df.index.drop_duplicates()\n",
        "    for ind in tqdm(seq):\n",
        "        loaded = df.loc[ind].values\n",
        "        count+=1\n",
        "        md_st = time.time()\n",
        "        output_ = prediction_fn(inputs=loaded)\n",
        "        out= output_[REQUIRED_OUTPUT]\n",
        "        assert out.ndim==2\n",
        "        assert out.shape[1]==59\n",
        "        assert out.dtype==np.float32\n",
        "        assert np.all(np.isfinite(out))\n",
        "\n",
        "        prediction_str = \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output_[REQUIRED_OUTPUT], axis=1)])\n",
        "        model_time += time.time() - md_st\n",
        "\n",
        "        #cur_lev = wer__(sample['phrase'], prediction_str)\n",
        "        #print(sample['phrase'], '|', prediction_str, '|', cur_lev)\n",
        "        #print()\n",
        "\n",
        "        #levs.append(cur_lev)\n",
        "\n",
        "#print(f'WER: {np.mean(levs):.5f}')\n",
        "print(f'Mean time: {(time.time() - st)/count:.2f}')\n",
        "print(f'Mean time only infer: {model_time/count:.2f}')\n",
        "\n",
        "out=prediction_fn(inputs=np.empty(0,dtype=np.float32))[\"outputs\"]\n",
        "print(out.shape,output_.dtype)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etCjY_x2Yi2B"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "Vh1J_bDCYi13"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}